# Effective Kafka

## 1 - Event Streaming Fundamentals

- Les **systèmes distribués** sont plus complexes que les systèmes non distribués, ils déplacent une partie de la **complexité du local vers le global**.
  - La raison pour laquelle on les utilise c’est qu’ils permettent de décomposer le système en plus petits problèmes qu’on va pouvoir diviser entre plusieurs équipes.
  - La complexité globale peut être réduite par certaines techniques, par exemple les messages asynchrones.
  - On y trouve des échecs partiels, intermittents, ou même byzantins (les nœuds envoient des informations fausses).
  - Le problème le plus important est sans doute celui de la consistance.
- L’**event-driven architecture** (EDA) consiste à avoir des _emitters_ envoyant des notifications d’event à des _consumers_.
  - Les emitters n’ont aucune connaissance des consumers. Et de même les consumers n’ont pas connaissance des emitters.
  - Les notifications d’event sont immutables, que ce soit côté emitter ou consumer.
  - L’EDA est la manière **la plus découplée** de faire communiquer des composants entre eux.
    - Le seul couplage sera dans le contenu des messages qui transitent.
    - Imaginons un système d’e-commerce, avec une plateforme BI et un CRM. Il leur suffira de consommer les events d’achat et d’y réagir en toute indépendance.
      - Parmi les autres possibilités qu’on aurait pour l’exemple e-commerce :
        - On peut les mettre dans un monolith (non-modulaire), mais la complexité risque d’augmenter à mesure que le modèle global est enrichi.
        - On peut utiliser des patterns d’intégration : des messages synchrones envoyés par le composant e-commerce ou par les deux autres. Dans ce cas on se rapproche du distributed monolith parce que les composants ne seront pas indépendants.
          - NDLR : toute communication synchrone relèverait du distributed monolith, un peu violent…
        - On peut utiliser la _data decapsulation_, où les composants BI et CRM viennent lire la DB du composant e-commerce. Dans ce cas on se retrouve dans un mode “get rich quick scheme” qui mène toujours à des pleurs. Le couplage est maximal.
      - Cet exemple montre que **l’EDA scale de manière linéaire**, alors qu’avec les approches plus couplées, la complexité explose quand on scale le nombre de composants.
  - L’EDA est beaucoup **plus résilient** que les approches couplées : si un composant est en situation d’échec, il a peu de chances d’impacter d’autres composants.
    - Si on reprend l’exemple d’e-commerce :
      - Dans le cas où le composant d’e-commerce est en situation d’échec, les autres composants vont continuer à pouvoir fonctionner, mais simplement ils ne recevront plus de nouveaux events.
      - Dans le cas où par exemple le CRM est en situation d’échec, les events continueront d’arriver, et il pourra toujours rattraper son retard dès qu’il est rétabli.
      - On peut aussi prévoir une mesure pour que si le message broker est en situation d’échec, l’émetteur puisse publier les events localement, pour les mettre dans le message broker plus tard.
    - Dans un système couplé, un composant qui est en échec peut entraîner des _correlated failures_ chez les autres qui en dépendent.
      - On peut aussi avoir des _congestive collapses_ dans le cas où certains composants sont temporairement surchargés, et que les requêtes synchrones mènent à avoir des timeouts, puis à envoyer plus de requêtes.
  - L’EDA a aussi des avantages en termes de **consistance**.
    - Il favorise l’ownership d’un élément stateful par un composant unique, les autres composants recevant les notifications d’event ne pouvant pas modifier cet état.
    - En dehors du composant owner, les events sont rejouables **dans le bon ordre**, garantissant une _sequential consistency_.
  - L’EDA n’est cependant pas adaptée dans certains cas.
    - Elle n’est **pas adaptée aux interactions synchrones**.
    - Par contre, dans les cas où on peut l’utiliser, elle permet des améliorations significatives des aspects non fonctionnels.
- L’**event streaming** est un moyen d’obtenir un stream **durable** et **ordonné**, d’events **immutables**, délivrés aux consumers qui ont souscrit.
  - L’event streaming n’est pas nécessaire pour implémenter l’EDA, qui peut d’ailleurs être implémenté dans un monolith (cf. outils comme React qui sont basés sur des events).
  - En revanche l’**event streaming est pertinent** comme choix face aux solutions concurrentes (comme les message queues) **dans le cadre d’EDA distribuées**, parce qu’il a été conçu pour ça.
    - L’event streaming supporte nativement l’immutabilité des events.
    - Il supporte la garantie d’ordre des events.
    - Il supporte le fait d’avoir de multiples consumers.

## 2 - Introducing Apache Kafka

- Kafka est une plateforme d’event streaming, mais elle comprend aussi un écosystème entier qui permet l’implémentation d’EDAs.
- L’event streaming est récent comparé aux formes traditionnelles de messaging (MQ-style).
  - Il n’y a pas de standard, mais Kafka est le leader du domaine, et son fonctionnement sert de modèle pour les solutions concurrentes comme **Azure Event Hubs** et **Apache Pulsar**.
- Historiquement, Kafka a été open-sourcé en 2011 par LinkedIn, et confié à la fondation Apache.
  - Il avait été conçu notamment pour gérer les events d’activité des utilisateurs.
  - En 2019, LinkedIn opérait 100 clusters Kafka, pour un total de 100 000 topics et 7 millions de partitions.
  - Aujourd’hui Kafka est utilisé par des géants de la tech, pour des usages comme le real-time analytics, la data ingestion, le log aggregation et le messaging pour l’EDA.
    - Uber par exemple l’utilise pour gérer au total plus de 1000 milliards d’events par jour.
- Parmi les usages qui permettent l’EDA, Kafka supporte :
  - **Publish-subscribe** : un emitter publie des events, et plusieurs consumers les consomment sans que ces noeuds se connaissent.
    - C’est notamment utilisé pour des microservices avec un faible couplage.
  - **Log aggregation** : un ensemble de sources publient des events sous forme de log (soit applicatifs, soit d’infrastructure), qu’on va ensuite agréger au sein du même topic, pour le consommer dans une DB optimisée pour la lecture, comme **Elasticsearch** ou **HBase**.
  - **Log shipping** : il s’agit de streamer des logs depuis une DB master vers un topic où plusieurs DB followers vont consommer et se mettre à jour.
    - Ce pattern permet notamment d’implémenter l’event sourcing.
  - **SEDA pipelines** : le Stage Event-Driven Architecture est l’implémentation d’une pipeline d’events, où on fait une opération à chaque étape, avant d'émettre un event modifié pour l’étape suivante.
    - C’est typiquement utilisé avec les data warehouses, data lakes, le reporting et autres outils de BI.
    - On peut voir le log aggregation comme une forme de SEDA.
  - **CEP** : le Complex Event Processing consiste à un composant qui consomme des events de multiples sources, et en extrait l’information pertinente.
    - Il a souvent besoin d’un stockage pour se rappeler les patterns déjà vus et y réagir.
    - Ça peut être par exemple pour le trading algorithmique, l’analyse des menaces de sécurité, l’analyse de fraude en temps réel etc.
  - **Event-sourced CQRS** : Kafka se place entre la DB master et les DBs de projection, en permettant de les alimenter chacune au travers du concept de _consumer groups_.
    - La différence avec le log shipping c’est que le log shipping opère plutôt à l’intérieur d’un subdomain, alors que le CQRS peut aussi opérer à travers les subdomains.

## 3 - Architecture and Core Concepts

- Kafka est composé de plusieurs types de noeuds :
  - **Broker nodes** : ce sont les composants principaux de Kafka, ils s’occupent des opérations I/O et de la persistance.
    - Ces nœuds sont des processus Java.
    - Chaque partition est sous la responsabilité d’un nœud master qui peut écrire dedans, les followers en ont une copie et peuvent être lus.
      - Un même nœud peut être master pour certaines partitions, et follower pour d’autres.
      - L’ownership peut passer à un autre nœud en cas de besoin (opération spéciale qui le nécessite ou échec du nœud qui était master de la partition).
      - Concernant l’attribution de l’ownership, ça se fait d’abord en élisant un des nœuds comme _cluster controller_, puis celui-ci assigne l’ownership des partitions au gré des besoins.
    - Augmenter le nombre de nœuds constitue un moyen de scaler Kafka.
      - On peut améliorer la durability en ayant plusieurs copies de chaque partition (autant que le nombre de nœuds).
      - On peut améliorer l’availability pour les données en lecture.
  - **Zookeeper nodes** : Zookeeper est un projet open source distinct de Kafka.
    - Ses nœuds sont chargés d’élire le broker qui sera le _cluster controller_, de garantir qu’il n’y en ait qu’un, et d’en réélire un s’il n’est plus opérationnel.
    - Ils fournissent aussi diverses métadonnées à propos du cluster, par exemple l’état des différents nœuds, des informations de quotas, les access control list etc.
  - **Producers** : les applications clientes qui écrivent dans les topics.
    - Un producer communique avec Kafka via TCP, avec une connexion par broker node.
  - **Consumers** : les applications clientes qui lisent des topics.
- Le fonctionnement de Kafka se base sur des notions d’ordering venant de la théorie des ensembles (set theory).
  - Le **total ordering** consiste à avoir un ensemble d’éléments dont une **seule configuration est possible**.
    - On peut l’illustrer avec un set de nombres entiers `{ 2, 4, 6 }`. Si on enlève l’élément 4, puis qu’on le remet, il ne pourra qu’être à la 2ème place, avant le 6 et après le 2.
  - Le **partial ordering** consiste à avoir un ensemble d’éléments ordonnés selon un critère spécifique, mais dont **plusieurs configurations sont possibles** pour satisfaire le critère.
    - Par exemple, si on a des entiers qu’on veut ordonner de manière à ce que le diviseur d’un nombre soit toujours après ce nombre, et qu’on a `[ 2, 3, 4, 6, 9, 8 ]`, on peut tout autant les organiser en `[ 3, 2, 6, 9, 4, 8 ]`.
  - La notion de **causal order** indique qu’on respecte le fait que certains éléments ont une relation _happened-before_ entre eux qui est respectée, quel que soit leur ordre d’arrivée à destination.
    - Cette notion vient de l’étude des systèmes distribués (et non de la théorie des ensembles).
    - Elle est une forme de partial ordering.
    - Elle est la conséquence du fait qu’il n’y ait pas d’horloge commune à l’ensemble des nœuds d’un système distribué, et que les events peuvent arriver dans le mauvais ordre.
- Les **records** sont l’unité principale de Kafka. Ils correspondent aux events.
  - Ils sont composés :
    - D’attributs assez classiques : la _value_ qui peut être sous forme binaire, des _headers_ pour donner des métadonnées, la _partition_ associée au record, l’_offset_ par rapport aux autres records de la partition, un _timestamp_.
      - La combinaison _partition_ + _offset_ permet d’identifier un record de manière unique.
      - L’_offset_ est une valeur entière qui ne peut qu’augmenter, même s'il peut y avoir des gaps entre deux offsets qui se suivent.
    - D’un champ binaire un peu plus inhabituel qui est la _key_, et qui est utilisée par Kafka pour associer les records avec une même partition.
  - Kafka est largement utilisé pour traiter des events à l’intérieur d’un bounded context, tout comme les events entre bounded contexts.
  - Il est aussi de plus en plus utilisé en remplacement des brokers traditionnels (**RabbitMQ**, **ActiveMQ**, **AWS SQS/SNS**, **Google Cloud Pub/Sub** etc.). Dans ce cas, les records ne correspondent pas forcément à des events, et on n’est pas forcément dans de l’EDA.
- Les **partitions** sont l’unité de stream principale qui contiennent les records.
  - Les records d’une même partition sont _totally ordered_.
  - Les records publiés dans une partition par un même producer seront donc aussi _causally ordered_ (la précédence respectée).
    - En revanche, si plusieurs producers publient dans la même partition sans eux-mêmes se synchroniser entre eux, les records de chaque producer seront causally ordered pour un même producer, mais ne le seront pas entre les producers (ça dépendra de qui l’a emporté pour publier plus vite).
    - Publier dans plusieurs partitions ne règle pas ce problème : les records de chaque producer ne seront pas causally ordered. Si on veut un tel ordre, il faut un seul producer.
- Les **topics** sont des unités logiques qui regroupent des partitions.
  - Vu qu’il s’agit d’une union de partitions qui sont chacune _totally ordered_, les topics peuvent être considérés comme _partially ordered_.
    - On peut donc écrire dans les records de plusieurs partitions en parallèle, et n’assurer que l’ordre des records dans chaque partition.
  - On peut indiquer à la main la partition vers laquelle on veut publier un record, mais généralement on indique la key, qui sera hashée pour correspondre avec une partition donnée.
    - Dans le cas où on **réduit le nombre de partitions**, les messages peuvent être **détruits**.
    - Dans le cas où on **augmente le nombre de partitions**, on peut **perdre l’ordre** qu’on voulait conserver avec nos keys, puisque la fonction de hash redirigera vers une autre partition.
    - Même si on a un nombre de partitions supérieur au nombre de keys, il est possible que deux keys mènent vers la même partition.
      - La seule chose qui est garantie, c’est qu’avec la même key, et si le nombre de partitions ne change pas, l’ordre sera respecté.
- Un consumer peut souscrire à un topic en tant que membre d’un **consumer group**, et bénéficier d’un mécanisme de **load balancing** avec d’autres consumers.
  - Le 1er consumer qui souscrit se voit assigner toutes les partitions. Quand un 2ème consumer souscrit au topic, il se voit assigner environ la moitié des partitions qui étaient assignées au 1er. et ainsi de suite.
  - Les consumers ne peuvent que lire les events sans impact sur eux.
    - Une des conséquences c’est qu’on peut en ajouter beaucoup sans stresser le cluster. Et c’est une des différences par rapport aux brokers classiques.
    - Ils maintiennent les offsets de là où ils en sont pour chacune des partitions qu’ils sont en train de lire.
    - Les consumers de différents consumer groups n’ont pas d’impact les uns sur les autres.
  - Kafka s’assure qu’il n’y a **qu’un consumer d’un même consumer group** qui peut lire dans une **même partition**.
    - Si un consumer ne lit plus de messages jusqu’à dépasser un timeout, Kafka assignera ses partitions à un autre consumer, considéré comme sain, du même groupe.
- Pour que Kafka puisse réassigner une partition à un autre consumer en gardant le bon offset, ou redonner le bon offset à un consumer qui se reconnecte après s’être déconnecté, il faut que **les consumers communiquent leurs offsets à Kafka**.
  - On appelle ça _committing offsets_.
  - On peut avoir un contrôle sur le **moment où on va faire ce commit**, et donc agir sur la **garantie de delivery** des messages, c’est-à-dire le fait qu’ils soient intégralement traités.
    - On peut passer d’une stratégie _at-most-once_ à une stratégie _at-least-once_ en faisant le commit après l’exécution de la callback au lieu du moment où le message est pris par le consumer.
    - Par défaut, Kafka va faire un commit toutes les 5 secondes, sauf si un record est toujours en train d‘être exécuté, auquel cas il attendra la prochaine occasion 5 secondes plus tard.
      - On peut régler cette durée de 5 secondes à une autre valeur avec la configuration `auto.commit.interval.ms`.
      - Ça implique que si le record est exécuté, et que dans les quelques secondes après, le cluster bascule la partition sur un autre consumer, on risque de ne pas avoir commité et de réexécuter la callback du record dans le nouveau consumer.
      - Si on veut avoir le contrôle sur le moment exact où on veut faire le commit, on peut désactiver le commit automatique (configuration `enable.auto.commit` à `false`), et le faire à la main dans le consumer.
  - Le commit peut se faire via un canal in-memory asynchrone pour ne pas bloquer le consumer, avec la possibilité de fournir une callback qui sera exécutée par Kafka quand le commit aura été pris en compte
    - Ou alors le consumer peut aussi utiliser un appel synchrone pour le commit.
  - Un cas classique est de traiter les records avec une stratégie _at-least-once_ par batch, qu’on appelle _poll-process loop_ :
    - Le consumer garde un buffer de records qu’il prefetch en arrière-plan.
    - Il traite les records un par un (ou parfois en parallèle avec un pool de threads si c’est OK d’un point de vue business).
    - Quand on arrive au dernier record, il fait le commit de l’offset.
    - Puis il prend le batch suivant et recommence.
- Même si c’est moins courant, il est possible de souscrire un consumer **sans qu’il soit membre d’un consumer group**.
  - Dans ce cas, il ne bénéficiera pas des divers mécanismes associés aux consumer groups : load balancing, rebalancing en cas d’échec, détection de l’échec par inactivité, persistance de l’offset.
    - Il devra indiquer les couples topic/partition auxquels il souscrit, et devra persister ses propres offsets lui-même dans un store.
  - Il peut y avoir deux cas d’usages :
    - Le besoin d’avoir vraiment le contrôle sur la manière de consommer les messages, en stockant soi-même son offset etc.
      - Mais ce cas d’usage est très rare, et difficile à implémenter correctement.
    - Un consumer éphémère qui est là juste pour monitorer ou débugger un topic, sans avoir besoin de persister d’offsets.
      - C’est ce que fait par exemple l’outil Kafkadrop qui permet de visualiser les messages présents dans les partitions via une interface web : à chaque fois il attache un consumer sans groupe.

## 4 - Installation

- Il y a 4 méthodes pour installer Kafka (et Zookeeper) :
  - En utilisant les images Docker.
    - Si on choisit une autre méthode que Docker, on aura juste besoin d’avoir d’avoir un JDK d’installé.
    - La méthode Kafka dans Docker est la plus immédiate pour avoir Kafka qui tourne, mais elle est aussi connue pour être difficile à configurer si on veut personnaliser.
  - En utilisant un package manager (yum, apt, homebrew etc.)
  - En clonant le dépôt git et en installant depuis les sources.
  - En téléchargeant des binaires sur le site de Kafka.
    - Il suffit de télécharger un tar.gz et de le désarchiver, pour obtenir les exécutables de Kafka qu’on peut lancer avec notre JDK.
    - Le livre part là-dessus.
- La configuration de Kafka peut se faire en changeant les fichiers de conf dans le dossier `config/`.
  - On peut voir les configs prises en compte dans les logs, à chaque fois qu’on démarre Kafka.
