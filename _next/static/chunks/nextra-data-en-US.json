{"/":{"title":"Introduction","data":{"":"These are my detailed notes taken while reading computer science related books. Most of the notes are in french.","complete-notes#Complete notes":"[fr] Designing Data-Intensive Applications - Martin Kleppmann - 2016\n[fr] The Design of Web APIs - Arnaud Lauret - 2019\n[fr] Unit Testing: Principles, Practices and Patterns - Vladimir Khorikov - 2020\n[fr] Learning Domain-Driven Design - Vlad Khononov - 2021\n[fr] Continuous Discovery Habits - Teresa Torres - 2021","incomplete-notes#Incomplete notes":"[fr] Get Your Hands Dirty on Clean Architecture - Tom Hombergs - 2019"}},"/notes/continuous-discovery-habits":{"title":"Continuous Discovery Habits","data":{"":"","part-i---what-is-continuous-discovery#Part I - What is continuous discovery":"","1---the-what-and-why-of-continuous-discovery#1 - The what and why of continuous discovery":"Les équipes produit font deux activités distinctes : la discovery où il s’agit de décider de ce qui sera fait, et la delivery où il s’agit d’implémenter effectivement ces choses.\nLa plupart des entreprises investissent lourdement sur la delivery, mais peu sur la discovery pour bien choisir quoi construire.\n\n\nUn peu d’historique :\nDans les temps “anciens” la discovery se faisait exclusivement par les business leaders, qui décidaient de budgéter une fois par an. Les projets étaient à la fois hors délai, et servaient mal les besoins des utilisateurs.\nEn 2001, un groupe de développeurs a écrit le manifeste agile, qui poussait notamment l’idée de cycles courts avec un feedback rapide. Leur idée s’est répandue, mais les business leaders avaient du mal à lâcher la prise de décision sur ce qui doit être produit (discovery).\nPetit à petit, on en est arrivé à l’idée que la discovery doit être de la responsabilité des product managers plutôt que des business leaders, puis enfin de l’équipe produit plutôt que du product manager.\nOn a aussi raccourci le cycle de discovery jusqu’à co-créer la discovery avec le client sur une base continue plutôt que de valider les idées qu’on a au préalable. C’est ça qu’on appelle la continuous discovery.\nNDLR : on a ici une idée similaire à celle du DDD à propos de la collaboration permanente avec les domain experts, qui apportent leur expertise du domaine pendant que l’équipe produit apporte son expertise technique et d’analyse dans le cadre d’une co-création.\n\n\n\n\nQuand on parle de l’équipe produit, on parle ici de tous ceux qui sont en charge de créer le produit : ça inclut les product managers mais aussi les software engineers, les designers et tout autre rôle qui peut être rattaché à la construction de features pour un produit donné (product analyst, user researcher, customer success etc.).\nLe framework proposé dans ce livre est à destination d’un product trio composé au moins d’un product manager, d’un designer, et d’un software engineer.\nLe product trio peut être un quatuor ou même un quintuor pour être plus représentatif, mais il faut noter qu’il y a ici un trade-off : plus il y a de monde et plus on est inclusif sur les sensibilités permettant d’obtenir une discovery fine, mais en même temps plus on a va aussi prendre du temps pour prendre les décisions. 3 semble un nombre raisonnable la plupart du temps.\n\n\nIl y a 6 mindsets nécessaires pour adopter cette méthode :\n1 - Outcome-oriented : on s’intéresse à l’outcome (impact sur les utilisateurs et le business) plutôt qu’à l’output (nombre de features produites).\n2 - Customer-centric : les besoins de l’utilisateur sont un enjeu central, au même niveau que les enjeux business.\n3 - Collaborative : accepter la nature cross-fonctionnelle des équipes produit. Plutôt que le PM décide, le designer design, et le développeur code, on a chacun qui apporte son expertise au processus de discovery.\n4 - Visual : utiliser des supports visuels pour tirer parti de nos capacités spatiales de raisonnement.\n5 - Experimental : adopter une approche scientifique expérimentale en listant les hypothèses et en récoltant des preuves.\n6 - Continuous : plutôt que de voir la discovery comme quelque chose qu’on fait au début du projet, il faut la voir comme quelque chose qu’on fait tout au long du processus de développement, pour éviter de faire fausse route régulièrement.\n\n\nLes techniques modernes de discovery sont en fait déjà existantes dans de nombreuses entreprises. Ce qui est plus rare c’est l’aspect structuré et continu de ces techniques. Par conséquent la définition de la continuous discovery est la suivante :\nAt a minimum, weekly touchpoints with customers\nBy the team building the product\nWhere they conduct small research activities\nIn pursuit of a desired outcome","2---a-common-framework-for-continuous-discovery#2 - A common framework for continuous discovery":"Il y a en général une tension entre l’outcome business et la satisfaction de l’utilisateur, avec un risque de privilégier le 1er au détriment du 2ème, ce qui ne sera pas viable sur le long terme.\nExemple : en 2016, la banque Wells Fargo connaît un scandale. Les employés avaient fait accepter des services supplémentaires aux clients de manière frauduleuse à cause des objectifs uniquement business, sans couplage à la satisfaction utilisateur.\nPlutôt qu’avoir comme objectif d’augmenter à tout prix les revenus, si l’objectif était par exemple d’obtenir des clients qui veulent ouvrir plus de comptes, on aurait pu éviter le problème.\n\n\nAutre exemple : quand on navigue sur un site et qu’on est bombardé de pub désagréable.\n\n\nSouvent, les product trios prennent un outcome business, et commencent directement à générer des idées. Ils sautent l’étape importante du framing de l’opportunity space.\nOn va appeler dans la suite opportunity space l’ensemble des besoins utilisateur, leurs pain points, et leurs désirs.\nLes désirs sont importants aussi, on ne peut pas les réduire aux besoins. Exemple : manger une glace ou aller à Disneyland sont des désirs.\n\n\nIl y a en fait de nombreuses manières d’atteindre notre outcome, et celles-ci dépendent :\ndu framing de notre space opportunity (la manière dont on construit l’espace des problèmes et des désirs).\ndu choix de l’opportunity pour arriver à l’outcome.\n\n\nIl faut toujours examiner plusieurs opportinuties, et voir les conséquences de chacune avant de choisir.\n\n\nTeresa propose de structurer la discovery au travers d’un Opportunity Solution Tree (OST).\nIl s’agit d’une représentation graphique sous forme d’arbre avec l’outcome en haut, les opportunities possibles en dessous, les solutions possibles associées à chaque opportunity, et les assuption tests qui permettent de valider chaque solution. outcome ← opportunity ← solution ← assumption test\nCe modèle permet :\nDe résoudre la tension entre les besoins du customer et les besoins business.\nOn choisit l’outcome business et on liste ensuite toutes les opportunities qui permettent d’y parvenir, pour être sûr de satisfaire forcément les deux à la fois.\n\n\nD’aider à construire une connaissance partagée pour le product trio.\nNous avons tendance en général à sauter sur la première solution qui nous vient en tête, et à la défendre ensuite contre l’avis des autres. Ces crispations finissent dans une situation où PM décide parce qu’il a le dernier mot, au lieu d’une vraie collaboration.\nEn gardant sous les yeux une représentation graphique de toutes les possibilités, on s’évite d’en considérer une comme étant la “notre” à défendre à tout prix.\n\n\nD’aider le product trio à adopter un continuous mindset.\nVu qu’on a dans notre OST de plus petites opportunities qui permettent de réaliser une plus grosse opportunity, ça nous permet de délivrer de la vraie valeur à chaque sprint, plutôt qu’un bout de quelque chose de gros qui est censé délivrer de la valeur plus tard => On est vraiment agiles.\n\n\nDe prendre de meilleures décisions.\nVu qu’on a sous les yeux les possibilités, y compris celles qu’on a déjà explorées, on évite de tomber dans des biais qui vont fausser notre décision.\nPar exemple : à chaque fois qu’on a une nouvelle demande client, tomber dans le travers de se demander si on arrête tout pour l’implémenter ou non, au lieu de garder la vue d’ensemble.\n\n\nA ce propos, elle conseille un livre à lire juste après le sien : Decisive de Chip et Dan Heath.\n\n\nD’obtenir des cycles d’apprentissage rapides.\nOn dit souvent que les PM doivent définir le problème, et les développeurs apporter la solution. C’est une erreur. Pour être efficace, il faut que les mêmes personnes (product trio) soient en charge des deux.\nl’OST permet de visualiser les deux, et à chaque fois qu’une solution est implémentée et ne permet pas de satisfaire l’opportunity, on peut la garder en tête tout en sachant pourquoi ça n’avait pas marché, et pouvoir basculer sur une solution mieux pensée.\n\n\nDe construire une confiance dans le fait de savoir quoi faire ensuite.\nOn peut facilement se rendre compte qu’on n’a pas assez d’opportunities et faire des interviews client, ou pas assez de solutions et faire des ateliers pour générer des idées.\nLes meilleures équipes ne travaillent pas de haut en bas (définir un outcome clair, puis créer l’opportunity space, puis chercher les solutions, et enfin définir des assumption tests), mais sur tout l’arbre à la fois : chaque semaine ils affinent chaque élément de l’arbre, et travaillent sur plusieurs éléments en parallèle pour le faire évoluer.\n\n\nDe permettre un rapport aux stakeholders plus simple.\nMême avec de la bonne volonté, la direction a tendance à parfois revenir sur des demandes d’output, surtout en période de stress. Pour éviter ça il faut les garder au jus, avec la bonne quantité d'informations.\nIl ne faut ni leur donner trop d’informations, ni leur donner juste les conclusions du product trio. Pour qu’ils se sentent impliqués et puissent donner un feedback, il faut leur donner les principaux éléments qui ont été considérés et choisis ou rejetés. L’OST permet de servir de support visuel pour ça.","part-ii---continuous-discovery-habits#Part II - Continuous discovery habits":"","3---focusing-on-outcomes-over-outputs#3 - Focusing on outcomes over outputs":"L’idée d’utiliser les outcomes plutôt que les outputs existe depuis des décennies, récemment elle a été repopularisée avec les OKR chez Google.\nLes outcomes permettent de donner plus d’autonomie à l’équipe produit, en leur laissant trouver la bonne chose à faire pour régler un besoin business ou une problématique utilisateur, plutôt que leur donner une roadmap de features.\nIl existe 3 types d’outcomes :\nLes business outcomes mesurent l’avancement du business.\nExemple : on a une boutique de vente de nourriture personnalisée pour chien, on veut augmenter la rétention des clients mesurée sur 90 jours.\nÇa peut être soit des aspects financiers (rentrée d’argent, réduction de coûts), soit des initiatives stratégiques (parts de marché ou augmentation de ventes dans une catégorie spécifique, rétention d’utilisateurs etc.).\nCe sont en général des lagging indicators, c’est-à-dire qu’ils vont indiquer le résultat de ce qui s’est déjà produit depuis un moment.\nIls vont impliquer en général une coordination entre plusieurs fonctions business (produit, CSM, vente etc.).\n\n\nLes product outcomes mesurent à quel point le produit fait avancer le business.\nExemple : augmenter la valeur perçue de la nourriture pour chien, et augmenter le nombre de chiens qui aiment cette nourriture sont deux indicateurs qui permettront d’atteindre une meilleure rétention des clients à 90 jours.\nCe sont en général des leading indicators, qu’on peut utiliser pour itérer rapidement en fonction des résultats qu’on obtient semaine après semaine.\nIls sont sous la responsabilité de l’équipe produit, et représentent des objectifs sur lesquels l’équipe produit peut agir seule. C’est avec ces outcomes qu’elle avancera le mieux.\n\n\nLes traction metrics mesurent l’utilisation d’une feature particulière.\nExemple : augmenter le nombre de propriétaires de chiens qui utilisent la fonctionnalité calendrier.\nOn est bien ici sur de l’outcome, mais qui est limité à un output spécifique. Il y a donc un risque que l’équipe produit soit coincée si l’output choisi dont on mesure l’utilisation n’était pas le bon.\nEn général l’équipe produit avancera mieux avec un product outcome, mais il existe 2 cas où les traction metrics peuvent être utiles :\nQuand on a un product trio plutôt junior, limiter leur responsabilité en fixant un output particulier à optimiser peut être pertinent.\nSi on a un produit déjà mature (dont on a déjà fait pas mal de discovery pour défricher), et qu’on est sûr qu’une feature particulière est très utilisée, on peut vouloir l’optimiser et donc faire de la discovery avec une traction metric.\n\n\n\n\n\n\nLe choix des outcomes de l’équipe produit est le résultat d’une négociation entre le product trio et le product leader.\nLe product leader (par exemple CPO) amène la connaissance de ce qui est utile à ce moment là pour l’organisation, alors que le product trio amène la connaissance des clients et de la technique.\nLa négociation porte sur des outcomes et pas des outputs ou des solutions, et ne doit pas permettre au product leader de les réduire à des traction metrics. Il peut par contre demander à restreindre le champ, par exemple “augmenter le nombre de chiens qui aiment la nourriture dans telle ou telle région”.\nLes deux vont convenir d’une métrique à augmenter d’une certaine valeur, et en fonction de celle-ci, éventuellement aussi des ressources (software engineers) pour mener à bien cet outcome.\nLe fait que la participation des équipes à la définition de leurs outcomes apporte de meilleures performances est appuyé par des études.\n\n\nOn peut avoir des performance goals qui sont des objectifs mesurables et challenging (exemple : augmenter l’engagement de 10%), et des learning goals qui sont des objectifs non mesurés (exemple : découvrir les opportunités qui drivent l’engagement).\nLes études ont montré que les performance goals étaient plus efficaces quand la stratégie était déjà connue et qu’on avait identifié une métrique pertinente grâce à une précédente discovery, mais que les learning goals étaient plus efficaces dans le cas contraire.\n\n\nQuelques anti-patterns à éviter :\nAvoir trop d’outcomes en même temps : on s’éparpille en avançant un peu sur chacun des outcomes, mais on n’a pas de gros impact sur l’un d’entre eux. En général, le mieux est de chercher à satisfaire un outcome à la fois.\nFaire du ping pong entre plusieurs outcomes : on a tendance à basculer d’outcome en outcome à cause d’une culture du firefighting où un besoin client en chasse un autre. Rester sur le même outcome pendant plusieurs trimestres permet de profiter de ce qu’on a appris dans le 1er et d’être vraiment efficace sur les 2ème et 3ème trimestres.\nDéfinir des outcomes individuels plutôt que des outcomes pour le product trio : on a parfois des objectifs individuels pour obtenir de la rémunération, portant sur des outcomes business pour le PM, des outcomes UX pour le designer, et des outcomes techniques pour le tech. Ces objectifs empêchent le bon fonctionnement du product trio. Il vaut mieux des** outcomes d’équipe**.\nChoisir un output au lieu d’un outcome :\nVu qu’on est habitués à utiliser des outputs, on va avoir tendance à revenir à ça. Le bon critère à regarder pour voir si on est plutôt vers l’outcome ou l’output c’est** l’impact**.\nUn bon début c’est de s’assurer que notre outcome représente un nombre.\n\n\nSe focus sur un seul outcome au détriment de tous les autres : il faut en choisir un à la fois à faire avancer, mais il ne faut pas oublier de monitorer les autres outcomes pour s’assurer qu’ils ne se dégradent pas.\nPar exemple l’acquisition de clients, en vérifiant en même temps que leur satisfaction ne baisse pas.","4---visualizing-what-you-know#4 - Visualizing what you know":"Quand le product trio commence avec un outcome, il va d’abord créer un experience map pour représenter le workflow lié à cet outcome, et rassembler les connaissances actuelles du trio.\nIl faut **définir le scope **plus ou moins large pour notre experience map : un scope très large permettra d’explorer des marchés adjacents, et un scope plus étroit permettra de se cantonner à notre produit. En général on cherche quelque chose d'intermédiaire.\nExemple : On a une application de streaming, et notre outcome c’est d’augmenter le nombre de minutes regardées. Un scope large serait “Comment est-ce que les clients se divertissent en général ?”, et un scope étroit serait “Comment est-ce que les clients se divertissent avec notre service ?”. Quelque chose d’intermédiaire peut être “Comment est-ce que les clients se divertissent avec de la vidéo ?”.\n\n\nL’experience map doit représenter les étapes du point de vue du customer.\nPar exemple, si notre scope c’est “Comment est-ce que les clients se divertissent avec de la vidéo ?”, on peut commencer par imaginer quelqu’un qui nous partage le nom d’un contenu vidéo, puis on fait une action de recherche. On peut éventuellement être face à des problèmes pour trouver le contenu, et ainsi de suite.\n\n\nIl contient uniquement du contenu dessiné. Les mots étant moins précis, moins spécifiques que les images, on va dessiner des boîtes, des flèches, de petits pictogrammes etc. pour constituer nos étapes. Ne pas savoir dessiner n’est pas un problème.\nBien que ce soit contre-intuitif, il est plus efficace de construire l’experience map d’abord chacun de son côté pour ensuite mettre en commun. Si on travaille ensemble dès le début, la dynamique de groupe va faire que certains ne vont pas s’exprimer pleinement.\nPour la mise en commun :\nOn va d’abord chacun expliquer notre map aux autres. On pose des questions pour clarifier, mais le but n’est pas de “défendre” sa vision.\nEnsuite on construit un schéma commun qui va être la combinaison des maps initiaux. Il n’est pas question d’en choisir un des trois et d’avancer avec ça.\nOn récupère tous les noeuds de chacun des maps. Ce sont les événements qui représentent des étapes.\nOn crée un nouveau map dans lequel on met tous ces noeuds.\nOn fusionne les noeuds similaires.\nOn relie les noeuds par des flèches. Pas seulement le happy path mais aussi les chemins d’échec, qui vont créer des frustrations chez le client.\nOn ajoute du contexte sur ce que pensent, ressentent les clients, toujours sous forme visuelle.\n\n\n\n\nQuelques anti-patterns à éviter :\nS’embourber dans de longs débats sur des détails : quand c’est le cas, il vaut mieux dessiner ce que chacun veut dire pour faire ressortir clairement le désaccord (ou bien souvent l’accord parce qu’on était en quiproquo), et passer à la suite.\nUtiliser des mots à la place des dessins : les dessins font appel à une autre partie de notre cerveau et sont ici plus efficaces. Avec le temps, on se débrouille pour dessiner de moins en moins mal.\nAller à la suite en considérant que notre map représente la vérité : c’est juste un brouillon, on va le retoucher un grand nombre de fois à mesure qu’on parle aux clients.\nOublier d’affiner notre map commune à mesure que nos connaissances augmentent : même en assistant aux mêmes réunions avec les clients, si on ne refait pas le point sur la map régulièrement, chaque membre du trio retiendra des choses différentes, et aura en tête une experience map différente.","5---continuous-interviewing#5 - Continuous interviewing":"Bien souvent, les clients ne savent pas à l’avance les solutions qu’ils veulent.\nEn parlant aux clients, notre but est d’obtenir une meilleure compréhension de l’opportunity space, pas de leur demander quelle feature ils aimeraient (l’espace des solutions).\n\n\nIl est important que le client nous parle de quelque chose qui l’intéresse. Donc la première chose à obtenir de lui c’est quels sont les besoins, pain points, et désirs qui comptent le plus pour lui. On pourra ensuite choisir des sujets à aborder parmi ceux-là.\nOn est parfois déçu de ne pas pouvoir faire parler le client de ce qu’on veut, ou même parfois d’avoir des clients non coopératifs. Ce n’est pas très grave : on en aura au moins un autre la semaine prochaine.\n\n\nEn posant des questions directes aux gens, on peut bien souvent obtenir une réponse fausse, parce que le cerveau a tendance à rationaliser pour reconstruire des informations manquantes, ou transformer les choses pour les mettre en cohérence avec la vision globale de la personne, la perception qu’elle a de son identité etc.\nCeci est démontré par des décennies de recherches sur la manière d’interroger les personnes.\nExemple : Teresa pose à une personne la question de savoir quel critère elle utilise pour s’acheter des jeans. Elle répond “le fit” comme critère numéro 1. Puis Teresa lui demande de lui raconter la dernière fois qu’elle a acheté un jean : c’était sur Amazon. Donc les vrai critères c’est le prix, la marque, la disponibilité rapide. Le fit n’était pas le critère numéro 1 pour l’achat.\n\n\nEn fait il va falloir distinguer les research questions qui sont les questions directes qu’on poserait pour obtenir des informations sur l’opportunity space, et les interview questions qui sont celles qu’on va réellement poser.\nLes interview questions vont consister à demander au client de nous raconter des cas concrets de choses qu’il a pu faire sur le sujet qui nous intéresse.\nExemple : Au lieu de demander “Quel critère utilise-tu pour acheter des jeans”, on demande “Raconte-moi la dernière fois que tu as acheté des jeans”.\n\n\nOn peut aussi faire varier le scope en fonction de ce qu’on recherche, de la même manière qu’avec l’experience map.\nExemple : pour le service de streaming.\nRacontez-moi la dernière fois que vous avez utilisé notre service de streaming. => en apprendre plus sur notre produit pour l’améliorer.\nRacontez-moi la dernière fois que vous vous êtes diverti en regardant du streaming. => comprendre comment on se place par rapport aux concurrents.\nRacontez-moi la dernière fois que vous-vous êtes diverti. => découvrir avec quoi la catégorie de notre produit est en compétition (exemple théâtre etc.).\n\n\n\n\n\n\nIl faut aider le client à raconter son histoire :\nLes gens sont habitués à parler autant que leur interlocuteur, si on veut les faire parler plus que nous, il faut leur dire dès le début qu’on veut qu’ils racontent leur histoire avec le plus de détail possible.\nIl faut l’aider à avancer dans les étapes :\nNoter les personnages clé, les challenges rencontrés etc. pour pouvoir le relancer.\nUtiliser notre experience map pour suivre son histoire à travers nos propres nœuds, et éventuellement le relancer sur des étapes dont il n’aurait pas parlé.\n\n\nSi le client généralise une réponse (“Quel challenge avez-vous eu ?” “En général les challenges sont…”) alors il faut lui demander de revenir à l’exemple concret. On a vu que la généralisation menait à une vision faussée de la réalité.\n\n\nPour pouvoir se rappeler des interviews sur la durée, il faut résumer le résultat de chaque interview dans un interview snapshot, qui est une page résumant l’interview.\nSi possible il faut y mettre une photo de la personne qu’on a interviewé, ou une image qui va nous faire penser à elle.\nIl faut aussi y mettre une phrase qui nous a marqué, pour pouvoir nous en rappeler.\nNDLR : un peu comme le channel #thingsclientssaid.\n\n\nLa partie quick facts permet d’indiquer de quel type de client il s’agit, en fonction de la catégorisation qu’on souhaite faire de nos clients. Ca permettra ensuite de comparer ce qui a été remonté avec les clients de la même catégorie.\nEnsuite on a de la place pour noter les opportunities.\nSi le client donne des solutions (features) qu’il aimerait, il faut essayer d’en extraire l’opportunity. Par exemple en posant la question : Qu'est-ce que cette feature vous apporterait ?\nIl faut s’efforcer de formuler l’opportunity avec les mots du client, pour que ça représente son point de vue, et non le point de vue de notre entreprise.\n\n\nOn a aussi une place pour noter des insights qui ne sont pas des opportunities (ni des besoins, ni des pain points, ni des désirs).\nEt enfin on a une place pour dessiner la story racontée par le client, avec des boîtes et des traits, sur le même modèle que notre experience map.\n\n\nIl est très important de** faire des interviews de clients chaque semaine**. Commencer ou arrêter une habitude est beaucoup plus difficile que d’en maintenir une.\nLe plus difficile pour ça c’est d’automatiser le process de recrutement des personnes qu’on va interviewer. On a parfois des urgences business, des absents dans l’équipe etc. et dans ces moments on a besoin d’avoir son interview de la semaine déjà bookée sans rien faire.\nParmi les manières de recruter :\nLe plus simple est d’avoir un petit bandeau qui s’affiche dans notre application, et qui dit “Est-ce que vous auriez 20mn à nous accorder pour améliorer notre service, en échange de 20€ ?”.\nÇa marche bien si on a beaucoup de trafic, dans ce cas on peut demander le numéro de la personne avec un formulaire.\nSi on a moins de trafic, à la place du numéro on peut proposer un service de scheduling en ligne pour que le client puisse directement réserver un créneau avec nous.\nSi on n’a pas de trafic du tout, on peut toujours utiliser de la publicité qui redirige vers une landing page, où il y aura le formulaire proposant de nous aider.\n\n\nUne autre solution est de demander aux** équipes customer facing** (CSM, sales etc.).\nD’abord on leur demande de rejoindre un de leurs calls 5 mn à la fin, pour prendre une story. Le but est de mettre un pied dans la porte pour qu’ils disent oui facilement.\nUne fois qu’on est plus à l’aise, on peut demander aux équipes CSM de programmer des interviews pour nous. On leur donne des “triggers” qu’on met à jour régulièrement.\nPar exemple : si un client pose une question sur la feature X, programme lui un interview avec nous.\n\n\n\n\nEnfin, si on a une audience trop petite ou trop difficile à aller chercher, on peut aussi constituer un customer advisory board, c’est-à-dire convaincre un groupe de clients de participer à des meetings réguliers avec nous.\nL’avantage c’est que ça permet de suivre l’évolution de leurs problématiques dans le temps. Le désavantage potentiel c’est qu’on risque d’avoir un échantillon pas forcément représentatif.\n\n\n\n\n\n\nIl faut que les interviews soient faites par l’ensemble du product trio.\nSi l’un des membres se retrouve à être “la voix du customer”, il acquerra trop de pouvoir dans le groupe, et déséquilibrera le processus de décision dans la discovery, avec un “Oui mais c’est ça que le client veut”.\nLe product trio est composé de personnes diverses justement pour prendre les meilleures décisions possibles, chacun amenant sa sensibilité.\n\n\nQuelques anti-patterns à éviter :\nCompter sur une seule personne pour le recrutement ou le fait de faire les interviews : si elle n’est pas là, l’habitude d’interviewer s’arrête. Il faut plutôt que chaque membre du trio sache recruter et interviewer.\nPoser des questions “Who, What, Why, How, When” : ça fait de longues interviews et donne des données non fiables. Il faut plutôt préparer des research questions, et des interviews questions correspondantes qu’il faudra poser.\nFaire des interviews seulement quand on en a besoin : l’habitude en elle-même a beaucoup de valeur. Et en plus on aura des réponses rapides.\nPartager ce qu’on apprend avec le reste de l’équipe en envoyant des pages de notes ou des enregistrements des interviews : il vaut mieux utiliser les interview snapshots. On ne peut pas demander au reste de l’équipe de consacrer autant de temps que nous aux interviews.\nS’arrêter pour synthétiser les 6 à 12 dernières interviews : si on est dans un mode continu, on synthétise aussi en continu avec les interview snapshots, pas par batch.","6---mapping-the-opportunity-space#6 - Mapping the opportunity space":"Avoir un backlog d’opportunities est un bon début, mais la priorisation est compliquée parce que le sizing de chaque opportunity est différent, et qu’elles s’overlap parfois. => L’OST répond à ces problématiques.\nIl faut essayer de trouver plus de sub-opportunities à mesure qu’on fait des interviews, pour deux raisons :\nÇa permet de répondre à des opportunities qui paraissent trop grosses.\nÇa permet de délivrer de la valeur en continu, en suivant la philosophie agile.\n\n\nDans l’OST, à propos de la relation entre les noeuds parents / enfants :\nLe noeud enfant permet d’avancer sur la problématique du parent.\nDeux nœuds enfants du même niveau doivent pouvoir être résolus chacun de leur côté sans impacter l’autre.\nExemple : Dans le cadre du streaming, “Je ne sais pas comment chercher un show particulier” permet de faire avancer la problématique parente “Je ne trouve rien à regarder”, sans faire avancer l’autre enfant du même niveau “J’ai terminé les épisodes de mon show favori”.\n\n\nIl y a deux techniques pour que les opportunities du même niveau soient indépendants, à chaque fois il faut les associer avec des moments distincts dans le temps :\nOn peut reprendre notre experience map, et associer chaque nœud à un nœud d’opportunity top level dans l’OST.\nSi notre experience map n’est pas encore bien formée, on peut parcourir nos interview snapshots, et essayer de trouver des nœuds qui reviennent dans le dessin de la story. Une fois qu’on a ces nœuds on peut les associer avec les noeuds top level de notre OST.\n\n\nUne fois qu’on a nos nœuds top level de l’OST, on peut y ajouter nos opportunities à partir de nos interview snapshots.\nAvant d’ajouter chaque opportunity, on revérifie bien qu’elle permet d’avancer sur notre outcome, et que c’est vraiment une opportunity.\nOn les place d’abord simplement en dessous de l’opportunity top level correspondante.\n\n\nOn va ensuite, branche top level par branche top level, ajouter de la structure à notre OST :\nOn va trouver des opportunities qui se ressemblent, et on va leur chercher une opportunity parente.\nSi l’opportunity parente n’existe pas, c’est OK de la créer, vu que les enfants eux sont bien issus de vrais interviews.\nSi deux opportunities sont vraiment les mêmes formulées différemment, on peut les regrouper en une.\nOn va ensuite chercher à regrouper nos mini-arbres ensemble avec des parents communs, jusqu’à arriver à l’opportunity top level.\n\n\nLe framing de l’opportunity space est une étape importante.\nSi la construction de l’OST nous prend 30mn, c’est qu’on l’a fait un peu vite fait. Mais il ne faut pas non plus y passer des heures et des heures.\nC’est un processus itératif, on va de toute façon remodeler l’OST de nombreuses fois.\n\n\nQuelques anti-patterns à éviter :\nCréer les opportunities depuis la perspective de l’entreprise : on peut se demander à chaque nœud de l’OST : est-ce que j’ai entendu un client dire ça ? Ou si on a dû le créer : est-ce qu'un client dirait ça ?\nExemple : un client ne dira jamais “J’aurais bien aimé avoir plus d’abonnements au service de streaming”, mais il pourra dire “J’aurais aimé avoir plus de contenu intéressant”.\n\n\nAvoir des sous-arbres verticaux : si on est dans ce cas, c’est\nsoit qu’on a deux opportunities qui sont en fait à peu près les mêmes, et qu’on peut reformuler en une.\nsoit il nous manque des opportunities de même niveau parce que l’opportunity enfant n’est pas suffisante pour résoudre le parent. Dans ce cas, il faudra trouver d’autres opportunities pour ne pas rester sur un sous-arbre vertical.\n\n\nDes opportunities ont plusieurs parents : si on a bien nos opportunities top level qui sont vraiment des moments différents dans le temps ça ne doit pas arriver. Il est possible que l’opportunity problématique soit formulée de manière trop générale.\n**Des opportunities non spécifiques **: des opportunities comme “J’aimerais que ce soit facile à utiliser” ne sont pas assez spécifiques, on peut les rendre plus spécifiques, par exemple “J’aimerais que voir un show soit plus facile”.\nDes opportunities qui sont des solutions déguisées : Il faut qu’il y ait plusieurs solutions possibles à une opportunity, sinon c’est déjà une solution.\nPar exemple : “Je veux passer rapidement les pubs” est en fait une solution. L’opportunity pourrait être “Je n’aime pas voir les pubs”, et auquel on pourra répondre par les passer, mais aussi par “Faire des pubs plus attrayantes” ou encore “Avoir un abonnement sans pub”.\n\n\nCapturer des sentiments comme des opportunities : si on a un sentiment qui est donné par le client, c’est qu’il y a une opportunity pas loin. Mais il faut alors** chercher la cause** de ce sentiment pour avoir l’opportunity.\nExemple : “Je me sens frustré” => Pourquoi => “Je déteste retaper mon mot de passe à chaque achat”.","7---prioritizing-opportunities-not-solutions#7 - Prioritizing opportunities, not solutions":"Il faut chercher à répondre à une seule opportunity à la fois.\nC’est en cohérence avec la philosophie Kanban qui consiste à limiter le nombre de tâches en cours.\nDes études ont montré que limiter le nombre de tâches en cours permet d’avoir une meilleure qualité, de délivrer de manière plus consistante, et d’avoir moins de plaintes côté client.\n\n\nGrâce à l’OST on va pouvoir choisir notre prochaine opportunity sans avoir à tout prioriser et re-prioriser.\nOn part du haut, et on compare les opportunities top level, pour en choisir une seule.\nOn descend dans le sous-arbre de celle qu’on a choisi, et on fait pareil : on choisit l’enfant de même niveau le plus prioritaire.\nOn fait ça jusqu’à atteindre le bas de l’arbre : on a alors notre opportunity.\n\n\nLes critères pour prioriser sont :\nLe sizing de l’opportunity : l’impact que l’opportunity a sur nos clients.\nPas besoin que ce soit précis : on a juste besoin de pouvoir comparer les nœuds enfants de même niveau entre eux.\nOn peut utiliser comme données : des analytics, le funnel des sales, les tickets de support, des sondages, nos interview snapshots etc.\nIl faut aussi distinguer combien de clients sont touchés, et à quelle fréquence ils sont touchés. Parfois c’est peu de clients très touchés, ou beaucoup de clients peu touchés.\n\n\nLes facteurs liés au marché :\nIl s’agit de maîtriser notre positionnement par rapport aux entreprises concurrentes, et éventuellement des segments de marché extérieurs qui pourraient venir grignoter notre marché actuel (par exemple la vidéo par câble qui grignote notre service de streaming).\nEn fonction de notre positionnement actuel, on pourra prioriser soit des enjeux importants pour les sales, soit des enjeux stratégiques pour nous maintenir sur notre marché et en conquérir d’autres.\n\n\nLes facteurs liés à l’entreprise : On doit prendre en compte la stratégie, la vision et les valeurs de l’entreprise, pour aller dans le même sens.\nLes facteurs liés au client : Il s’agit de l’impact sur la satisfaction client : on va choisir l’opportunity qui en procurera le plus.\n\n\nIl ne faut pas attribuer de note précise à chaque opportunité et pour chaque critère. Les critères permettent d’avoir un débat au sein du product trio, et le choix doit être grossier et subjectif.\nIl est important que les décisions de discovery en général soient considérées par le product trio comme potentiellement **modifiables au bout de quelques jours **(la discovery est continuous).\nAvec cet état d’esprit on n’a pas besoin de passer trop de temps à choisir l’opportunity. Si elle se révèle mauvaise, on la changera très vite.\n\n\nQuelques anti-patterns à éviter :\nRemettre la décision à plus tard, quand il y aura plus de données : on en apprendra plus en examinant les conséquences d’une opportunity qu’on choisit maintenant, qu’en attendant d’avoir plus de données pour un meilleur choix.\nLe mieux est de limiter le temps de décision : on se donne une heure ou deux (ou au pire un jour ou deux) et à la fin du délai on décide de l’opportunity.\n\n\nUtiliser principalement un des 4 critères pour le choix de l’opportunity : les 4 critères permettent d’avoir un point de vue différent sur l’importance de chaque opportunity. Il faut les utiliser tous.\nAvoir une conclusion pré-établie, et tenter de la justifier : faire ça est une perte de temps. Il faut faire l’exercice avec une ouverture d’esprit pour s’ouvrir à d’autres perspectives.","8---supercharged-ideation#8 - Supercharged ideation":"Les études montrent que quand on recherche la créativité, **les meilleures idées sont parmi les dernières qu’on trouve. **Plus on en cherche, et plus on tombe sur des choses originales et pertinentes.\nIl faut donc résister à notre propension à choisir la première idée ou solution qui nous passe par la tête.\n\n\nToutes les opportunities ne nécessitent pas d’y passer du temps pour trouver des solutions originales. On en a besoin surtout pour celles qui sont stratégiques, où on veut se distinguer de la concurrence.\nLes études montrent que faire un brainstorming est moins efficace que de chercher des idées chacun de son côté : on trouve plus d’idées différentes, des idées plus originales et plus variées.\nLes raisons sont entre autres le fait d’être poussé à plus chercher quand on est seul, alors qu’on peut se reposer sur les autres quand on est en groupe. Il y a aussi le fait que les premières idées du groupe formatent les suivantes, avec une gêne à proposer des idées à priori pas assez bonnes.\nCeci dit on est moins souvent coincés en groupe, donc on a l’impression d’être plus performants.\nUne solution encore plus efficace est d’alterner les sessions seul et en groupe : on trouve efficacement des idées chacun de son côté, puis on est débloqués grâce à l’apport du résultat des autres, et on peut à nouveau avancer chacun de son côté à partir de là.\n\n\nOn est tous performants pour générer des idées. Ça peut prendre un peu de temps au début si on est rouillé mais ça revient toujours.\nPlutôt que de prendre une grosse heure pour trouver des idées, il vaut mieux répartir ça dans la journée et dans différents endroits (entre deux meetings, en marchant après manger etc.).\nOn peut aussi profiter du fait que notre cerveau travaille alors qu’on arrête d’y penser, et le lendemain on tombe sur une bonne idée qui était en gestation.\nOn peut s’inspirer de la concurrence, et aussi d’entreprises dans des domaines qui n’ont rien à voir (bien des problématiques sont communes à différents domaines).\nOn peut aussi considérer le point de vue d‘utilisateurs particuliers : les utilisateurs qui arrivent pour la première fois, les utilisateurs handicapés, jeunes, vieux, vivant loin etc.\n\n\nConcrètement pour générer des solutions à nos opportunities :\n1 - On revoit notre opportunity cible et son contexte, on vérifie qu’elle est bien sizée etc.\n2 - On génère des idées seul, avec les techniques ci-dessus.\n3 - On partage nos idées avec l’équipe. Ça peut être en live ou en asynchrone (slack ou autre).\n4 - On répète les étapes 2 et 3, jusqu’à obtenir 15 à 20 idées.\n5 - On élimine les idées qui ne répondent pas à l’opportunity qu’on vise.\n6 - On va dot-voter avec l’équipe : chacun a 3 points à mettre sur les idées de son choix (avec possibilité de mettre sur les mêmes), et on élimine celles qui en ont le moins, jusqu’à arriver à seulement 3 idées.\nLes études montrent que pour ce qui est du choix des idées, on est plus efficaces en groupe.\nOn peut être amenés à faire plusieurs tours de dot-vote pour éliminer les idées.\nPas besoin de consensus sur chaque idée, par contre il faut que chacune des idées choisies aient au moins une personne qui soit enthousiasmée par celle-ci. Si ce n’est pas le cas, il faut revoter.\n\n\n\n\nQuelques anti-patterns à éviter :\nNe pas inclure une diversité de perspectives : si la plupart des exercices du livre sont faits pour le product trio, il est préférable de faire la génération d’idées avec l’équipe produit entière. On peut même inviter d'autres stakeholders importants. Plus on aura du monde, plus on aura des idées diverses.\nGénérer trop de variations de la même idée : quand on est bloqués, on a tendance à reprendre les mêmes idées un peu différentes pour avoir l’impression d’en avoir plein. Il faut se forcer à en trouver d’autres, y compris en allant chercher ce que font d’autres produits.\nLimiter la recherche d’idées à une seule session : les études montrent que le fait sur un temps plus long, par petites sessions, est plus efficace.\nChoisir des idées qui ne permettent pas de répondre à l’opportunity choisie : avant de dot-voter, il faut bien éliminer les idées sans rapport, même si elles ont l’air intéressantes en elles-mêmes. Sinon on se disperse.","9---identifying-hidden-assumptions#9 - Identifying hidden assumptions":"On a en général** tendance à s’accrocher à nos idées**.\nParmi les biais ici il y a notamment :\nLe biais de confirmation qui fait qu’on va accorder de l’importance à ce qui confirme notre idée, et peu d’importance à ce qui la réfute.\nLe biais des coûts irrécupérables qui fait que plus on investit dans une idée, plus on pense devoir continuer.\n\n\nPour éviter ce phénomène, on a choisi de traiter 3 idées en même temps, et on va itérer rapidement pour en traiter le maximum sans rester longuement sur une idée particulière.\n\n\nPour pouvoir tester 3 idées en même temps, on ne peut pas les implémenter à chaque fois toutes. Il faut à la place tester les assumptions sous-jacentes.\nLa plus grande difficulté c’est de trouver ces assumptions.\n\n\nIl y a 5 types d’assumptions qui nous intéressent :\nDesirability assumptions : le fait de savoir si nos utilisateurs ont envie de faire ce qu’on imagine qu’ils ont envie de faire à travers notre idée.\nViability assumptions : le fait de savoir si l’idée va vraiment apporter au business suffisamment de valeur (et pas seulement aux clients).\nFeasibility assumptions : le fait de savoir si c’est faisable d’un point de vue technique, mais aussi de potentiels problèmes légaux, culturels etc.\nUsability assumptions : le fait de savoir si les clients vont pouvoir l’utiliser, le comprendre etc.\nEthical assumptions : le fait de savoir si on pourrait causer des problèmes éthiques ou dangereux.\nCa concerne ce qu’on va faire des données qu’on collecte :\nEst-ce qu’on va partager ces données avec des tiers ?\nEst-ce que nos clients comprennent ce qu’on fait de leurs données.\nS’ils le comprenaient est-ce qu’ils seraient d’accord ?\n\n\nÇa peut aussi être d’autres types de problèmes :\nEst-ce que notre produit peut devenir addictif et nuire à l’utilisateur ?\nEst-ce que certains utilisateurs seront exclus de notre feature ?\nEst-ce qu’on contribue aux inégalités sociales ?\nEst-ce qu’on expose l’identité de personnes à qui ça pourrait causer du tort ?\nComment les trolls d’internet pourraient-ils détourner ça ?\n\n\nCa pourrait aussi être des dommages pour notre business :\nEst-ce que la solution va aider ou nuire à notre marque ?\nEst-ce qu’on comblera les attentes des clients ou est-ce qu’on les laissera déçus ?\n\n\nUne question qui marche bien pour trouver les problèmes : “Si le New York Times (ou un autre grand média) publiait un article en première page, détaillant notre solution, l’ensemble de nos échanges internes, les conséquences sur l’écosystème etc. Est-ce que ce serait une bonne chose ou non ?”\n\n\n\n\nChaque membre du trio peut mettre des assumptions différentes derrière une idée. Pour s’aligner sur les assumptions, on va utiliser le story mapping : cartographier les étapes faites par l’utilisateur pour obtenir de la valeur de notre produit.\n1 - On va partir du principe que la solution existe déjà, et voir l’apport de valeur sur l’utilisateur. Il ne s’agit pas de mesurer la difficulté d’implémentation.\n2 - On va ensuite identifier les acteurs clés : ça peut être plusieurs users dans un réseau social, des acheteurs/vendeurs, un user et un chatbot etc.\n3 - On liste les étapes nécessaires pour chaque acteur, pour que la fonctionnalité qui nous intéresse apporte de la valeur.\n4 - On dispose les étapes en** séquence horizontale **sur un graphique.\nIl faut respecter l’ordre causal des étapes entre acteurs.\nSi on a des étapes optionnelles on les représente aussi.\nOn représente les successful paths, et s’il y en a plusieurs, on les représente tous.\n\n\n5 - On explicite les suppositions résultant de nos étapes. On va passer sur chacune des étapes et se poser la question de chaque type d’assumption (desirability, feasibility etc.). On va en avoir facilement des dizaines.\nExemple : nous explorons 3 solutions pour notre service de streaming. Parmi elles, on commence par “Intégrer des chaînes locales dans notre service (ABC, NBC etc.)”.\nOn part du principe que le service existe déjà, et on liste les acteurs : le client, la plateforme et la chaîne locale partenaire.\nOn liste les étapes :\nLe client arrive pour regarder du sport en direct.\nNotre plateforme montre les choix de contenu possible.\nLe client choisit.\nLa chaîne locale envoie le contenu.\nLe client le regarde.\n\n\nOn va ensuite expliciter les assumptions qui en résultent.\nPar exemple pour l’étape 1 “Le client arrive pour regarder du sport en direct”, on a les assumptions suivantes :\nDesirability : notre client veut regarder du sport.\nDesirability : Notre client veut regarder du sport sur notre plateforme.\nUsability : Notre client sait qu’il peut regarder du sport sur notre plateforme.\nUsability : Notre client pense à notre plateforme quand il est temps de regarder du sport.\nFeasibility : Notre plateforme est disponible quand le client veut regarder du sport.\n\n\nOn peut aussi tirer des assumptions de viability ou ethical à partir de la story map. Par exemple :\nViability : Intégrer un flux de chaîne partenaire locale ne coûtera pas trop cher.\nEthical : Les clients seront d’accord pour que nous partagions les données de vues avec les chaînes locales.\n\n\n\n\n\n\n\n\nIl ne faut pas trop s’inquiéter d’avoir des dizaines d’assumptions. Si on a bien fait notre travail, la plupart seront justes, et l’important c’est surtout de trouver celles qui sont risquées.\nUne autre technique pour identifier les assumptions risqués c’est de faire une session** pre-mortem**.\nIl s’agit de se placer dans quelques mois, d’imaginer que le produit a été un échec cuisant, et se demander pourquoi.\nPour que ça marche il est important d’imaginer que le produit a été un échec, pas qu’il pourrait l’être.\n\n\nUne autre manière de les trouver encore c’est d’utiliser l’opportunity solution tree, en remontant depuis les solutions vers les opportunities, puis vers l’outcome.\nIl s’agit d’utiliser une phrase du genre “La solution permet d’adresser l’opportunity parce que…”, ou “L’opportunity permet de driver l’outcome parce que…”.\nPar exemple :\nAjouter des chaînes locales permettra à nos clients de regarder du sport en direct parce que…\nLes sports que nos clients veulent sont sur les chaînes locales.\nLa plupart des sports populaires sont sur les chaînes locales.\nNos clients ont des chances de vouloir regarder des sports populaires.\n\n\nAdresser l’opportunity “Regarder des sports en direct” permettra de faire avancer l’outcome produit “Augmenter les minutes regardées par semaine” parce que…\nLes gens regarderont des sports en plus de ce qu’ils regardent déjà.\nMême s’ils regardent moins les autres choses, les show sportifs sont plus longs et le temps augmentera.\nSi chaque session de visionnage est plus longue, le temps global regardé sera plus long.\n\n\nAdresser l’outcome produit “Augmenter les minutes regardées par semaine” permet d’adresser l’outcome business “Augmenter le nombre de renouvellement d’abonnement” parce que…\nLes gens qui regardent longtemps sont plus enclins à renouveler leur abonnement.\n\n\n\n\n\n\nLa combinaison de ces méthodes permet de trouver les assumptions de chaque catégorie. Mais à force, si on devient suffisamment fort, on ne sera plus obligé de les utiliser toutes.\nSouvent, les équipes ont des points faibles, qu’ils peuvent combler par une ou deux techniques. Par exemple remonter l’OST pour les viability assumptions, utiliser les questions à se poser pour les ethical assumptions etc.\n\n\nPour prioriser nos assumptions à tester, on va utiliser l’assumption mapping.\nIl s’agit de trouver les assumptions les plus risquées, celles qui impliquent un “acte de foi” (leap of faith).\nOn va placer les assumptions sur un graphique avec en abscisse le niveau de preuves qu’on a sur le fait que notre assumption est vraie, et en ordonnée l’importance de l’assumption pour la réussite de notre solution.\nToutes les assumptions sont certes importantes, mais certaines sont plus problématiques à contourner si jamais elles se révèlent fausses.\nIl n’y a pas besoin d’être précis, tout ce qui compte c’est de placer les assumptions dans le graphique relativement aux autres assumptions.\n\n\nIl s’agira ensuite de récolter les assumptions “leap of faith” en haut à droite de notre graphique, de le faire pour les 3 idées et de tester chacune de celles-ci.\n\n\nQuelques anti-patterns à éviter :\nNe pas générer assez d’assumptions : Teresa en génère en général 20/30 par idée. On n’aura pas à toutes les tester, mais si n’en génère pas beaucoup on ne trouvera pas non plus les plus risquées.\nFormuler les assumptions de manière négative : on est parfois tenté de formuler une assumption comme “Les clients ne retiendront pas leur mot de passe”, mais cette formulation rendra le test plus difficile. Il vaut mieux formuler ce qui est nécessaire pour que la solution fonctionne (que les clients retiennent leur mot de passe par exemple).\n**Ne pas être assez spécifique **: une assumption comme “Les clients auront le temps” n’est pas assez précise, il vaut mieux quelque chose comme “Les clients prendront le temps de parcourir toutes les options de la page de démarrage”.\nFavoriser certaines catégories au détriment d’autres : si on a par exemple des difficultés de faisabilité, on aura tendance à oublier de tester que la solution est au moins désirable. On oublie aussi souvent les problématiques éthiques etc. Il faut utiliser les catégories pour trouver les angles morts.","10---testing-assumptions-not-ideas#10 - Testing assumptions, not ideas":"Une fois qu’on a notre “leap of faith” d’assumptions pour les 3 idées, il vaut mieux éviter de se précipiter pour les tester.\nParfois les tests ne sont pas pensés pour les assumptions visées, mais pour tester l’idée en entier.\nParfois on teste sur la mauvaise audience, ou on s’éparpille vers des données intéressantes mais sans rapport avec le problème.\n\n\nIl faut tester les trois idées en même temps. Si on les traite une par une, on risque de céder à nos biais (confirmation et coûts irrécupérables).\nPour rendre nos assumptions les plus risqués moins risqués, on va** collecter des données sur ce que les clients font vraiment dans un cas spécifique**, pas sur ce qu’ils disent qu’ils feraient en général.\nIl va s’agir de simuler une mise en situation, et de comparer le comportement de l’utilisateur avec celui que notre assumption aurait sous-tendu.\nExemple : si on reprend notre exemple de plateforme de streaming, et qu’on veut tester l’assumption “Nos clients veulent regarder du sport”. On va pouvoir simuler la situation en présentant un mockup de la page d’accueil de notre plateforme, et leur demander : “Qu’est-ce que vous aimeriez regarder maintenant ?” en leur proposant plusieurs options.\n\n\nOn se retrouve souvent avec plusieurs idées qui partagent la même assumption : du coup la traiter permet de traiter plusieurs idées.\nIl est important de définir à l’avance la condition de succès. Si on ne la définit pas à l’avance, on n’aura pas de résultat actionable, et surtout on va céder à nos biais.\nExemple : Pour le test de notre assumption sur les utilisateurs qui veulent regarder du sport, on décide à l’avance que le succès serait d’avoir 4 utilisateurs sur 10 qui choisissent de regarder du sport.\nPour ce qui est du chiffre en question, on va le négocier au sein du product trio. A noter qu’il ne s’agit pas de prouver la chose, mais juste de réduire le risque.\n\n\nIl est préférable de commencer par de petits tests pas chers pour avoir un potentiel signal d’échec rapide, plutôt qu’investir beaucoup sur un gros test sans avoir déjà eu un retour.\n1 - On choisit l’assumption la plus risquée pour faire un test et la rendre moins risquée ou l’éliminer.\n2 - On choisit à nouveau l’assumption la plus risquée. Si la précédente est toujours la plus risquée, alors on fait un test plus important sur elle, sinon on prend une autre plus risquée avec un petit test.\n3 - On fait des tests de plus en plus gros sur nos assumptions risquées qui ont survécu jusqu’à ce qu’implémenter l’idée dans notre application soit moins cher que le test.\nAvec de petits échantillons on risque d’avoir des faux positifs ou des faux négatifs, mais ce n’est pas très grave.\nPour alléger ce risque, on peut tenter de diversifier les personnes interviewées, en particulier sur des critères en rapport avec l’assumption testée.\nSi on tombe sur un faux négatif (l’assumption est invalidée alors qu’elle était bonne), on pourra toujours se rattraper sur l’assumption suivante. Au pire on se retrouvera avec un petit redesign ou une idée abandonnée. Mais des idées il y en a des milliers.\nC’est pareil en cas de faux positif (l’assumption est validée alors qu’elle était fausse), on s’en rendra compte au test de assumption suivante, ou au test plus important de cette même assumption qui sera probablement en contradiction avec notre faux positif.\n\n\nComparé à ce que fait la science : on utilise ici une méthode proche de la méthode scientifique, mais notre but est d’aller bien plus vite. On réduit simplement le risque plutôt que de rechercher la Vérité.\n\n\nSelon Marty Cagan, les meilleures équipes font 15 à 20 itérations de discovery par semaine (qu'est-ce qu’elle veut dire par “itération de discovery” ?).\nPour être efficaces il y a deux outils pour tester rapidement les assumptions :\n1 - Le user-testing non modéré (unmoderated user-testing) : on a un outil qui nous permet de créer un prototype, et d’ajouter des tâches à faire ou des questions. On envoie ça à des utilisateurs qui pourront le remplir quand ils auront le temps. Et on n’aura plus qu’à visionner les vidéos de ce qu’ils ont fait.\nCes outils (elle ne donne pas d’exemples) sont des game-changers : ils permettent de faire en un jour ou deux ce qu’on mettait plusieurs semaines à faire en terme de test d’assumptions.\n\n\n2 - Le sondage à question unique (one question survey) : on crée un formulaire avec une seule question, et on l’envoie à nos clients.\nÇa peut être utile par exemple pour trianguler un test d’assumption qu’on aurait déjà fait avec le user-testing non modéré, en posant la question d’une autre manière.\nÇa peut être pour tester les préférences des utilisateurs : “Veuillez sélectionner vos sports préférés parmi la liste”.\n\n\nLes mêmes règles d’assumption testing s’appliquent à ces outils : on simule des instances spécifiques pour mettre la personne en situation ou aller chercher un comportement passé précis. Et on ne demande pas ce qu’elle fait en général ou ce qu’elle fera dans le futur.\n\n\nOn a aussi parfois les données qu’on cherche déjà dans nos bases de données.\nPar exemple : le nombre de fois où les sports ont été recherchés par les utilisateurs.\nAttention là aussi à définir à l’avance le nombre qui constituerait un critère de succès du test.\n\n\nPour aller plus loin sur les types de tests à mettre en place elle conseille deux livres :\nUX for Lean Startups de Laura Klein.\nTesting business ideas de David Bland.\n\n\nQuelques anti-patterns à éviter :\nDes simulations trop longues : le but est d’aller vite pour pouvoir itérer dans la bonne direction. Nos tests devraient être complétés en un jour ou deux, ou une semaine max.\nUtiliser des pourcentages au lieu des nombres bruts pour les critères de succès : définir les nombres (total et de succès) permet de garder en tête la fiabilité au cours des différents tests plus ou moins importants.\nNe pas définir suffisamment de critères d’évaluation : il faut au moins le nombre total de personnes à interroger, et le nombre total de succès. Mais parfois si notre test est plus complexe, il peut y avoir plusieurs valeurs à mesurer au total et de succès.\nPar exemple, si on envoie des emails, on peut mesurer ceux qui ouvrent, ceux qui cliquent sur notre lien etc.\n\n\nTester avec les mauvais utilisateurs : bien s’assurer que les personnes interviewées ont bien les besoins, pain points ou désirs de l’opportunity visée.\nConcevoir les tests avec un scénario plus difficile que le plus basique : il faut que notre scénario soit celui qui a le plus de chances de marcher, et on le challengera par la suite avec plus de monde et des cas plus complexes. Ca permet d’accorder une vraie valeur à un cas d’échec, plutôt que de se dire que ça aurait pu marcher avec d’autres personnes ou fait autrement.","11---measuring-impact#11 - Measuring impact":"Il ne faut pas essayer de tout mesurer dès le début.\nOn peut passer des semaines à perdre du temps à essayer de tout planifier, choisir le nom des events qu’on track etc. pour se rendre compte que ce n’était pas ce qu’on croyait.\n\n\nIl faut commencer par mesurer ce dont on a besoin pour valider nos assumption tests, et pas au-delà.\nExemple : Teresa était dans une université, sur un système permettant d’aider les étudiants à trouver un job.\nSon équipe avait remarqué qu’on posait les mauvaises questions aux étudiants sur l’application, et qu’à cause de ça ils ne s’engageaient pas sur la plateforme.\nIls ont créé un prototype d’une version alternative, et l’ont utilisé pour tester des assumptions :\nLes étudiants feront plus de recherches si on leur pose des questions plus simples.\nLes étudiants verront plus de jobs recommandés.\nLes étudiants postuleront à plus de jobs recommandés.\n\n\nPour tester ces assumptions, les critères d’évaluation étaient :\n250 visiteurs sur 500 utiliseront l’interface.\n63 étudiants sur 500 verront un job recommandé.\n7 étudiants sur 500 postuleront à un job recommandé.\n\n\nEt pour mesurer ça, ils ont collecté :\n# de personnes qui ont visité la page de recherche\n# de personnes qui ont fait une recherche\n# de personnes qui ont vu au moins un job\n# de personnes qui ont postulé à au moins un job\n\n\nIls n’ont pas commencé par mesurer tous les clics mais bien seulement les actions pour valider leurs tests.\n\n\n\n\nOn peut mesurer le nombre total d’actions, ou le nombre de personnes faisant l’action au moins une fois.\nPour choisir l’un ou l’autre, on peut se demander si une même personne effectue l’action plusieurs fois, est-ce que ça apporte plus de valeur vis-à-vis de ce qu’on recherche.\n\n\nIl faut aussi mesurer l’impact sur l’outcome, même si c’est difficile, et ne pas se contenter de leading indicators.\nSi on reprend l’exemple de l’université, les mesures faites représentaient des leading indicators (nombre d’étudiants postulant à un job via la plateforme), mais ne mesuraient pas l’outcome business qui était : “Les étudiants trouvent un job via la plateforme”.\nL’outcome business lui-même était hors de leur contrôle du fait que l’obtention du job se passait hors de la plateforme.\nIls l’ont mesuré quand même au travers d’un questionnaire envoyé aux étudiants quelques temps après leur process. Ils ont itéré dessus pour augmenter le nombre de réponses.\n\n\n\n\nQuelques anti-patterns à éviter :\nRester coincé en essayant de tout mesurer : le problème le plus fréquent est de penser qu’on peut connaître à l’avance ses besoins produit à mesurer.\nIl vaut mieux mesurer seulement les assumption tests du moment, puis leur lien avec le product outcome, et avec le temps le lien avec le business outcome.\n\n\nSe concentrer à fond sur les assumption tests et en oublier de remonter dans l’OST : il ne faut pas oublier de remonter de l’outcome produit vers l’outcome business pour bien vérifier qu’on apporte de la valeur d’une manière qui sera durable.","12---managing-the-cycles#12 - Managing the cycles":"Ce chapitre présente de vrais exemples pour illustrer le fait que le processus de discovery n’est pas linéaire. Il faut prendre en compte les résultats pour soit continuer, soit revenir à une autre étape pour changer quelque chose.\n1 - Simply Business est une compagnie d'assurance.\nL’équipe produit reçoit régulièrement un pain point de la part de leurs clients : les retards de paiements. Elle confirme aussi ça avec une étude de marché.\nElle imagine 3 solutions pour aider les clients : des articles pour conseiller les petites entreprises, des rabais pour les payeurs rapides de leurs clients, et une solution technique de collection des paiements.\nEn une semaine ils ont fait 3 assumption tests pour chaque solution, et les ont proposé sous forme d’unmoderated tests. Les premiers tests cherchaient à savoir si les utilisateurs avaient bien compris les différentes offres.\nLes résultats tombent très vite : les utilisateurs n’étaient en fait pas intéressés, parce qu’ils pensaient qu’être sortis de la boucle serait dommageable pour leur business. Ils ont le problème, mais ne veulent pas de l’aide de Simply Business sur ça.\nL’équipe produit a donc choisi de déprioriser cette opportunity, et d’en choisir une autre dans leur OST, puis d’utiliser leurs interviews suivantes pour rebondir rapidement sur une autre sujet.\nConclusion : ils ont gagné du temps en évitant de créer une feature sur un mauvais sujet.\n\n\n2 - CarMax reconditionne et revend des voitures.\nL’équipe produit repère une opportunity : “Je veux être confiant sur l’état de la voiture”.\nElle teste d’abord que des réparations cosmétiques sont vraiment importantes pour le client avec un assumption test présentant une voiture avec un défaut cosmétique moins cher, et une voiture sans le défaut mais plus chère. Le test est concluant.\nIl y a 2 types de solutions possibles :\nCelles qui vont être spécifiques à chaque voiture, où il faudra indiquer ce qui a été réparé par CarMax. Celles-ci sont difficiles à mettre en œuvre et impliqueront plusieurs équipes.\nEt puis il y a la solution de type “quick win” où il s’agira de communiquer sur la qualité générale de l’entreprise, et son attention portée au détail.\n\n\nIls ont décidé de tenter la solution quick win, et ont mené des assumption tests avec des communications sur certaines photos bien placées.\nMalheureusement leurs seuils n’ont pas été atteints, parce que les clients voulaient absolument des éléments spécifiques à la voiture qui les intéresse.\nIls ont donc choisi de remettre l’opportunity à plus tard parce qu’elle prendrait du temps. En attendant, ils ont lancé l’idée auprès des autres équipes, et sont repartis sur autre chose.\nEt justement ils ont fini par la mettre en place plus tard.\nConclusion : il faut parfois explorer des solutions, et les repousser à plus tard où elles seront plus pertinentes.\n\n\n3 - FCSAmerica prête de l’argent à des agriculteurs.\nL’équipe produit s’est intéressée à la possibilité de digitaliser certaines actions, mais ils avait aussi noté que les clients aimaient la relation de confiance avec de vraies personnes de l’entreprise.\nIls ont investigué et on trouvé que les clients cherchaient déjà en ligne pour savoir combien ils pouvaient emprunter.\nIls ont donc choisi de digitaliser ce service. Et pour ajouter la touche humaine, ils y ont ajouté un chat interactif.\nMalgré toutes les tentatives pour le rendre attractif, les clients fermaient systématiquement le chat. La conclusion a été que les clients ne voulaient pas de contact humain à ce moment-là, mais seulement plus tard.\nConclusion : on peut parfois se baser sur ce que les clients font déjà, pour les pousser plus loin et les faire aller dans une direction qui sert les besoins business.\n\n\n4 - Snagajob permet aux chômeurs de trouver un emploi.\nPour améliorer la satisfaction des clients, l’équipe produit s’est attaquée à un pain point côté employeurs : “Les candidats ne répondent pas aux appels”.\nIls ont commencé par aller les voir, et les conseiller gratuitement pour suivre ce qu’ils faisaient pendant un mois.\nLe premier problème constaté était que les candidats avaient changé, et utilisaient beaucoup le mobile avec les textos, et très peu les appels vocaux.\nLa solution ne pouvant pas être d’utiliser des textos côté employeurs parce qu’ils ne voulaient pas, ils ont créé un service accessible via mobile pour les candidats.\nPuis ils ont découvert un problème de lenteur pour réserver un rendez-vous avec beaucoup de va-et-vient. Ils ont alors développé une solution pour améliorer ça.\nIls ont alors découvert que les candidats ne se présentaient parfois pas etc. A chaque fois ils ont amélioré la solution en allant d’opportunity en opportunity.\nConclusion : adresser des opportunities et sub-opportunities permet de petit à petit résoudre un outcome.\n\n\nQuelques anti-patterns à éviter :\nTrop s’engager sur une opportunity : les histoires de Simply Business et CarMax montrent qu’il faut savoir accepter qu’une opportunity peut être intéressante mais pas pour notre contexte, ou pas pour tout de suite.\nÉviter les opportunities difficiles : les quick wins sont bien sûr intéressants, mais une fois qu’on les a réalisés il ne faut pas rejeter les opportunities difficiles à traiter qui peuvent apporter beaucoup. Dans l’histoire de CarMax, ils ont certes laissé l’opportunity pour plus tard, mais ils ont quand même lancé l’idée pour qu’elle fasse son chemin, et ont fini par la traiter.\nTirer des conclusions à partir d’éléments superficiels : dans le cas de FCSAmerica ils auraient pu abandonner l’opportunity de digitalisation de par l’information que les gens préféraient le contact humain pour la confiance. Mais ils ont trouvé un moyen de concilier le besoin business avec les besoins des clients en cherchant un peu.\nAbandonner avant que les petits changements aient pu porter leurs fruits : dans l’histoire de Snagajob ils ont dû régler plusieurs petites opportunities avant d’entrevoir l’amélioration itérativement.","13---show-your-work#13 - Show your work":"Même en faisant une excellente discovery, si on n’a pas le reste de l’entreprise et en en particulier les décideurs avec soi, nos idées ne seront pas mises en place. Il faut donc les convaincre.\nQuand on présente notre travail, il ne faut pas aller directement à la conclusion et donc aux solutions, mais plutôt insister sur les opportunities.\nTout le monde a des solutions pré-établies en tête, et les leaders aussi. Si on amène la discussion sur le terrain des solutions, on va entrer en confrontation avec celles du leader. Or c’est lui qui a le pouvoir et qui aura donc le dernier mot, même s’il n’a pas fait le travail de discovery.\nIl y a même un dicton chez les PM : “The HiPPO always wins” (HiPPO = Highest Paid Person’s Opinions).\n\n\nIl faut présenter le cheminement qu’on a suivi en s’appuyant sur l’OST, et en prenant le temps de le faire.\nL’intérêt de faire ça c’est qu’on va rendre les personnes comme actrices du processus de discovery, et donc plus enclines à accepter les choix qui en découlent.\nD’abord présenter l’outcome, puis donner des éléments de contexte au travers des opportunities.\nIl faut bien mentionner les choix importants qu’on a pu faire au cours de la discovery et pourquoi. On peut leur présenter certains interview snapshots pour appuyer ce qu'on dit.\nIl ne faut pas oublier de prendre des feedbacks de la part de nos interlocuteurs à chaque étape, pour les intégrer à notre travail.\nUne fois le contexte posé on peut présenter les solutions et les assumption tests, en prenant toujours les feedbacks.\n\n\nQuelques anti-patterns à éviter :\nDire les conclusions au lieu de montrer le cheminement.\nSurcharger les stakeholders avec trop de détails : ne pas aller directement aux conclusions ne veut pas dire détailler l’analyse de chaque interview snapshot et le résultat de chaque discussion en interne.\nEn fonction de qui il s’agit, la personne peut vouloir plus ou moins de détails : notre manager voudra sans doute des détails chaque semaine, alors que le CEO voudra quelque chose de très concis\nAttention cependant : concis ne veut pas dire parler des conclusions plutôt que des opportunities. On peut être concis en mentionnant le cheminement de la discovery avec seulement les éléments les plus déterminants qui nous ont mené à nos conclusions.\n\n\nArgumenter avec les stakeholders sur pourquoi leurs idées ne peuvent pas fonctionner : même si l’idée est mauvaise au regard des éléments de discovery, il faut donner des éléments au stakeholder pour qu’il parvienne à cette conclusion par lui-même, plutôt que lui dire et risquer de le braquer.\nSi son idée est bonne mais ne fit pas avec l’outcome, il faut le guider dans l’OST et lui dire qu’on peut la considérer, mais plutôt pour un autre outcome.\n\n\nEssayer de gagner le “combat idéologique” au lieu de nous concentrer sur les décisions qui dépendent de nous : quelle que soit la qualité de notre discovery, si une personne plus haut placée que nous veut prendre une décision qui va contre nos conclusions, nous ne pourrons pas gagner contre elle.\nSi on se retrouve à dire “C’est la manière dont c’est censé être fait”, alors il faut prendre une inspiration et aller faire un tour.\nLa seule chose qu’on puisse faire c’est donner les clés de compréhension pour que la personne arrive à nos conclusions par elle-même. Si elle a des conclusions différentes malgré tout, on ne gagnera de toute façon pas contre elle. Teresa conseille de choisir ses combats, et de choisir plutôt ceux qu’on peut faire avancer.","part-iii---developing-your-continuous-discovery-habits#Part III - Developing your continuous discovery habits":"","14---start-small-and-iterate#14 - Start small, and iterate":"Durant toute sa carrière, Teresa a pu être confrontée à des environnements qui ne pratiquaient pas la discovery moderne, où il s’agissait de créer des produits puis de les présenter aux clients.\nSa méthode qui a toujours marché a été de faire les choses bien de son côté, sans se préoccuper du fonctionnement global de l’entreprise qui ne dépendait pas d’elle.\nLa chose la plus importante qu’elle a cherché à faire à chaque fois c’est chercher un contact avec les clients, et le garder tout au long de l’évolution du produit.\n\n\nPour mettre en place la méthode de ce livre dans notre entreprise, elle conseille de :\n1 - Se constituer un product trio.\nLes activités décrites dans ce livre sont destinées à être faites en groupe. Si on est PM, il faut trouver un software engineer et un designer qui acceptent de participer aux activités de discovery. Si pas de designer dans l’entreprise, on peut prendre une personne qui a une sensibilité sur le sujet.\nOn peut commencer petit en les incluant dans certaines activités et décisions, et itérer en allant de plus en plus loin.\n\n\n2 - Commencer à parler aux clients.\nUne fois qu’on a notre trio, on est prêt pour établir notre keystone habit : le contact hebdomadaire avec les clients. Cette habitude sera la pierre angulaire des autres habitudes de la continuous discovery.\nIl est souvent difficile d’établir ce contact avec les clients. Soit parce que les sales ou autres veulent en garder l’exclusivité, soit parce que les clients sont difficilement joignables etc. Mais il y a toujours des moyens d’y arriver.\n\n\n3 - Travailler à l’envers.\nDans le cas où on se trouve dans une entreprise orientée delivery uniquement, et où les features à faire sont élaborées par la direction sans discovery, on peut partir des features qu’on nous donne et remonter la chaîne : vers l’opportunity, puis vers l’outcome produit et l’outcome business.\nSi on parle régulièrement avec les clients, on pourra confirmer les opportunities sous-jacentes, et trouver des assumption tests pour les features. On pourra alors identifier les tests qui peuvent être problématiques.\nOn peut demander aux stakeholders ce qu’ils espèrent comme impact de la feature, et mesurer ça. Si l’impact n’est pas atteint (ce qui, sans discovery, va forcément arriver), on peut revenir vers eux et leur dire qu’on peut faire mieux, en leur proposant de générer des idées avec nous à partir de notre OST.\nAttention par contre à ne pas jouer les je-sais-tout ou, je-vous-l’avais-dit. Il faut qu’ils se sentent acteurs de cette nouvelle manière de faire, et pas en position défensive.\n\n\n4 - Utiliser les rétrospectives pour s’améliorer.\nOn peut par exemple utiliser les rétrospectives de Scrum si on applique cette méthode pour y ajouter une partie sur la discovery.\nOn peut se poser la question “Qu’est-ce qu’on a appris qui nous a surpris dans ce sprint ?”, suivi de “Qu’est-ce qu’on aurait pu faire pour le savoir plus tôt ?”.\nSi on n’a pas eu l’impact espéré, est-ce qu’on a négligé de tester une assumption ? Est-ce qu’on l’avait mal catalogué hors de notre leap of faith ? Est-ce qu’on a eu des problèmes sur la faisabilité et pourquoi ?\n\n\n\n\nQuelques anti-patterns à éviter :\nSe dire que ça ne fonctionnera jamais chez nous : Teresa a vu cette méthode implémentée dans toutes sortes d’entreprises, petites comme grandes. Il vaut mieux se concentrer sur ce qui est en notre pouvoir pour faire marcher la discovery.\nÊtre le champion de “la bonne manière” de faire : vouloir appliquer la méthode strictement et d’un coup peut aussi être dommageable. Il faut y aller itérativement et l’adapter à son contexte.\nAttendre d’avoir la permission au lieu de commencer à faire ce qui est dans nos possibilités : il ne faut pas hésiter à parler aux clients si on en a la possibilité. Ne pas hésiter aussi à parler à des personnes similaires à nos clients dans notre entourage. Tout est bon à prendre pour commencer.","15---whats-next#15 - What’s next":"Teresa propose plusieurs ressources pour aller plus loin :\nSouscrire à la newsletter mensuelle “Product Talk” : chaque mois il y a un article, soit sur une team qui marche bien, soit sur un sujet particulier de discovery.\nRejoindre la communauté “Continuous Discovery Habits” pour interagir avec d’autres personnes intéressées par le sujet.\nS’inscrire à une Master Class en petit groupe avec Teresa.\nS’inscrire à un cours d’approfondissement.\nEmbaucher un coach de chez Product Talk (teresa@producttalk.org)."}},"/notes/designing-data-intensive-applications":{"title":"Designing Data-Intensive Applications","data":{"":"","1---reliable-scalable-and-maintainable-applications#1 - Reliable, scalable and maintainable applications":"Data-intensive désigne le fait que les données soient le bottleneck, par opposition à compute-intensive qui fait référence au CPU.\nLes frontières entre les différentes catégories (base de données, cache, système de queuing etc.) deviennent parfois floues. Par ex : Redis est un cache utilisé comme système de queuing, ou encore Kafka qui est un système de queuing avec une garantie de persistance comme une BDD.\nIl y a 3 enjeux principaux auxquels on répond quand on conçoit un système de données :\nLa fiabilité (reliability) consiste à fonctionner correctement malgré les fautes matérielles, logicielles, ou humaines.\nLes disques durs sont connus pour faire des fautes tous les 10 à 50 ans, ce qui veut dire que sur un parc de 10 000 disques, il y en a un qui saute tous les jours. On peut prévenir ce genre de problème par de la redondance (RAID par ex).\nLes fautes logicielles sont beaucoup plus insidieuses, et peuvent causer des dégâts en chaîne. Pour les prévenir on peut mettre en place du monitoring, prévoir des restarts de processus en cas de crash etc. Mais ça reste bien maigre en soi.\nLes fautes humaines sont inévitables, il faut concevoir les systèmes de manière à décourager les actions problématiques, faire beaucoup de tests automatisés, rendre facile le fallback etc.\n\n\nLa scalabilité consiste à accompagner le système dans sa montée en charge en termes de données, de trafic ou de complexité.\nParler de scalabilité tout court n’a pas vraiment de sens, il faut préciser sur quel aspect on scale.\nIl faut d’abord décrire le load sur lequel on veut scaler. Par ex (page 11) : pour twitter le load clé c’est le nombre de followers par personne :\nla 1ère solution consiste à recréer la timeline de tweets de chaque utilisateur depuis la base de données\nla 2ème à constituer des timelines à jour dans un cache, et de mettre à jour les timelines des followers à chaque tweet. Du coup avec la solution 2 tout dépend du nombre de followers.\nTwitter a fini par adopter une solution hybride : la 2ème solution par défaut, et la 1ère pour les comptes avec énormément de followers. Par défaut la timeline est dans le cache, mais si une célébrité est suivie, une requête sera faite pour récupérer les tweets.\n\n\nEnsuite il faut décrire la métrique de performance. Il s’agit d’augmenter le load qu’on a décrit pour voir jusqu’où on tient.\nSi notre métrique concerne un service en ligne, on va en général prendre le temps de réponse.\n(Le temps de réponse et la latence sont différents : la latence concerne le temps pendant lequel la requête est latente, c'est-à-dire qu’elle attend d’être traitée. Le temps de réponse est plus long que ça.)\nIl faut reproduire la requête un grand nombre de fois, et prendre la médiane pour avoir une idée du temps que ça prend. Dans la même idée on peut prendre les percentiles pour voir par ex. si on arrive à rester sous un certain seuil pour 99.9% de nos requêtes (appelé p999).\n\n\n\n\nPour répondre aux problématiques de scalabilité :\nUne réponse à un certain load ne marchera pas pour un load beaucoup plus important : il faut repenser régulièrement son architecture si on scale vraiment.\nIl y a le scale vertical (machine plus puissante) et le scale horizontal (plus de machines, qu’on appelle aussi shared-nothing architecture).\nEn réalité, on utilise souvent un mix des deux : des machines puissantes pour certaines tâches, et du scaling horizontal pour d’autres.\n\n\nLa création de machines supplémentaires peut être manuelle ou “élastique”. La version élastique permet d’adapter aux grandes variations mais est plus complexe aussi.\nHabituellement, avoir une application stateful qui est sur plusieurs machines est difficile à gérer, donc on essaye de garder la BDD sur une seule machine jusqu’à ce que ce ne soit plus possible. Avec l’évolution des outils, ceci sera sans doute amené à changer.\nIl n’y a pas de magic scaling sauce : chaque application de grande échelle a ses propres contraintes, ses propres bottlenecks, et donc sa propre architecture.\nQuand on crée un produit, il vaut au début passer surtout du temps à développer les fonctionnalités qu’à penser son hypothétique scaling.\n\n\n\n\n\n\nLa maintenabilité consiste à pouvoir à la fois perpétuer le système et le faire évoluer en un temps de travail raisonnable.\nPour qu’un système soit maintenable dans le temps, il faut travailler sur ces aspects :\noperability : la facilité pour les ops de faire tourner le système.\nIl faut faciliter la vie au maximum pour les ops. Ex : fournir un bon monitoring, permettre d’éteindre une machine individuellement sans affecter le reste, avoir de bonnes valeurs par défaut et un comportement auto-réparateur, tout en permettant aux ops de prendre la main.\n\n\nsimplicity : que le système soit le moins complexe possible pour le comprendre rapidement et pouvoir travailler dessus.\nOn peut par exemple réduire la complexité accidentelle, c’est-à-dire la complexité non nécessaire liée seulement à l’implémentation mauvaise.\nSinon globalement une bonne chose à faire c’est d’introduire des abstractions pour appréhender le système plus facilement. Par ex. les langages haut niveau sont des abstractions de ce qui se passe dans la machine.\n\n\nevolvability : la facilité à changer ou ajouter des fonctionnalités au système.\nIl s’agit ici de l’agilité mais appliquée à tout un système, et pas à de petites fonctionnalités.","2---data-models-and-query-languages#2 - Data Models and Query Languages":"Le modèle de données relationnel a dominé le stockage depuis les années 70, en apportant de l’abstraction autour de la manière dont les données étaient structurées, contrairement aux autres alternatives.\nToutes les tentatives de détrôner SQL ont échoué, la hype est retombée.\n\n\nNoSQL arrive dans les années 2010 et regroupe tout un ensemble de technologies qui permettent de pallier aux problématiques de scalabilité, et d’offrir une plus grande flexibilité que les BDD relationnelles\nParmi elles, il y a notamment les BDD basées sur le modèle de document.\nIl est probable que les BDD relationnelles et NoSQL soient utilisées conjointement dans le futur.\n\n\nIl y a un décalage entre la POO et le format de BDD relationnel, qui oblige à une forme de conversion. Pour certaines données on pourrait utiliser une structure en document comme JSON par exemple au lieu du relationnel. Par ex pour le cas des infos d’un CV, on pourrait la ville d’un job autant de fois qu’elle apparaît.\nOn répète alors éventuellement plusieurs fois certaines informations dans les entrées, ou alors on les met dans une table à part mais on fait les jointures à la main depuis le code applicatif.\nEn réalité, ce problème est apparu dès les années 70. Le modèle hiérarchique (proche du modèle sous forme de document qui a fait résurgence récemment donc) faisait face à 2 autres modèles : le modèle relationnel et le modèle en réseau (network model) qui a fini par être abandonné.\nLe modèle en réseau consistait à avoir un modèle hiérarchique mais avec la possibilité pour chaque donnée d’avoir plusieurs parents. Mais ça rendait le code applicatif difficile à maintenir.\n\n\n\n\nLa normalisation consiste justement dans les BDD relationnelles à trouver ce genre de répétition, et à les factoriser en une nouvelle table. Le but est d’éviter la duplication, et donc de renforcer la consistance des données. Ça permet aussi de les modifier facilement en un seul endroit.\n\n\nComparaison aujourd’hui du modèle relationnel et du modèle de document :\nSimplicité du code applicatif :\nLe modèle de document mène à un code applicatif plus simple dans le cas où il y a peu de relations many to many ou many to one (pour les one to many c’est ok puisqu’on répète de toute façon la donnée dans la table du modèle de document).\nDans le cas contraire il faudrait faire les jointures à la main donc le modèle relationnel serait meilleur (code applicatif plus simple et jointures par la BDD plus efficaces).\nDans le cas où il y a une forte interconnexion entre les données (de nombreuses relations many to many), c’est alors le modèle en graphe qui serait le plus pertinent.\n\n\nFlexibilité du schéma de données :\nC’est un peu comme la différence entre le typage statique et dynamique des langages de programmation : le modèle relationnel force à déclarer un type de données et à s’y conformer ou faire une migration. Le modèle de document permet de changer de type de données en cours de route et donc la gestion des données est entièrement confiée à l’application, qui gagne en liberté et du coup en responsabilité.\nLe modèle de document est vraiment meilleur quand les données sont de type hétérogène, ou encore si elles sont déterminées par un système extérieur sur lequel la BDD n’a pas le contrôle.\n\n\nLocalité des données :\nVu que dans le modèle de document les données sont copiées dans chaque entrée, elles sont locales à celles-ci. On peut donc les avoir avec juste une requête, et on utilise moins le disque dur qu’avec le modèle relationnel. En revanche on va chercher le document entier, donc si on a souvent besoin d’un tout petit morceau ça n’en vaut peut être pas le coup.\nCertaines BDD relationnelles permettent aussi de localiser des tables vis-à-vis d’autres (ex : Spanner database de Google, Oracle, ou encore Bigtable data model (utilisé dans Cassandra et Hbase).\n\n\n\n\nLes différentes implémentations de BDD ont tendance à converger : la plupart des BDD relationnelles supportent les opérations dans du contenu XML ou JSON, et RethinkDB et MongoDB permettent de faire une forme de jointure automatique, même si moins efficace.\nLe modèle relationnel offre un langage déclaratif, alors que le modèle hiérarchique n’offre qu’un langage impératif. L’avantage du déclaratif c’est que ça abstrait des détails qui peuvent être laissés à la discrétion de l’outil de BDD qu’on utilise pour faire des optimisations.\nMapReduce, qui est un modèle popularisé par Google et disponible dans MongoDB, CouchDB et Hadoop est entre le déclaratif et l’impératif. Il abstrait certaines opérations mais permet aussi d’ajouter du code en plein milieu d’une requête qui aurait été atomique en SQL.\n\n\nDans les bases de données de graphes, les données sont représentées sous forme d’entités reliés par des traits.\nEx : Facebook utilise un graphe géant où sont présentes des entités variées (personne, lieu, commentaire), reliés entre eux avec des types de liens différents.\nModèle property graph (implémenté par Neo4j, Titan, InfiniteGraph) :\nIl y a deux tables : les entités (vertices) et les traits (edges) avec chacun leurs propriétés, et pour les edges la liste des couples d’entités reliés par son biais.\nOn peut facilement créer de nouveaux types de liens, sans avoir besoin de vraiment modifier la structure de la BDD.\nLe langage Cypher est un langage déclaratif inventé pour Neo4j.\nL’avantage c’est que le langage de graphe permet de trouver des données en parcourant un nombre indéterminé de chemins, et donc de faire un nombre non connu à l’avance de jointures. C’est possible en SQL mais avec une syntaxe beaucoup plus longue.\n\n\nModèle triple-store (implémenté par Datomic, AllegroGraph) :\nIl s’agit de la même chose que le property graph, mais présenté différemment : on a un groupe de 3 données qui sont (sujet, prédicat, objet).\nTurtle et SPARQL sont des langages qui permettent d’utiliser le triple-store.","3---storage-and-retrieval#3 - Storage and retrieval":"Un des moyens d’organiser les données dans une BDD est d’utiliser un système de log : l’ajout de données est fait en ajoutant le contenu à la fin d’un fichier (ce qui est très rapide), et la lecture est faite en parcourant l’ensemble des données (ce qui est très lent O(n)).\nPour accélérer la lecture, on peut créer des index sur les champs dont on estime qu’ils vont souvent servir à faire des recherches. Ça accélère la lecture, mais ça ralentit l’écriture puisqu’il faudra mettre à jour l’index à chaque fois.\nOn peut utiliser des Hash index tels que implémentés dans Bitcast, le moteur de stockage de Riak. Il s’agit d’avoir une structure associant une clé à un offset en mémoire vive. A chaque recherche on n’a qu’à trouver la clé et on peut directement lire la donnée sur disque.\nPour des raisons pratiques (consistance des données, performance grâce aux opérations séquentielles et non pas random), les fichiers de BDD ne sont jamais modifiés. On écrit les nouvelles données séquentiellement (donc pas de concurrence pour l’écriture) toujours à la fin du fichier, et on fait du ménage dans le fichier dans un nouveau fichier de BDD régulièrement. Pareil pour supprimer une donnée : on insère une commande dans le fichier et ce sera supprimé à la prochaine copie / optimisation du fichier de BDD.\nLes limitations c’est qu’il faut que les clés tiennent en mémoire vive sinon c’est foutu, et que les recherches de “ranges” de clés ne sont pas efficaces, ça revient à chercher les clés une par une.\n\n\n\n\n\n\nOn peut aussi stocker les données sous forme triée dès le début. On a alors les Sorted String Table (SSTable). Ca consiste à avoir une structure d’arbre triée en mémoire où vont les nouvelles données (qu’on va appeler la memtable). Et tous les quelques Mo on écrit ça sur DD. Puis régulièrement on va faire des opérations en tâche de fond pour grouper les arbres triés en un seul. Lors d’une recherche, on va d’abord chercher dans le bloc le plus récent, puis de moins en moins récent, jusqu’à arriver au gros bloc, sachant que tous les blocs sont déjà triés.\nUn des avantages c’est qu’on n’a plus à faire entrer toutes les clés en RAM. On peut avoir en mémoire un nombre de clés beaucoup plus épars qui indique les offsets.\nAutre avantage aussi qui répond au problème du hash index : on peut faire des recherches de “range” d’index, vu que tout est déjà trié.\nEt aussi, comme tout est déjà trié et qu’on a les offsets des données groupe par groupe, on peut compresser des groupes de données ensemble.\nCe mécanisme est aussi appelé Log Structure Merge Tree (LSM Tree) en référence à un papier décrivant le mécanisme.\nDe nombreux moteurs de BDD utilisent ce principe :\nLevelDB (peut être utilisé dans Riak) et RocksDB\nCassandra et HBase, inspirés du papier Bigtable et Google.\nLucene (moteur d’Elasticsearch et de Solr) utilise un mécanisme similaire pour indexer le texte.\n\n\nEn terme d’optimisations :\nLa recherche peut être lente : on cherche dans la structure en mémoire, puis dans le premier bloc en BDD et ainsi de suite tant qu’on ne trouve pas, jusqu’à avoir cherché dans le bloc déjà compacté. Pour remédier à ça on peut approximer la recherche avec des structures efficaces appelées Bloom Filters.\nIl y a 2 types de stratégies de compaction :\nsize-tiered : les nouveaux et petits blocs sont régulièrement fusionnés avec les anciens et plus gros blocs.\nHBase utilise cette technique, alors que Cassandra supporte les deux.\n\n\nleveled : les blocs sont plus petits et la compaction se fait de manière plus incrémentale, utilisant moins d’espace disque.\nLevelDB tient son nom du fait qu’il utilise cette technique. On a aussi RocksDB ici.\n\n\n\n\n\n\n\n\nLa structure de BDD la plus utilisée et depuis longtemps est le B-Tree. La plupart des BDD relationnelles l’utilisent, mais aussi une bonne partie des BDD NoSQL.\nIl s’agit d’avoir des pages de taille fixe (en général 4 ko), organisés en couches (rarement plus de 4 couches). Chaque page contient des clés et des références vers des zones physiques de disque pour aller chercher les clés entre deux clés indiquées (sorte de dichotomie donc). On descend de couche en couche jusqu’à arriver à une page qui contient des données et pas de références vers d’autres pages.\nComme avec les LSM-Tree, pour que les B-Tree survivent à un crash sans perte de données, on va écrire toutes les opérations dans un fichier de log avant de modifier la BDD elle-même. Ensuite on peut détruire ce fichier de log.\nIl peut y avoir des problèmes de concurrence avec les B-Tree, on va alors utiliser des locks locaux pour bloquer correctement une partie de la BDD pour le thread qui écrit dedans. Ce problème n’existe pas avec les LSM-Tree puisque les opérations de restructuration sont faites en arrière plan.\n\n\nComparaison B-Tree / LSM-Tree :\nChacun a des avantages et inconvénients, le mieux selon Kleppmann c’est de tester empiriquement dans notre cas particulier lequel a la meilleure performance.\nA priori, la plupart du temps l’écriture serait plus rapide sur les LSM-Tree (à priori parce qu’il y aurait souvent une write amplification moins importante), alors que la lecture serait plus rapide sur les B-Tree (parce que les LSM-Tree ont besoin de lire plusieurs groupes de données triées jusqu’à ce que la compaction soit faite en arrière-plan).\nLes LSM-Tree sont meilleurs en particulier sur les disques durs mécaniques étant donné qu’ils organisent leurs données séquentiellement et ne font pas d’accès random.\nLes LSM-Tree stockent leurs données sur moins d’espace grâce à la compression, mais en même temps au moment où les données arrivent, ils les stockent dans un autre fichier que la BDD principale. Jusqu’à ce que les opérations d’arrière-plan soient exécutées, il y a des copies plus ou moins récentes des données qui coexistent sur le disque.\nLes B-Tree offrent plus de prédictibilité. Même si une opération d’écriture peut prendre plus de temps, on reste constant et évite des pics dans les hauts percentiles, qui peuvent arriver avec les LSM-Tree dans le cas où le disque serait par exemple surchargé et que les opérations d’arrière plan prendraient du retard.\n\n\nEn plus des index primaires il est possible de faire des index secondaires, qui vont indexer en fonction d’une autre colonne dont on estime qu’elle sera utile pour la recherche de données. La différence avec l’index primaire c’est qu’on n’a pas besoin d’avoir une unicité sur les données de la colonne indexée.\nCet index peut soit contenir une référence vers l’endroit où est stockée la donnée (qu’on appelle heap file), soit une version dupliquée de la donnée elle-même (on parle de clustered index). Il y a des avantages et inconvénients évidents à le faire et ne pas le faire (rapidité de recherche vs temps d’écriture et consistance).\n\n\nOn peut aussi faire des index multi-colonnes. Ça permet de chercher par plusieurs champs en même temps.\nLe plus connu est l’index concaténé, qui consiste à accoler plusieurs champs ensemble dans l'index, par ex “NomPrénom”, qui permet de chercher par “Nom”, ou par “NomPrénom”, mais pas par “Prénom”.\nIl y a aussi les index multi-dimensionnels, qui permettent de pouvoir chercher avec plusieurs colonnes indépendamment, utile par exemple pour la recherche de coordonnées géospatiales longitude / latitude.\nC’est implémenté par ex par PostGIS dans PostgreSQL, qui utilise des R-trees en interne.\n\n\n\n\nLucene permet de faire des recherches de termes avec des distances (une distance de 1 signifie qu’avec une lettre différente dans le mot, il sera retenu) grâce à sa structure de clés en mémoire particulière.\nLa RAM étant de moins en moins chère, on peut imaginer des BDD entièrement en RAM.\nPour pallier le risque de perte de données, on peut écrire sur disque en parallèle, avoir de la RAM avec batterie, ou encore faire des réplications en mémoire.\nPlusieurs moteurs de BDD fonctionnent comme ça :\nVoltDB, MemSQL et Oracle TimesTen, ainsi que RAMCloud qui est open source.\nRedis et Couchbase offrent une durabilité faible en écrivant sur disque de manière asynchrone.\n\n\nContrairement à ce qu’on pourrait penser, le gain de performance d’utiliser des BDD in-memory ne vient pas forcément de l’écriture/lecture sur DD en elle-même, puisque l’OS met de toute façon les données récemment manipulées en cache dans la RAM. En fait, le gain vient surtout du temps de conversion des données dans un format qu’on peut écrire sur DD.\n\n\nCe qu’on appelle transaction n’a pas forcément besoin d’être ACID (atomic, consistant, isolé, durable). Il s’agit simplement d’un terme désignant des lectures/écritures avec faible latence, par opposition à batch, qui lui désigne les jobs faits périodiquement dans le temps.\nLa transaction classique est appelée OLTP (OnLine Transaction Processing). Il existe un autre type de transaction : OLAP (OnLine Analytic Processing) qui consiste à agir sur peu de colonnes mais un très grand nombre d’entrées, pour faire des analyses de données (par exemple des comptages, statistiques etc.).\nDepuis les années 80 les grandes entreprises stockent une copie de leur BDD dans un Data Warehouse : une base de données structurée de manière à optimiser les requêtes d’analyse, et ne risquant pas d’affecter la prod.\nSQL permet d’être performant sur l’OLTP comme sur l’OLAP, globalement c’est ça qu’on va utiliser sur les data warehouses. Par contre les BDD sont structurées bien différemment pour optimiser l’analyse.\nUn certain nombre d’acteurs proposent des data warehouses avec des licences commerciales onéreuses : Teradata, Vertica, SAP HANA, ParAccel (ainsi que Amazon RedShift qui est une version hostée de ParAccel)\nD’autres acteurs open source de type SQL-on-Hadoop concurrencent les premiers : Apache Hive, Spark SQL, Cloudera Impala, Facebook Presto, Apache Tajo, Apache Drill.\n\n\nDe nombreux data warehouses sont organisés selon un star schema. On a la fact table au centre avec en général des dizaines voir centaines de champs, et représentant les événements étudiés. Et autour on a les dimension tables, répondant aux questions who, what, where, when, how, why et liées à la fact table par des foreign keys, ils représentent en quelques sortes les metadata.\nUne variation du star model s’appelle snowflakes, il s’agit d’une version plus normalisée, où on va davantage découper les dimention tables en sous-tables.\n\n\nLa plupart du temps, les data warehouses utilisent un column-oriented storage plutôt qu’un row-oriented. Il s’agit de stocker les données des colonnes physiquement côte à côte, parce que les requêtes vont avoir besoin de lire en général quelques colonnes entières.\nLa plupart du temps les bases en colonne sont relationnelles, mais il y a par ex Parquet, basé sur Google Dremel, qui est orienté colonne mais non-relationnel.\nCassandra et HBase ont un concept de column families, mais ça consiste seulement à stocker toutes les colonnes d’une entrée ensemble, elles sont en réalité essentiellement row oriented.\nDans le cas où les valeurs dans les colonnes se répètent (en particulier s’il y a beaucoup plus de valeurs que de valeurs possibles), on peut faire une compression sur les colonnes. Par exemple une compression de type bitmap encoding\nUn des avantages des column oriented storages c’est que ça se prête bien à un traitement optimal entre la RAM et le cache du CPU, avec de petits cycles de traitement de données compressées provenant de la même colonne.\nOn peut profiter du mécanisme des LSM-Trees avec les données en mémoire et le reste de la BDD sur disque, pour trier les entrées d’une façon particulière. Par exemple, on peut choisir la colonne qui est souvent la plus recherchée, et trier les entrées de manière à avoir toutes les entrées avec la même valeur dans cette colonne côte à côte. Et ainsi de suite pour les colonnes secondaires. Ça permet une meilleure recherche mais aussi une meilleure compression pour ces colonnes-là.\nOn peut également choisir de trier différemment chaque copie de la BDD qu’on possède, pour choisir celle qui nous arrange le plus au moment de faire une requête. C-Store et Vertica font ça.\n\n\nPour optimiser les requêtes dans les column oriented databases, à la place d’un inde on peut mettre en place une materialized view, qui consiste à ajouter une valeur ou une colonne de valeurs contenant des calculs (MIN, MAX, SUM etc.).\nUn cas particulier s’appelle le data cube, il s’agit de prendre deux colonnes comme composant les deux dimensions d’une valeur qu’on cherche à analyser, et d’ajouter une colonne représentant un agrégat (par ex la somme des valeurs sur une des dimensions).\nCette pratique permet d’accélérer les requêtes parce que certaines choses sont pré-calculées, mais ça offre aussi moins de flexibilité. Donc en général on s’en sert comme boost de performance tout en laissant aux data analyst la possibilité de faire les requêtes qu’ils veulent.","4---encoding-and-evolution#4 - Encoding and evolution":"Quand on change les fonctionnalités, y compris la BDD, il est utile de pouvoir faire une rolling upgrade (ou staged rollout). Il s’agit de déployer le code sur certains nœuds, s’assurer que tout va bien, puis déployer progressivement sur les autres.\nCela implique que le code et la BDD doivent être backward-compatibles (supporter les fonctionnalités du code précédent) mais aussi forward-compatibles (que le code précédent ignore les nouvelles fonctionnalités).\n\n\nLes données ont besoin d’au moins 2 représentations : une en mémoire avec des pointeurs vers des zones mémoire, et une quand on veut transmettre la donnée sur disque ou à travers le réseau. Il faut alors que tout soit contenu dans le bloc de données. La conversion de la mémoire vers la version transportable s’appelle encoding (ou serialization ou marshalling).\nIl existe des formats liés à des langages, comme pickle pour python, mais ces formats ne sont ni performants, ni ne gèrent bien la rétro (et forward) compatibility. Et le format de données est trop centré sur un langage particulier.\nOn a les formats plus standards comme XML, JSON et CSV.\nXML et CSV ne distinguent pas les nombres des strings, alors qu’en JSON on distingue les nombres mais pas les flottants des entiers par exemple.\nXML et JSON ont la possibilité d’avoir des schémas associés mais ceux-ci ne font pas consensus.\nGlobalement ces formats sont suffisants pour une communication entre organisations, tant que celles-ci s’accordent sur des conventions. En réalité, la plus grande difficulté est que des organisations différentes s’accordent sur quoique ce soit.\n\n\nOn a enfin les formats binaires non spécifiques à un langage et conçus pour la performance.\nIl existe des versions binaires (ne faisant pas consensus) pour JSON et XML. Par exemple pour JSON il y a MessagePack, BSON, BJSON, UBJSON, BISON et Smile. Le souci de ces formats c’est qu’ils sont assez peu compacts parce qu’ils embarquent le nom des champs répété à chaque entrée de donnée.\nApache Thrift, développé par Facebook, et Protocol Buffers (ou protobuf), développé par Google sont deux formats binaires apparus en open source en 2007/2008. Leur particularité est qu’ils ont besoin d’un schéma, et qu’ils ne répètent pas le nom des champs pour gagner de la place. Ils ont tous les deux des adaptations dans la plupart des langages pour sérialiser / désérialiser des données dans ce format à partir des structures du langage.\nA propos des formats :\nConcernant Thrift : Il a deux formats différents :\nBinaryProtocol qui remplace le nom des champs par des chiffres faisant référence aux champs du schéma.\nCompactProtocol qui possède des optimisations supplémentaires pour gagner de la place en encodant le numéro du champ et le type de champ sur un seul byte, et en utilisant par ex des entiers de longueur variable.\n\n\nConcernant Protobuf : il est globalement assez similaire au CompactProtocol de Thrift, avec des petites différences dans la manière d’encoder les bytes.\n\n\nA propos de l’évolution des schémas (Protobuf et Thrift) :\nAjout de champ : comme les champs sont représentés par un numéro reporté dans le schéma, on peut facilement ajouter un champ. Ce sera backward-compatible puisque le nouveau code pourra toujours lire les données qui n’ont pas les nouveaux champs, et ce sera forward-compatible parce que l’ancien code pourra juste ignorer les champs ayant un numéro qu’il ne connaît pas.\nOn ne peut juste pas ajouter une donnée obligatoire après une donnée optionnelle, parce que le nouveau code ne pourrait plus lire les données anciennes qui n’auraient pas ce champ obligatoire.\n\n\nSuppression de champ : c’est possible à condition qu’ils soient optionnels (les obligatoires ne pourront jamais être enlevés).\nModification de type de données : c’est parfois possible, mais il y a parfois le désavantage que notre donnée peut être tronquée par le code ancien qui ne lit pas toute la longueur prise par la donnée.\nProtobuf ne possède pas de tableaux mais demande à ajouter un même champ plusieurs fois si on le veut dans un tableau. Cela permet de pouvoir transformer un champ unique en tableau du même type et inversement.\nThrift ne fournit pas cette flexibilité de transformation puisqu’il a un type pour le tableau, mais il supporte les tableaux imbriqués.\n\n\n\n\n\n\n\n\nApache Avro a été développé en 2009 pour Hadoop.\nIl est similaire à Thrift et Protobuf, et a deux schémas : un (Avro VDL) lisible par les humains, et un autre plus pratique pour les machines.\nPour aller chercher un format encore plus compact, Avro ne mentionne pas de numéros pour les champs, il les met simplement les uns à la suite des autres dans le bon ordre, avec juste leur type et le contenu.\nLe support de l’évolution du schéma dans Avro se fait en considérant que la machine qui a écrit la donnée a son schéma, et la machine qui lit a le sien. A partir de ces 2 schémas, et à condition qu’ils soient compatibles, Avro calcule exactement la conversion nécessaire pour que les données lues soient correctement interprétées.\nTous les champs avec une valeur par défaut dans le schéma peuvent être ajoutés, supprimés ou changés d’ordre d’apparition.\nOn peut modifier les types de champs, avec les mêmes problématiques que pour Protobuf et Thrift.\n\n\nL’information du schéma de celui qui a écrit la donnée étant centrale dans l’encodage / décodage, elle doit être fournie avec la donnée.\nPour un grand fichier contenant plein de données, on met le schéma au début du fichier et il concerne toutes les données.\nPour une base de données qui a potentiellement des données avec des schémas différents, on peut ajouter un nombre faisant référence au schéma à chaque entrée de donnée, et avoir une table avec tous les schémas. C’est ce que fait Espresso (la base de données de document de Linkedin, qui utilise Avro)\nPour une connexion par réseau, les deux entités connectées peuvent se communiquer le schéma au début de la connexion et le garder tout au long de celle-ci. C’est ce qui se fait pour le Avro RPC protocol.\n\n\nUn de principaux avantages d’Avro sur Thrift et Protobuf est que comme le numéro des champs n’est pas dans les données, on peut facilement générer des données organisées dans n’importe quel ordre, ou avec des champs en plus etc. les mettre au format Avro avec un schéma associé, et ils pourront être lus sans problèmes. Avro a été conçu pour gérer des données générées dynamiquement, ce qui n’est pas le cas de Thrift et Protobuf.\n\n\n\n\n\n\nIl est intéressant de noter que les données associées à des schémas sont pratiques à bien des égards, que ce soit pour la flexibilité, le faible espace occupé, la documentation vivante que ça fournit. Et ces données se couplent bien avec les bases de données “schemaless” (les non relationnelles principalement donc) qui permettent de gérer les schémas au niveau de l’application.\nQuand on passe les données d’un processus à un autre il faut s’assurer que la donnée est bien comprise malgré les versions des programmes tournant sur ces processus. Il y a 3 manières de passer les données encodées d’un processus à un autre :\n1- Dataflow through databases\nDans le cas des bases de données, le processus qui écrit encode la donnée, et le processus qui lit la décode. Ces processus peuvent embarquer des versions différentes du code, et donc il faut une backward compatibility pour pouvoir lire les données avec l’ancien schéma, et éventuellement une forward compatibility dans le cas où un noeud avec le nouveau code aurait écrit des données, et que ces données doivent être lues avec un noeud dont le code est ancien.\nIl y a aussi une autre chose à laquelle il faut penser dans le code applicatif : pour le cas de la forward compatibility, si un vieux code traite des données possédant de nouveaux champs, il pourra ignorer ceux-ci, mais il faut absolument qu’il pense à les garder s’il veut mettre à jour ces données, sinon elles pourraient être supprimées sans le vouloir.\nContrairement au code qui finit par être chargé à la version la plus récente sur tous les nœuds, la base de données a en général diverses versions des données, certaines de plusieurs années. Dans la mesure du possible on ne remplit pas le contenu des nouveaux champs dans les anciennes entrées, mais on met juste null dedans. Le format Avro fournit une bonne manière de travailler avec des données nouvelles et anciennes de manière transparente.\n\n\n2- Dataflow through services\nLa communication par réseau se fait souvent avec une architecture client / serveur. Par exemple, le navigateur est client et le serveur fournit une API sur laquelle le navigateur va faire des demandes.\nOn a le même principe côté serveur avec l’architecture orientée services (SOA) (contrairement au nom SOAP n’est pas spécifiquement lié à SOA) ou plus récemment avec quelques changements ce qu’on appelle l’architecture microservices. Il s’agit d’avoir des entités indépendantes qui communiquent entre-elles via messages, et qui peuvent être mises à jour de manière indépendante, tout en communiquant avec le même format de données qui assure leur compatibilité.\nQuand des services utilisent le protocole HTTP, on appelle ça des web services. On peut les trouver entre les utilisateurs et les organisations, entre deux services d’une même organisation, ou entre deux organisations avec par exemple les systèmes de carte de crédit ou le protocole OAuth pour l’authentification.\nIl y a 2 approches populaires pour les web services : REST et SOAP (et GraphQL qui a été open sourcé en 2015 et ne figure donc pas dans le livre ?).\nREST : principes de design généraux utilisant à fond les fonctionnalités du protocole HTTP et collant à son fonctionnement (par exemple pour le contrôle du cache, pour le fait d’identifier les ressources avec des URLs).\nOpenAPI (Swagger) permet de documenter convenablement les API REST.\n\n\nSOAP : protocole basé sur XML appelé Web Service Description Language (WSDL). SOAP se base beaucoup sur la génération de code et les outils. Les messages sont en eux-mêmes difficiles à lire par un être humain. Malgré les efforts ostentatoires, l’interopérabilité n’est pas très bonne entre les diverses implémentations de SOAP. SOAP est surtout utilisé dans les grandes entreprises parce qu’il est plus ancien.\n\n\nEn plus des web services, il y a un autre groupe de protocoles de communication à travers le réseau appelé Remote Procedure Call (RPC). Il s’agit en RPC d’appeler des méthodes sur des objets, et d’attendre une réponse de ces appels.\nIl y a d’anciens protocoles spécifiques à un langage ou super complexes comme EJB (Java), DCOM (Microsoft), COBRA (trop complexe et non backward compatible). Mais il y a aussi de nouveau protocoles comme gRPC utilisant Protobuf, Finagle qui utilise Thrift, Rest.li qui utilise JSON sur HTTP, et Avro et Thrift qui ont leur propres implémentations de RPC.\nLes protocoles RPC essayent de faire passer les appels réseau pour des appels à des méthodes, mais ces choses sont de nature complètement différente : un appel réseau est imprédictible, il peut prendre un temps variable, il peut finir en timeout, il peut réussir tout en n’envoyant pas de réponse, il ne peut pas stocker de valeurs en mémoire liés par des pointeurs etc. REST quant à lui assume que les appels réseau sont de nature bien différente en les présentant comme tels.\nCeci est à tempérer un peu avec les implémentations récentes de RPC qui sont plus explicites sur la nature différente en fournissant par exemple des promesses pour encapsuler les appels asynchrones.\n\n\nBien que les protocoles RPC avec un encodage binaire permettent une plus grande performance que du JSON par dessus REST, REST bénéficie d’un débuggage plus facile avec la possibilité de tester à travers les navigateurs, il est supporté partout, et il est compatible avec un large panel d’outils construits autour (serveurs, caches, proxies etc.). Pour toutes ces raisons, le RPC est utilisé seulement au sein d’une même organisation, typiquement dans un même datacenter.\n\n\n\n\n\n\n3- Message-passing dataflow\nIl existe une manière de transmettre des données entre 2 processus qui se situe entre les appels RPC et les messages passés par une base de données : il s’agit de la transmission de messages asynchrone. On ne passe pas par le réseau mais par un message broker (ou message queue).\nLes avantages de cette approche comparé à RPC sont :\nLe broker peut faire buffer le temps que le(s) consommateur soit disponible.\nLe message peut être redélivré en cas d’échec ou de crash.\nCelui qui envoie et qui reçoit ne se connaissent pas, il y a un découplage à ce niveau.\nIl peut y avoir plusieurs consommateurs d’une même queue.\n\n\nUn des inconvénients potentiels c’est que le receveur n’est pas censé répondre. L'envoyeur envoie puis oublie.\nLes message brokers sont historiquement des logiciels propriétaires comme TIBCO, IBM WebSphere, et webMethods. Plus récemment on a des brokers open source comme RabbitMQ, ActiveMQ, HornetQ, NATS et Apache Kafka.\nLes message brokers n’imposent en général pas de format de données, donc on peut très bien utiliser les formats Avro / Thrive / Protobuf, et profiter de leur flexibilité pour pouvoir déployer indépendamment les producteurs et consommateurs.\n\n\n\n\n\n\nL’actor model consiste à se débarrasser de la problématique de concurrence avec la gestion de threads et de ressources partagées en créant des actors indépendants, ayant chacun leurs états encapsulés, et communiquant avec les autres actors via messages asynchrones.\nDans la version distribuée, ces interlocuteurs peuvent alors être sur le même nœud ou sur un nœud différent, auquel cas le message sera sérialisé pour être transmis via le réseau de manière transparente.\nIl y a une plus grande transparence vis-à-vis du fait que les messages peuvent être perdus qu’avec RPC.\nUn framework actor distribué inclut un broker pour transmettre les messages. Il y en a 3 qui sont populaires :\nAkka qui utilise la sérialisation de Java, mais peut être utilisé avec par exemple Protobuf pour permettre une meilleure backward/forward compatibilité.\nOrleans supporte les rolling upgrades avec son propre système de versionning.\nErlang OTP supporte les rolling upgrades mais il faut les planifier avec attention.","5---replication#5 - Replication":"Il y a 3 raisons pour vouloir faire de la réplication :\nGarder une copie des données proche des utilisateurs et donc avoir une faible latence.\nPermettre au système de fonctionner même si certains composants sont foutus.\nPour scaler le nombre de machines et donc le nombre de requêtes qu’on peut traiter.\n\n\nTout l’enjeu de la réplication réside dans le fait de propager les changements dans tous les réplicats. Il y a 3 algorithmes pour ce faire : single-leader, multi-leader, et leaderless.\nLa réplication des BDD distribuées a été étudiée depuis les années 70 et n’a pas changé depuis parce que la nature des réseaux n’a pas changé. En revanche, l'utilisation industrielle de ces techniques est quant à elle récente.\nLa réplication la plus évidente est la leader-based replication. Pour écrire une donnée il faut le faire auprès du nœud leader, qui va mettre à jour sa copie de la BDD et envoyer un log (ou stream) de mise à jour à tous les nœuds suiveurs qui vont l’appliquer.\nCe type de réplication est intégré au sein :\ndes BDD relationnelles suivantes : PostgreSQL, MySQL, Oracle Data Guard, SQL Server’s AlwaysOn Availability Groups.\ndes BDD non relationnelles suivantes : MongoDB, RethinkDB, Espresso.\nEt même au sein de message brokers distribués comme : Kafka et RabbitMQ.\n\n\nLa réplication peut être synchrone ou asynchrone. Si elle est synchrone, alors le nœud leader doit attendre que tous les suiveurs aient répondu “ok” de leur côté pour répondre à son tour que la transaction s’est bien passée. Si elle est asynchrone, il répond tout de suite même s' il y a eu un problème du côté des followers.\nDans la pratique on choisit rarement le mode synchrone parce que n’importe lequel des nœuds pourrait mettre plusieurs minutes à répondre à cause de problèmes réseau.\nOn choisit parfois une réplication semi-synchrone, qui consiste à avoir un seul nœud suiveur synchrone, et le reste asynchrones. De cette manière on est assurés d’avoir les données à jour sur au moins 2 nœuds. Et si le nœud suiveur synchrone ne répond plus, on promeut un autre nœud suiveur comme synchrone pour le remplacer.\nLa réplication asynchrone est également souvent choisie, surtout si les suiveurs sont nombreux ou distribués géographiquement.\n\n\nL’ajout d’un noeud suiveur supplémentaire se fait en faisant un snapshot de la base de données du noeud leader, en copiant ça sur le nouveau noeud, puis en demandant au leader tous les logs de mise à jour (toutes les transactions) qui ont eu lieu depuis le snapshot. Le nouveau nœud peut alors rattraper son retard et devenir un nœud suiveur normal.\nEn cas d’échec :\nd’un nœud suiveur : le nœud sait à quel log il s’est arrêté, donc dès qu’il va mieux, il peut demander au nœud leader l’ensemble des transactions qu’il a ratées, et se remettre à jour. Ça s'appelle le catch-up recovery.\nd’un nœud leader : c’est plus compliqué à gérer. Il faut un timeout pour déterminer qu’un nœud leader est en échec, et passé ce timeout on entame un processus de failover c’est-à-dire de remplacement du leader.\nLa procédure peut être automatique ou manuelle. Un timeout trop long peut mener à une interruption du service trop longue, et un timeout trop court dans un contexte de surcharge peut mener à gérer encore moins bien la charge. Pour cette raison, certaines organisations préfèrent la méthode manuelle.\nLe choix du nouveau leader est un problème de consensus, discuté plus tard dans le livre. A priori le nœud le plus à jour serait le meilleur choix.\nIl est possible que l’ancien leader revienne et pense qu’il est toujours le leader en acceptant les opérations en écriture. C’est une situation dangereuse qu’il faut prévoir correctement.\nDans le cas de réplication asynchrone, certaines transactions peuvent ne pas avoir été passées aux suiveurs. Si l’ancien leader revient en tant que suiveur ensuite, que faire de ces transactions ? En général on les supprime, mais c’est pas trop trop.\nEt ça peut être même problématique si les transactions sont en lien avec d’autres outils. Par exemple chez Github, un suiveur asynchrone MySQL était devenu leader avec des transactions manquantes. Il se trouve que la clé primaire était aussi utilisée dans un cache Redis qui lui avait les nouvelles transactions. Comme les nouvelles entrées ont été assignées à des valeurs de la clé primaire qui avaient existé auparavant, des utilisateurs ont pu avoir accès à des clés privées d’autres utilisateurs, issus du cache.\n\n\n\n\n\n\nFonctionnement de la réplication au niveau des messages (logs) :\nStatement-based replication : il s’agit de faire suivre toutes les instructions de base de données aux suiveurs. Par exemple un INSERT, un UPDATE, un DELETE etc.\nDans le cas d’instructions non déterministes comme RAND(), on va se retrouver avec des valeurs différentes dans les suiveurs.\nPour les champs auto-incrémentés, il faut absolument que l’ordre des requêtes soit exactement le même, ce qui limite les transactions concurrentes.\nIl est possible de travailler à rendre déterministe toutes les instructions qui pourraient poser problème, en envoyant une valeur plutôt qu’une instruction dans ces cas-là, mais il y a plein d’edge cases à traiter.\nEn général cette approche n’est pas très utilisée pour cette raison-là. MySQL l’utilisait jusqu’à une certaine version, mais utilise l’approche row-based replication depuis. VoltDB utilise en revanche cette approche.\n\n\n\n\nWrite-ahead log (WAL) shipping : il s’agit d’envoyer aux suiveurs le log des messages bas niveau (tel qu’il est utilisé par les BDD LSM-Tree, ou tel qu’il est utilisé par les B-Tree le temps que l’opération se fasse, et pour pouvoir la refaire à partir de ce log en cas d’échec).\nL’avantage c’est que c’est déterministe, mais l’inconvénient c’est que les messages sont couplés à une implémentation bas niveau de la BDD. Ce qui veut dire qu’on ne peut pas faire tourner une version différente entre le leader et les followers. Et donc exit les zero-downtime rolling updates : il faut une période de downtime.\nCe mécanisme est utilisé par PostgreSQL et Oracle.\n\n\nLogical (row-based) log replication : il s’agit de faire un peu la même chose qu’avec le WAL, mais on utilise un format de log indépendant de la BDD, avec les fonctionnalités minimales pour pouvoir mettre à jour correctement la BDD.\nOn est donc découplé du format bas niveau utilisé par la BDD, et on peut faire du zero-downtime.\nÇa permet aussi d’envoyer les logs à une autre base de données de type data warehouse en temps réel.\nMySQL binlog peut être configuré pour utiliser ce mécanisme.\n\n\nTrigger-based replication : dans le cas où on recherche plus de flexibilité, plutôt que d’utiliser les mécanismes built-in des BDD, on peut bouger la logique de réplication au niveau applicatif.\nOracle GoldenGate permet de mettre à disposition les logs de la BDD pour le code applicatif et implémenter ce mécanisme.\nUn autre moyen de l’implémenter est d’utiliser les triggers et stored procedures qui existent dans la plupart des BDD relationnelles. On peut grâce à ça exécuter du code applicatif à chaque transaction. Le résultat est placé dans une table à part et lu par un processus à part.\nDatabus for Oracle et Bucardo for Postgres font ça par exemple.\n\n\nCe mécanisme arrive avec son lot de bugs, et est moins performant. Mais il offre de la flexibilité.\n\n\n\n\nDans le cas où on choisit la leader-based replication avec des followers asynchrones parce qu’on cherche à scaler en lecture en ayant plein de followers, les followers vont se retrouver régulièrement en retard. En général c’est une fraction de seconde, mais dans la montée en charge ou avec des problèmes de réseau ça peut devenir des minutes. Ce retard s’appelle le replication lag. Et on parle d’eventual consistency pour désigner ce problème de consistance momentané des données.\nParmi les problèmes survenant il y a :\nread-after-write consistency : le fait, pour un utilisateur, de pouvoir lire ses propres writes juste après : s’il écrit un message et recharge la page, et qu’il ne voit pas son message, il pourrait se mettre à paniquer.\nOn peut lire toute donnée qui a potentiellement été modifiée par l'utilisateur depuis le leader, et les autres depuis les suiveurs. Par exemple, le profil d’un utilisateur ne peut être modifié que par lui, donc on le lit depuis le leader.\nDans le cas où la plupart des données sont potentiellement modifiables par l’utilisateur, on perdrait l’intérêt du scaling à tout lire depuis le leader. On peut alors par exemple ne lire depuis le leader que les données qui ont été modifiées dans les dernières minutes, ou encore monitorer le replication lag pour ne pas lire depuis les suiveurs qui sont trop en retard.\nLe client peut retenir le timestamp (temps ou donnée d’ordre logique) de la dernière écriture, et l’envoyer avec la requête. Le serveur peut alors n’utiliser que les suiveurs qui sont à jour jusqu'à ce timestamp, ou attendre qu’ils le soient avant de répondre.\nDifficulté supplémentaire dans le cas de dispersion géographique : toute requête envoyée au leader ne sera pas forcément proche de l’utilisateur.\nAutre problématique : si on veut que l’utilisateur puisse voir ses écritures depuis tous ses outils (navigateur, mobile etc.).\nÇa rend inopérant la technique de se souvenir de la dernière modification côté client puisqu’il y a alors plusieurs clients.\nOn a aussi des problèmes supplémentaires dans le cas où il y a plusieurs datacenters. Les deux appareils pourraient être dirigés vers des datacenters différents.\n\n\n\n\nmonotonic reads : un utilisateur pourrait obtenir des données récentes depuis un replica à jour, puis recharger la page et obtenir des données anciennes depuis un replica moins à jour. Ça donne l’impression d’aller dans le passé.\nPour éviter ce phénomène on peut servir un même utilisateur toujours avec le même nœud suiveur tant que celui-ci est vivant.\n\n\nConsistent prefix reads : il s’agit ici de respecter l’ordre causal des choses. Il faut que les données écrites en BDD le soient toujours dans le bon ordre. C’est un problème qui survient quand on partitionne la BDD (on en parlera au chapitre suivant).\nOn peut alors essayer de mettre les informations liées entre-elles dans la même partition.\nOn peut aussi utiliser des algorithmes qui empêchent les données d’être dans un ordre non causal.\n\n\nConcernant le replication lag et l’eventual consistency qui en résulte, une des solutions pour y répondre s’appelle les transactions. Leur but est d’abstraire tout l’aspect distribué du code applicatif, et de s’occuper de répondre aux problèmes décrits ici (read-after etc.). Certaines personnes disent d’abandonner les transactions qui seraient trop coûteuses, mais on nuancera ça par la suite.\n\n\n\n\nOn a ensuite la multi-leader replication. On a plusieurs leaders qui mettent à jour des suiveurs, et qui se mettent aussi à jour entre eux.\nEn général la complexité supplémentaire induite par le fait d’avoir plusieurs leaders n’en vaut pas la peine si on n’a qu’un seul datacenter.\nLes avantages du multi-leader dans un environnement multi datacenter :\nOn peut par exemple avoir un leader par datacenter, ce qui permet d’éviter de traverser la terre pour faire des requêtes d’écriture, donnant une perception de performance aux utilisateurs.\nUn datacenter entier et son leader peut avoir un problème, puis rattraper son retard sur les autres datacenters dès que c’est bon.\nLes erreurs réseau hors du datacenter impactent moins ce qui se passe dans le datacenter, étant donné qu’on n’est pas obligé d’aller chercher un nœud leader d’un autre datacenter pour écrire.\n\n\nCertaines BDD supportent le multi-leader nativement, mais en général il faut un outil externe. C’est le cas pour Tungsten Replicator pour MySQL, BDR pour PostgreSQL et GoldenGate pour Oracle.\nIl y a également CouchDB qui a été conçu pour permettre de résoudre facilement les situations de multi-leader.\n\n\nCes deux exemples illustrent le même principe que la réplication multi-leader :\nUne application de calendrier pour mobile, desktop etc. pourrait fonctionner en maintenant une copie de la BDD dans chaque client, les laissant ajouter des événements même en étant hors ligne, et synchroniser les BDD quand les clients sont à nouveau en ligne. On a bien plusieurs leaders qui peuvent écrire, une possibilité d’eventual consistency le temps que le réseau revienne, et un travail de résolution des conflits à faire.\nLes applications d’édition collaborative de texte comme Etherpad ou Google Docs fonctionnent comme si plusieurs leaders pouvaient faire des changements sur leur propre version locale, et propager ces changements de manière asynchrone. Ca aurait pu être du single-leader si chaque personne prenait un lock avant de faire un changement (un peu comme dans dropbox), mais là chaque changement est ajouté à un niveau vraiment atomique au document.\n\n\nLe problème principal de la multi-leader replication c’est les conflits entre leaders ayant eu chacun une transaction en écriture sur une même donnée.\nOn pourrait demander aux leaders d’attendre que l’autre leader ait fini sa transaction avant d’en accepter une, mais alors on reviendrait à la position de single-leader, on perdrait l’avantage d’avoir plusieurs leaders acceptant des connexions en même temps.\nDans la mesure du possible, vu que la résolution de conflit est complexe et en général mal gérée, il vaut mieux éviter les conflits. On peut par exemple rediriger les requêtes d’un même utilisateur toujours vers le même datacenter.\nParfois un datacenter est hors d’usage, ou un utilisateur peut se déplacer et se rapprocher d’un autre datacenter, et on va vouloir le rediriger vers un autre leader. Il faut alors faire converger le conflit vers un état consistent :\nOn peut donner un identifiant à chaque transaction, basé sur un timestamp, un nombre aléatoire etc. et se dire que le plus grand nombre gagnera lors du conflit pour faire valider sa transaction, annulant l’autre. Dans le cas du timestamp c’est du last write wins (LWW). C’est très populaire mais on perd des transactions.\nOn peut donner un identifiant à chaque leader, et se dire que celui qui a le plus grand gagne toujours la résolution de conflit. Mais c’est pareil qu’avec l’identifiant de transaction : on va perdre des données.\nOn peut choisir une stratégie de fusion des données des 2 requêtes, par exemple ordonner le texte alphabétiquement et le concaténer.\nBucardo par exemple permet d’écrire un bout de code en perl pour choisir quoi faire des requêtes en conflit dès que le conflit apparaît au moment de l’écriture.\n\n\nEnregistrer le conflit avec les 2 données quelque part, et laisser le code applicatif gérer ça par exemple en demandant à l’utilisateur quoi faire pour ce conflit.\nCouchDB fonctionne de cette manière-là : il stocke les 2 données, puis à la lecture les envoie toutes les deux à l’application..\n\n\n\n\nUn conflit peut être de manière évidente la modification d’un même champ, mais ça peut aussi être plus subtile et difficile à détecter. Par exemple une vérification au niveau applicatif qu’une chambre d'hôtel ne peut être réservée et donc mentionnée que par une seule réservation. Si le code applicatif a validé la requête, mais qu’on en l’a faite une dans chaque leader, on va réserver deux fois.\n\n\nQuand on a 2 leaders, ils vont forcément s’envoyer chacun des updates. Mais si on en a plus, alors on peut avoir diverses topologies de propagation des mises à jour entre leaders :\nLa star topology consiste à avoir un leader au centre qui va mettre à jour tous les autres, et prendre des mises à jours d’eux.\nLa circular topology consiste à ce que chaque leader mette à jour son voisin, et finissant en boucle. Une information au niveau de la requête permet alors de savoir si elle a déjà été traitée par le nœud courant pour arrêter la boucle.\nLa plus générale est le all-to-all topology, où tous les leaders mettent à jour tous les autres.\nUn des avantages du all-to-all est que si un nœud ne fonctionne plus, c’est transparent. Pour le circular et star il pourrait bloquer l’information et il faut alors reconfigurer la topologie.\nL’inconvénient des all-to-all est que certaines connexions peuvent être plus rapides que d’autres, et alors si on se basait sur des timestamp pour l’ordre des transactions par exemple, cet ordre pourrait ne pas être bon.\nPour pouvoir faire quand même respecter la causalité dans ce cas, on peut utiliser la technique des version vectors.\n\n\nGlobalement la résolution de conflits est plutôt mal gérée dans les solutions existantes : par exemple PostgreSQL BDR ne fournit pas de garantie causale des écritures, et Tungsten Replicator pour MySQL ne détecte même pas les conflits.\n\n\n\n\n\n\nOn a enfin la leaderless replication, qui consiste à ce que tous les nœuds puissent accepter les requêtes en lecture et écriture.\nCette idée était tombée dans l’oubli depuis longtemps et a été remise au goût du jour quand Amazon l’a implémentée dans sa base de données Dynamo system. Elle est depuis utilisée dans les BDD open source Riak, Cassandra et Voldemort. Elles sont connues pour être les “BDD Dynamo-style”.\nEn cas de problème dans un des nœuds, celui-ci va rater des requêtes. Et quand il reviendra, ses données seront anciennes et le client risque d’obtenir des données pas à jour en lisant depuis ce nœud.\nPour résoudre le problème chez le client, le client peut envoyer la requête à tous les nœuds, et à la réception utiliser la version la plus à jour parmi ceux reçus, grâce à des numéros de version dans ces messages.\nPour s’assurer que le noeud se remet à jour il y a 2 moyens implémentés dans les systèmes Dynamo-style :\nRead repair : Quand le client lit une valeur en parallèle depuis tous les réplicas, s’il constate une différence chez l’un d’entre eux qui aurait une version de transaction plus ancienne, il le met à jour avec une requête d’écriture. Ceci permet de mettre à jour les valeurs souvent lues.\nAnti-entropy process : Pour les valeurs peu lues, on peut avoir des tâches de fond qui tournent, et dont le but est de repérer les différences entre nœuds, et mettre à jour ceux qui sont en retard.\n\n\n\n\nPour savoir si une requête a réussi, on peut utiliser le quorum consistency.\nSoit :\nn le nombre de nœuds à qui on envoie les reads et writes.\nw le nombre de nœuds qui doivent confirmer un write pour le considérer comme réussi.\nr le nombre de nœuds qui doivent confirmer un read pour qu’il soit considéré comme réussi.\n\n\nAlors pour respecter le principe du quorum il faut que w + r > n.\nTypiquement on choisit n impair, et w = r = (n + 1) / 2 (arrondi au supérieur).\nSi on a beaucoup de lectures et peu d’écritures, on pourra mettre r = 1, comme ça dès qu’un seul nœud valide la lecture alors la transaction est validée. Les lectures sont alors plus rapides mais un seul nœud qui est down empêche alors l’écriture en BDD pour respecter la formule w + r > n).\n\n\nAvantages et inconvénients du quorum :\nL’intérêt de ce quorum c’est que les reads et writes se chevauchent, et donc qu’il y ait forcément au moins un nœud qui soit complètement à jour, pour être sûr que la donnée lue qui sera gardée sera complètement à jour.\nOn peut très bien choisir de ne pas respecter la formule du quorum et avoir moins de reads et writes nécessaires pour la validation. On aura alors une plus faible latence, une plus grande availability, mais une moins bonne consistance (on aura régulièrement des lectures renvoyant des données pas tout à fait à jour).\n\n\nMême avec le quorum, on peut se retrouver avec des données pas à jour dans certains cas :\nSi on utilise le sloppy quorum, on peut se retrouver avec les writes sur d’autres nœuds que les reads, et donc le chevauchement n’est plus garanti.\nIl s’agit d’une option activable sur les BDD qui permet, dans le cas où une large partie de noeuds est momentanément non disponible, de choisir de prendre quand même les writes sur d’autres noeuds partitionnés qui ne font pas habituellement partie de n pour ces valeurs-là. Et quand les nœuds sont de retour, on leur donne ces valeurs (hinted handoff). Le problème c’est que pendant le temps où ils n’étaient pas là, ils avaient peut être certaines valeurs plus à jours qu’eux seuls avaient, et les reads ont pu être servis avec des valeurs pas à jour.\n\n\nDans le cas de writes concurrents, on est en présence d’un conflit qu’il faut résoudre comme discuté précédemment. Si on choisit de résoudre en annulant une des requêtes, alors on perd des données.\nSi un read se fait en concurrence avec un read, le write pourrait être effectif chez certains replicas, et on ne sait pas ce que retournera alors le read.\nSi un write a réussi sur certains réplicas mais pas tous, et que la transaction est en voie d’annulation, les réplicas où ça a réussi peuvent renvoyer cette valeur qui sera fausse.\nSi le nœud à jour échoue, le nombre de nœuds en écriture tombe en dessous de w, et on peut n’avoir aucun nœud qui a la version à jour par rapport aux écritures déjà validées au moment de répondre.\nDes problèmes de timing dont on parlera plus tard peuvent aussi survenir.\n\n\nOn voit bien que les Dynamo-style databases ne garantissent qu’une eventual consistency, même en respectant le quorum. Pour avoir des garanties plus fortes comme le “read your writes”, “monotonic reads” etc. il faudra faire appel aux transactions et au consensus.\n\n\nMalgré l’eventual consistency de la leaderless replication, il peut être important de quantifier à quel point les données sont peu à jour dans les divers nœuds. Il faudrait mettre en place du monitoring mais c’est beaucoup moins simple que pour le leader-based où on peut facilement observer le replication lag du leader vers les followers. Là on peut avoir des valeurs peu lues très anciennes.\nLa leaderless replication est tout à fait aussi adaptée au multi-datacenter :\nCassandra et Voldemort traitent les nœuds dans les divers datacenters comme des nœuds normaux, avec le n global et un n configurable pour chaque datacenter. En général les clients n'attendent que le quorum du datacenter le plus proche pour maximiser le temps de réponse.\nRiak ne fait du leaderless classique qu’au sein des datacenters, la synchronisation cross-datacenter se fait de manière asynchrone, un peu à la manière du multi-leader replication.\n\n\n\n\nA propos de la gestion des écritures concurrentes :\nIl faut noter que malheureusement ça ne se fera pas automatiquement par les implémentations des BDD qui sont relativement mauvaises. En tant que développeur, il faut connaître ces problèmes et implémenter des solutions nous-mêmes.\nVoici quelques éléments de réflexion :\nLast write wins (LWW) : on en avait parlé, il s’agit d’éliminer une des deux transactions concurrentes en déterminant par une méthode arbitraire laquelle est la dernière dans le cas où il n’y a pas de relation de causalité. C’est arbitraire parce que la causalité entre des événements qui ne se connaissent pas n’a pas de sens. C’est ça qu’on appelle des transactions concurrentes.\nC’est problématique parce que même après avoir dit au client que la transaction s’est bien passée, elle peut être annulée en arrière-plan de manière silencieuse.\nLWW est la seule méthode de résolution de conflit supportée par Cassandra, et une feature optionnelle dans Riak. Dans Cassandra il est recommandé d’utiliser un UUID comme clé pour éviter autant que possible des écritures concurrentes.\n\n\nCe qu’il nous faut donc c’est pouvoir distinguer deux événements concurrents de deux événements causaux. Dans le cas où c'est causal on pourra faire respecter l’ordre. C’est seulement dans le cas de la concurrence qu’on est condamné à perdre des données, fusionner les données ou avertir l’utilisateur.\nPour fusionner les données, on peut utiliser des structures spéciales qui le permettent facilement comme les structures CRDT supportés par Riak.\nEn interne il s’agit de fusionner les éléments dans une liste en cas d’ajout, et de poser des tombstones dans le cas d’une suppression plutôt que de supprimer directement. Cela permet de mieux gérer la suppression au niveau de plusieurs nœuds qui en prennent connaissance au fur et à mesure, et ont besoin d’effectuer l’opération eux-aussi.\n\n\nPour distinguer les événements causaux des concurrents, on peut utiliser les version vectors. Chaque replica a sa version qu’il incrémente à chaque traitement, et l’ensemble de ces versions sont appelées version vector. Ces valeurs sont utilisées par chaque réplica pour déterminer s’il y a de la causalité ou si on garde les deux versions concurrentes.\nLes version vectors sont disponibles dans Riak 2.0, et sont appelés causal context. Le version vector est envoyé aux clients quand les valeurs sont lues, et renvoyé par les clients quand une valeur est écrite.","6---partitioning#6 - Partitioning":"Les partitions sont appelées :\nshard dans MongoDB, Elasticsearch et SolrCloud\nregion dans HBase\ntablet dans Bigtable\nvnode dans Cassandra et Riak\nvBucket dans Couchbase\n\n\nUn cluster shared-nothing signifie qu’il s’agit de plusieurs machines distinctes, par opposition au scaling vertical où c’est la même machine qui partage le processeur, la RAM etc. là on ne partage rien à part à travers le réseau.\nLes partitions existent depuis les années 80, et ont été redécouvertes par les BDD NoSQL et les Data warehouses Hadoop-based.\nVis-à-vis de la réplication, la notion de partition vient s’y superposer. On peut par exemple avoir des nœuds (ordinateurs) avec plusieurs partitions, et chacun d’entre eux peut être soit leader soit follower pour telle ou telle copie de telle ou telle partition.\nLa raison principale de vouloir des partitions est la scalabilité. Avec la réplication on pouvait scaler pour lectures, mais le partitionnement permet de scaler aussi en écriture.\nCependant, pour que le scaling fonctionne bien, il faut que la charge soit équitablement répartie entre les nœuds. Pour ce faire, on peut par exemple répartir aléatoirement les données dans les partitions (mais ça nécessiterait de demander à tous les nœuds en parallèle à chaque recherche).\nQuand la charge est mal répartie on appelle ça des partitions skewed (biaisées). Et quand un seul nœud se retrouve à tout gérer on l’appelle le hot spot.\n\n\nParmi les types de partitions on a :\nLa partition par key range. On va attribuer un range de clés à chaque nœud, et y stocker ces données-là. Si on connaît ce range à l’avance, on pourra même directement demander au nœud concerné pour notre recherche.\nOn pourra par exemple avoir le 1er nœud qui a les clés commençant par A et B, et le dernier les clés commençant par W, X, Y et Z.\nBigtable et son équivalent open source HBase, ainsi que RethinkDB et MongoDB jusqu’à la version 2.4 utilisent cette technique.\nDans chaque partition, on peut garder les entrées triées de la même manière que les LSM-Tree.\nOn a un risque de hot spot, par exemple dans le cas où on recherche par la clé qui serait le timestamp, et que les partitions sont groupées par journée. La partition du jour courant risque de devenir un hot spot. Dans ce cas on peut préfixer la clé par un nom ou autre chose, pour constituer un index concaténé par exemple.\n\n\nLa partition par hash of key. On a la même manière de stocker par clé qu’avant, sauf qu’on va hasher la clé avec une fonction de hash simple (mais qui ne donne pas de duplicata). Et on va assigner des ranges de hashs aux partitions. Ceci fait que les données seront aléatoirement réparties.\nMongoDB, Cassandra et Voldemort utilisent ce mécanisme.\nLe désavantage de hasher la clé c’est qu’on ne peut plus faire facilement de recherche par range.\nDans MongoDB, si on a activé les clés hashées, il faut envoyer les queries de range à toutes les partitions.\nRiak, Couchbase et Voldemort ne supportent pas du tout les queries de range.\nCassandra utilise un compromis entre les deux stratégies (hash et clé normale) : on a une clé composée avec une première partie hashée déterminant la partition, et ensuite une 2ème partie permettant de faire une recherche, y compris de range, dans la SSTable. On doit donc d’abord fixer la partition et ensuite on peut chercher ce qu’on veut efficacement.\n\n\nHasher la clé peut parfois ne pas suffire à éliminer les hot spot : dans le cas spécifique où on a une donnée qui est accédée / écrite de manière massive (par exemple une célébrité qui est fortement suivie qui s’exprime), il faut diviser cette entrée-là en plusieurs entrées sur plusieurs machines. On peut par exemple préfixer le hash d’un nombre et le répartir sur 100 machines différentes. Mais alors les lectures devront à chaque fois faire appel à toutes ces partitions et reconstruire la bonne donnée.\nPour le moment les BDD ne gèrent pas automatiquement ce genre de fonctionnalité, donc il faut le faire à la main.\n\n\nIl existe un concept appelé consistent hashing, mais il est surtout utilisé pour les caches, et n’est pas efficace avec les BDD. Certaines docs de BDD l'invoquent par erreur, mais pour éviter la confusion il vaut mieux qu’on parle de hash partitioning.\n\n\n\n\nLes indexes secondaires sont extrêmement pratiques pour faire des recherches dans la BDD, mais elles introduisent une complexité supplémentaire.\nPar rapport à leur support :\nHBase et Voldemort ont évité de les supporter pour éviter la complexité de l’implémentation.\nRiak a commencé leur support.\nPour Elasticsearch et Solr, ils sont leur raison d’être.\n\n\nOn a 2 manières de les implémenter avec le partitionnement :\nDocument-based partitioning : on va créer un index local à la partition. Toutes les entrées de la partition seront indexées pour la colonne choisie, mais l’index n’aura aucune idée de ce qui est indexé sur une autre partition.\nLe problème c’est que quand on veut faire une recherche par index secondaire, on va alors devoir faire une requête auprès de toutes les partitions, puisque les partitions sont séparées par clé primaire, pas par l’index secondaire. On appelle ça le scatter / gather. Ceci fait que la requête va coûter cher, et devoir attendre que tous les nœuds répondent (donc on est soumis au problème des hauts percentiles qui nous ralentissent potentiellement beaucoup).\nCette approche est utilisée quand même dans MongoDB, Riak, Cassandra, Elasticsearch, SolrCloud et VoltDB.\n\n\nTerm-based partitioning : on crée un index global. Mais bien entendu il est hors de question de le mettre sur un seul nœud, au risque que ça devienne un bottleneck. On va le partitionner de même qu’on a partitionné l’index primaire : les premières clés de l’index secondaires seront dans la partition 1, celles juste après dans la partition 2 etc.\nCette technique rend la recherche rapide : puisqu’on sait quel nœud contient l’index qu’on veut, on lui envoie la requête directement. Par contre l’écriture est plus lente puisqu’elle va impliquer des modifications dans plusieurs partitions (celle de la donnée et de l’index primaire, et celle de l’index secondaire pour le mettre à jour).\nEn pratique, la mise à jour de l’index secondaire avec le term-partitioning se fait de manière asynchrone, et tant pis si une recherche avec l’index secondaire immédiatement après une écriture ne fonctionne pas.\nParmi les implémentations :\nAmazon DynamoDB met à jour son index term-partitioned de manière asynchrone.\nRiak et Oracle data warehouse permettent de choisir la technique de partitionnement de l’index secondaire.\n\n\n\n\n\n\n\n\nRégulièrement, pour augmenter les capacités ou remplacer une machine malade, on doit rediriger les requêtes et déplacer les données d’une machine à l’autre. On appelle ça le rebalancing entre partitions. Il y a plusieurs stratégies pour l’implémenter :\nUne stratégie à ne pas faire : hash mod N. Si on décidait de faire le modulo du hash de nos transactions pour les répartir dans les noeuds (par exemple le hash % 12 si on a 12 noeuds), alors à chaque fois que le nombre de noeuds changerait, on devrait faire du rebalancing, ce qui est beaucoup trop coûteux.\nFixed number of partitions. On va choisir un grand nombre de partitions, plus grand que le nombre de nœuds qu’on imagine qu’on va avoir, et on va attribuer plusieurs partitions par nœud (par exemple 100 par nœud). De cette manière, dès qu’on ajoute ou supprime un nœud, on peut déplacer quelques partitions ici et là pour équilibrer le tout.\nIl faut bien choisir le bon nombre de partitions, s’il y en a trop ça crée un manque de performance du fait de chercher dans trop de partitions, s’il n’y en a pas assez on va déplacer de trop gros blocs au moment du rebalancing. Ça peut être difficile à trouver si notre charge varie beaucoup.\nCette approche est utilisée par Riak, Elasticsearch, Couchbase et Voldemort.\n\n\nDynamic partitioning. Pour les BDD utilisant le partitionnement de type key range (et pas hash range), avoir un nombre de partitions fixe peut être problématique par rapport au skewing, et choisir à la main combien en mettre par nœud est fastidieux. On va donc vouloir un système qui répartir dynamiquement les partitions, par rapport à la quantité de données présente dans chaque partition.\nQuand une partition est jugée dynamiquement trop grosse, elle est coupée en 2 et une moitié est éventuellement déplacée sur un autre nœud.\nHBase et RethinkDB par exemple utilisent le partitionnement dynamique (puisqu’ils utilisent aussi le key range partitioning).\nPour le key range c’est obligatoire, mais le dynamic partitioning peut aussi être utilisé avec le hash range partitioning. MongoDB par exemple donne le choix de key range ou hash range, et dans les deux cas fait le rebalancing de manière dynamique.\n\n\nPartitioning proportionally to nodes. Le nombre fixe de partitions et le nombre dynamique de partitions est basé sur la taille des partitions. On peut choisir plutôt de se baser sur le nombre de partitions par nœud indépendamment de leur taille. On fixe un nombre de partitions par nœud et on répartit les données dedans. Si on ajoute un nœud, les partitions existantes maigrissent pour transférer une partie de leur données dans les partitions du nouveau nœud.\nCassandra et Ketama utilisent cette méthode.\n\n\nOn a un peu évoqué l’aspect manuel / automatique, mais plus concrètement :\nCouchbase, Voldemort et Riak créent des suggestions de rebalancing automatiquement, mais demandent la validation d’un administrateur humain pour opérer le rebalancing.\nLe rebalancing complètement automatique peut être tentant, mais il faut bien voir que c’est une opération longue et coûteuse, et que faire un mauvais rebalancing dans certaines conditions peut créer une cascade d’échec, le système croyant à tort que certains noeuds surchargés sont morts ou ce genre de chose. Globalement avoir un humain dans la boucle du rebalancing est une bonne idée.\n\n\n\n\nA propos de la question du routing de la requête, comment le client va savoir à quel nœud envoyer sa requête ?\nIl existe plusieurs solutions open source. Globalement 3 possibilités se dégagent :\nLe client envoie à un nœud en mode round robin (chacun son tour), et ce nœud qui connaît le bon nœud va faire lui-même la demande, va réceptionner la réponse, et la retransférer au client.\nLe client envoie la requête à un routing tier qui connaît le partitionnement actuel, et va pouvoir envoyer la requête au bon nœud.\nLe client connaît déjà le bon nœud, et va directement lui envoyer la requête.\n\n\nDans tous les cas, il y a le problème de savoir comment l’entité qui connaît le partitionnement actuel reste à jour malgré les rebalancing ? C’est un problème difficile.\nIl y a des protocoles pour atteindre un consensus dans les systèmes distribués, mais ils sont compliqués. On en parlera au chapitre 9.\nDe nombreux systèmes utilisent un service dédié au mapping entre partition / nœud et adresse ip.\nZooKeeper est l’un d'entre eux : tous les nœuds s’enregistrent auprès de ZooKeeper et lui notifient les rebalancings. C’est lui qui fait autorité en matière de routing. Et il notifie les entités qui en ont besoin (par exemple le routing tier) de l’état du réseau de nœuds / partitions.\nEspresso de Linkedin utilise Helix, qui lui-même utilise ZooKeeper.\nHBase, SolrCloud et Kafka utilisent aussi ZooKeeper.\nMongoDB utilise son outil maison et mongos daemons comme routing tier.\nCassandra et Riak utilisent un gossip protocol pour que les nodes s’échangent leurs changements de topologie. La requête peut alors arriver sur n’importe quel nœud qui la redirigera correctement vers le bon. Ça met plus de complexité sur les nœuds, mais ça évite la dépendance à un outil externe comme ZooKeeper.\nCouchbase ne fait pas de rebalancing automatique. Et il est couplé en général avec moxi, qui est un routing tier écoutant les changements venant des nœuds.\n\n\n\n\nEnfin concernant l’accès au routing tier par le client, son adresse ip en changeant que rarement, une configuration de nom via DNS est suffisante pour y accéder.","7---transactions#7 - Transactions":"Les transactions sont des unités logiques regroupant plusieurs lectures / écritures. Soit elles réussissent, soit elles échouent et alors le client peut réessayer en toute sécurité. Il s’agit d’abstraire tout un pan d’échecs partiels qu’il faut gérer sinon à la main.\nPresque toutes les BDD relationnelles, et certaines non relationnelles utilisent les transactions pour encapsuler les requêtes. Cependant avec la hype récente du NoSQL, on a un certain nombre de BDD qui arrivent avec l’idée que pour la scalabilité et la high availability, les transactions doivent être abandonnées ou donner des garanties beaucoup plus faibles.\nACID signifie Atomicity, Consistency, Isolation and Durability. Malheureusement il y a de l'ambiguïté sur chacun des termes, surtout sur l’isolation.\nAtomicity aurait pu être appelé abortability, parce qu’il s’agit d’annuler une partie des requêtes d’une même transaction si la partie suivante échoue. Comme ça on peut recommencer la transaction entière sans soucis.\nConsistency est ici entendu comme étant la cohérence des données du point de vue applicatif. Contrairement aux 3 autres termes, la consistency relève bien de la responsabilité du code applicatif. Il s’agit de règles liées au domaine en question, par exemple les débits et les crédits doivent s’annuler.\nIsolation consiste à gérer les transactions concurrentes : chaque transaction doit pouvoir s'exécuter sans être parasitée par d’autres transactions en plein milieu. On parle aussi de serializability, pour dire qu’il faut la même garantie que si les transactions étaient exécutées en série les unes à la suite des autres. La plupart des BDD ne fournissent cependant pas ce niveau de garantie.\nDurability veut dire qu’une fois la transaction commitée, elle ne peut pas disparaître toute seule mais reste dans la BDD. Ca implique par exemple la technique du log write-ahead pour les B-Tree ou LSM-Tree, pour ne pas perdre les données. Cela implique aussi la réplication dans le cas de systèmes distribués.\n\n\nL’atomicité et l’isolation concernent les transactions avec plusieurs écritures (plusieurs objets), mais aussi les “transactions” avec une seule écriture. Si un problème survient en plein milieu de l’écriture, il faut s’assurer que la base de données ne se retrouve pas dans un état inconsistant.\nOn dit parfois qu’on supporte les transaction (et même qu’on est ACID) quand on assure l’intégrité pour une seule écriture, mais c’est une erreur, la transaction désigne principalement le groupe de plusieurs écritures.\nLa garantie pour les écritures sur un seul objet est parfois suffisante, mais dans pas mal de cas il faut une garantie sur plusieurs objets :\nDans les BDD relationnelles (ou de graphe), les clés étrangères (ou les edges) doivent être mises à jour en même temps que l’objet change.\nDans les BDD de document, les données à mettre à jour sont en général dans le même document, donc pas de besoin de multi-object transaction de ce côté. Cependant les BDD de document encouragent aussi la dénormalisation à la place des jointures, et dans ce cas les données doivent être mises à jour conjointement dans plusieurs endroits pour ne pas que la BDD devienne inconsistante.\nQuand on a des index secondaires, alors il faut mettre à jour aussi cet index, et ces index sont des objets différents du point de vue de la BDD, donc on doit bien avoir des transaction multi-objets.\n\n\nConcernant l’annulation des transactions, c’est dans cette philosophie qu’est construite la notion d’ACID : si ça échoue on recommence la transaction.\nCertaines BDD ne sont pas du tout dans cette philosophie : les BDD répliquées en mode leaderless sont plutôt sur du “best effort”. La BDD exécute ce qu’elle peut, et si on est dans un état inconsistant, c’est à l’application de gérer les erreurs.\nCertains ORM comme celui de Rails et Django ne réessayent pas les transactions automatiquement, alors que c’est là le but même de l’ACIDité de celles-ci.\nCertains problèmes peuvent quand même survenir quand une transaction est abandonnée :\nIl se peut qu’elle ait fonctionné mais qu’on ne reçoive pas la réponse.\nSi l’erreur est due à une surcharge de requêtes, réessayer la transaction n’arrangera pas les choses, au contraire.\nIl ne faut pas réessayer si l’erreur est de nature permanente (par exemple une violation de contraintes, ie. une transaction qui fait quelque chose d’interdit), mais seulement si l’erreur est de nature temporaire (réseau, crash d’un node, etc.).\nSi la transaction a d’autres side-effects que sur la BDD (par exemple l’envoi d’un email), alors réessayer juste après peut refaire les side-effects. On parlera des Atomic commit et Two-phase commit plus tard.\nSi en réessayant à nouveau on échoue quand même, la requête pourrait être complètement perdue.\n\n\n\n\n\n\n\n\nL’isolation au sens strict de transactions sérialisables est quelque chose de coûteux que les BDD ne veulent souvent pas implémenter. On a donc seulement des weak isolation levels qui ne répondent pas à tous les problèmes posés par les transactions concurrentes. Il faut bien comprendre chaque problème et chaque solution proposée pour choisir ceux qu’on a besoin pour notre application.\nRead commited est le niveau d’isolation le plus basique.\nCa garantit :\nQu’il n’y aura pas de dirty reads : si au cours d’une transaction non terminée une écriture a été faite, une autre transaction au cours de la lecture ne doit pas pouvoir lire ce qui a été écrit.\nQu’il n’y aura pas de dirty writes : si au cours d’une transaction non terminée une écriture a été faite mais pas encore commitée, et au cours d’une autre transaction l’écriture est écrasée, alors il on peut se retrouver avec des données inconsistantes.\n\n\nRead commited est l’isolation par défaut dans de nombreuses bases de données, parmi elles : Oracle 11g, PostgreSQL, SQL Server 2012, MemSQL.\nCôté implémentation :\nPour les dirty reads, l’objet tout entier est bloqué avec un lock par la transaction, jusqu’à ce qu’elle soit commitée ou abandonnée.\nPour les dirty rights, on pourrait aussi mettre un lock, mais c’est perdre beaucoup en efficacité parce que certaines requêtes lentes vont empêcher de simples lectures. Alors la plupart du temps 2 valeurs sont conservées : l’ancienne valeur de l’objet qu’on donne aux nouveaux lecteurs, et la nouvelle valeur qui sera la valeur finale quand la transaction en cours sera terminée.\n\n\n\n\nSnapshot isolation and repeatable read. Le read committed garantit que sur une même donnée il n’y aura pas des lectures / écritures de transactions différentes, mais ça ne garantit pas que différents objets de la base de données resteront cohérents entre eux au cours d’une même transaction.\nProblèmes :\nOn peut par exemple lire une donnée, puis le temps qu’on lise la suivante celle-ci a été modifiée, et la combinaison des deux lectures donne quelque chose d’incohérent. En général il suffit de refaire la 1ère lecture et on a quelque chose de cohérent à nouveau.\nPlus grave : une copie de BDD peut prendre plusieurs heures, et le temps de la copie des changements peuvent être faits, de manière à ce qu’au final on ait copié au fur et à mesure quelque chose d’incohérent. Même chose avec une requête d’analyse énorme qui met beaucoup de temps à lire un grand nombre de données : si elles sont modifiées en cours de route.\n\n\nLa snapshot isolation est supportée par PostgreSQL, MySQL avec InnoDB, Oracle, SQL Server et d’autres.\nCôté implémentation :\nEn général pour les writes on a un write lock qui bloque les autres writes sur un même objet.\nEn revanche les reads n’utilisent pas de locks, et le principe c’est que les writes ne bloquent pas les reads et les reads ne bloquent pas les writes.\nChaque transaction va avoir son snapshot de données en fonction des données sur lesquelles il opère, et ces données ne seront pas changées de toute la transaction. On appelle ça le multi-version concurrency control (MVVC).\n\n\nLa snapshot isolation est appelée de différentes manières en fonction des BDD :\nDans Oracle elle est appelée serializable.\nDans MySQL et PostgreSQL c’est appelé repeatable read.\nCe terme repeatable read vient du standard SQL qui ne contient pas la notion de snapshot isolation, vu qu’elle n’existait pas à l’époque de System R (sur lequel est basée la norme SQL).\nEt pour compliquer le tout, IBM DB2 utilise le terme de repeatable read pour désigner la serializability, ce qui fait qu’il n’a plus vraiment de sens.\n\n\n\n\n\n\nPreventing lost updates. Jusqu’ici on s’est intéressé aux problèmes de lecture dans un contexte d’écritures dans d’autres transactions. Mais il y a également des problèmes survenant lors d’écritures concurrentes entre-elles. Les dirty writes en sont un exemple, et les lost updates un autre.\nSi deux transactions modifient une même valeur de manière concurrente, la dernière transaction écrasera la valeur écrite dans la première. On dit aussi qu’elle va la clobber.\nExemples : un compteur incrémenté deux fois mais qui se retrouve finalement incrémenté de 1, ou encore deux utilisateurs modifiant la même page wiki en envoyant la page entière, le dernier écrasant les modifications de l’autre.\nCe problème courant a de nombreuses solutions :\nAtomic write operations : vu que le problème des lost updates vient du fait qu’on lit d’abord la valeur avant de la mettre à jour, certaines BDD donnent la possibilité de faire une lecture suivie d’un update avec une atomicité garantie.\nMongoDB fournit aussi la possibilité de faire des modifications locales à un document JSON de manière atomique.\nRedis permet de modifier par exemple des priority queues de manière atomique.\nEn général les BDD le font en donnant un lock sur l’objet concerné par l’écriture.\n\n\nExplicit locking : on peut, en pleine requête SQL, indiquer qu’on prend un lock manuellement sur le résultat d’une partie de la requête, pour le réutiliser dans une écriture juste après.\nOn peut facilement oublier de le faire ou mal prendre en compte la logique applicative.\n\n\nAutomatically detect lost updates : de nombreuses BDD permettent de vérifier la présence de lost updates, et en cas de détection d’annuler la requête et de la retenter juste après.\nL’avantage aussi c’est qu’on peut le faire avec la même fonctionnalité que le snapshot isolation. PostgreSQL, Oracle et SQL Server le font de cette manière. MySQL / InnoDB en revanche ne supportent pas cette fonctionnalité.\n\n\nCompare-and-set : certaines bases de données qui ne fournissent pas de transactions permettent des opérations compare-and-set qui consistent à exécuter un changement seulement si la donnée n’a pas été modifiée depuis la dernière fois qu’on l’a lue, ce qui permet normalement d’éviter les lost updates.\nDans le cas des BDD avec réplication : quand on a de la réplication les locks ne servent à rien, et le compare-and-set non plus. La meilleure solution est d’exécuter les deux requêtes et de garder une copie des deux résultats, puis de faire appel à du code applicatif ou d’utiliser des structures spéciales de fusion pour résoudre le conflit.\nRiak 2.0 fournit des structures qui permettent d’éviter les lost updates à travers les réplicas.\nMalheureusement la plupart des BDD ont par défaut une stratégie last write wins (LWW) qui est provoque des lost updates.\n\n\n\n\n\n\nWrite skews and phantoms : on généralise ici le cas des dirty writes et des lost updates dans la mesure où on va écrire sur des objets différents. Chaque requête concurrente lit les données, puis écrit dans un objet différent, mais comme ils le font indépendamment, le code applicatif ne se rend pas compte qu’ils cassent une contrainte applicative qui devait être garantie par le code applicatif. On appelle ça des write skew.\nExemple : il faut au moins un docteur on-call, il en reste deux et les deux décident de cliquer sur le bouton pour se désister. Les deux transactions se font en parallèle et modifient des objets différents liés au profil de chaque docteur.\nLes solutions sont moins nombreuses :\nLes BDD ne fournissent pas de moyen de mettre des contraintes sur des objets différents. On peut en revanche utiliser du code custom avec les triggers ou les materialized views si c’est supporté.\nOn peut locker les objets concernés par notre logique métier à la main au moment de faire la requête.\nCette solution marche si on a déjà les objets dont on veut que la valeur ne change pas. Mais si dans notre cas la condition c’est qu’une entrée avec une certaine caractéristique n’existe pas pour pouvoir faire quelque chose (par ex insérer un nom d’utilisateur s’il n’est pas déjà pris), alors on ne peut pas locker à la main une absence d’objet.\nDans ce cas où le write skew est causé par une écriture dans une transaction, qui change le résultat d’une recherche dans une autre transaction, le phénomène est appelé un phantom.\nUne solution (peu élégante) peut consister à matérialiser les phantoms en créant une table spéciale avec un champ pour chaque élément possible, et demander au code applicatif de faire un lock manuel sur l’élément matérialisé correspondant à chaque write. Dans la plupart des cas, il vaut cependant mieux privilégier la serializability.\n\n\n\n\n\n\nMalheureusement la snapshot isolation ne suffit pas, il faut une vraie serializability dont on va parler un peu plus loin.\n\n\n\n\n\n\nSerializability : il y a un niveau au-dessus de tous les autres, qui permet de garantir que les transactions vont s’exécuter avec le même niveau de garantie vis-à-vis des race conditions que s’ils étaient exécutés les uns à la suite des autres, sans parallélisme du tout. Il y a 3 techniques pour l’implémenter dans un contexte non distribué :\nActual serial execution : on va exécuter les transactions vraiment les uns à la suite des autres, sur un seul thread.\nCette option est envisagée maintenant alors qu’elle était rejetée auparavant parce que la RAM est peu chère et on peut mettre l’essentiel de la BDD dedans, ce qui permet de rendre les transactions très rapides. Et aussi parce que les transactions OLTP sont courtes et impliquent peu de requêtes, alors que les OLAP sont certes longues mais sont read-only donc peuvent se faire hors de l’execution loop.\nCette approche est utilisée dans VoltDB / H-Store, Redis et Datomic.\nPour que ce soit possible sur un seul thread, il faut qu’il ne soit pas bloqué pendant qu’on demande à l’utilisateur la suite en plein milieu de la transaction. Il faut donc collecter les données qu’il faut pour toute la transaction, et faire la transaction entière en une fois. Pour ce faire, on utilise les stored procedures.\nCes procédures permettent d’exécuter du code écrit dans un langage spécifique : pour Oracle PL/SQL, pour SQL Server T-SQL, pour PostgreSQL PL/pgSQL, mais ces langages sont vieux, peu testables, et n’ont pas beaucoup de fonctionnalités.\nDes BDD modernes permettent cependant d’utiliser des langages modernes pour les stored procedures : VoltDB utilise Java et Groovy, Datomic utilise Java et Clojure, Redis utilise Lua.\n\n\nPour la réplication, VoltDB permet d’exécuter les stored procedures sur chaque machine. Il faut alors que ces procédures soient déterministes.\nDans le cas où on veut scaler en écriture on a besoin de partitionnement. On peut alors créer autant de partitions que de coeurs de processeur sur la machine, et assigner un thread par partition. Chaque partition exécutera bien les transactions de manière séquentielle.\nAttention par contre aux requêtes qui ont besoin d’effectuer des opérations à travers plusieurs partitions (à peu près tout sauf les données key/value), ça provoque des ralentissement de plusieurs ordres de grandeur.\n\n\nDonc les contraintes pour utiliser l’exécution en série :\nChaque transaction doit être petite et rapide.\nLa BDD doit entrer en RAM. Une partie peu utilisée de la BDD peut rester sur disque, mais si on doit aller la chercher dans le thread unique c’est chaud au niveau perf. Une solution pourrait être d’abandonner la transaction, mettre la donnée dont on a besoin en RAM, et la retenter.\nLa charge en écriture doit être assez faible pour être traitée par une machine, ou alors il faut un partitionnement sans requêtes qui s’exécutent sur plusieurs partitions.\n\n\n\n\nTwo-Phase Locking (2PL) : c’est l’algorithme qui a été utilisé pendant 30 ans. Il s’agit de mettre un lock sur la donnée dès lors qu’on est en présence d’une transaction qui fait un write, même vis-à-vis de transactions qui ne font que des reads. En revanche s’il n’y a que des transactions qui font des reads, pas besoin de lock.\nComparé au snapshot isolation où les writes ne bloquaient pas les reads, et les reads ne bloquaient pas les writes, ici les writes bloquent aussi les reads.\n2PL est utilisé dans MySQL (InnoDB), SQL Server et DB2.\nFonctionnement : il y a les shared locks et les exclusive locks. A chaque fois qu’un read est fait sur un objet, la transaction prend un shared lock, qui permet de la faire attendre au cas où l’exclusive lock serait pris. Si une transaction veut faire un write, alors elle prend l’exclusive lock dès qu’elle peut, et tout le monde doit attendre pour accéder à cet objet que sa transaction entière soit terminée (d’où le 2-phase : on prend le lock, puis on termine le reste de la transaction de manière exclusive).\nPour être vraiment comme des transactions sérialisées, il faut aussi résoudre le problème des phantoms (un write qui modifie le résultat d’une recherche). On le fait en créant des locks sur des prédicats : si une transaction a besoin de faire une query pour chercher quelque chose, alors elle déclare un shared lock sur un prédicat, et si un write modifie le résultat correspondant à ce prédicat, alors ils se bloqueront mutuellement.\nLe lock sur des prédicats étant très mauvais d’un point de vue performance, on approxime souvent les prédicats sous forme de lock d’index, en s’assurant qu’on lock éventuellement plus d’objets, et pas moins pour respecter la sérialisabilité.\n\n\n\n\nLe souci de cette méthode c’est la performance, en partie du fait de nombreux locks, mais surtout du fait que n’importe quelle transaction peut faire attendre toutes les autres. Donc on a un flow assez imprédictible, et des high percentiles mauvais.\nLes deadlocks sont détectés et résolus en annulant l’une des transactions, mais s’ils sont nombreux, ça fait d’autant moins de performance.\n\n\n\n\nSerializable Snapshot Isolation (SSI) : il s’agit d’un algorithme très prometteur qui fournit la sérialisabilité, et en même temps n’a que très peu de différence de performance avec la snapshot isolation.\nLa SSI est à la fois utilisée par les BDD single node (PostgreSQL depuis la version 9.1) et distribuées (FoundationDB).\nFonctionnement : contrairement à l’idée de faire des locks pour protéger la transaction d’un conflit éventuel, qui est une approche dite pessimiste, ici on adopte une approche optimiste et on réalise toutes les transactions dans un snapshot à part. Au moment du commit on vérifie qu’il n’y a pas eu de conflits. S’ils ont eu lieu, on annule la transaction et on laisse l’application recommencer.\nIl y a une difficulté vis-à-vis du fait de détecter si une transaction avec lecture initiale suivie d’une écriture devient invalide parce que la donnée lue est modifiée par une autre transaction. Il y a 2 solutions pour régler ça :\nDétecter les lectures faites sur le MVCC (multi version concurrency control) qui ne sont plus à jour au moment où la transaction veut être commitée. Si on détecte, on annule la transaction.\nDétecter les writes qui affectent les reads d’une autre transaction en plaçant une balise sur l’index concerné pour indiquer que plusieurs transactions utilisent la donnée. Au moment de commiter, la BDD vérifie qu’il n’y a pas de conflit par rapport au write fait par la transaction qui avait été marquée. Si oui on annule la dernière qui veut commiter. Le marquage peut être enlevé quand la situation de concurrence est résolue.\n\n\n\n\nAu niveau de la performance, plus la BDD est précise sur quelle transaction doit être annulée, et plus ça lui prend du temps. D’un autre côté si elle en annule trop ça fait plus de transactions annulées.\nComparé au 2PL on a quelque chose de plus performant mais aussi de plus prédictible, vu que les requêtes n'ont pas à attendre qu’une longue requête ait terminé. Et si on a une forte charge de lectures c’est parfait aussi puisqu’elles ne sont jamais bloquées.\nComparé à l’exécution vraiment en série, on n’est pas limité au CPU d’une seule machine, FoundationDB distribue la détection des conflits sur plusieurs machines.\nGlobalement, vu qu’une transaction peut vite voir ses prémisses invalidées par d’autres, pour qu’on n’ait pas beaucoup d’annulation de transactions, il faut que celles-ci soient assez courtes et rapides. Mais d’un autre côté, 2PL et l’exécution sériale ne font pas mieux avec les transactions longues.","8---the-trouble-with-distributed-systems#8 - The trouble with distributed systems":"Les fautes partielles :\nLe souci avec les systèmes distribués c’est qu’ils peuvent agir de manière non déterministe, et qu’une partie du système peut être en échec alors que le reste fonctionne. C’est une chose dont on n’a pas l’habitude dans un seul ordinateur.\nLes superordinateurs choisissent en général d’écrire des checkpoints en DD, et d’arrêter tout le système pour réparer le composant problématique en cas de panne, pour ensuite reprendre là où ça en était à partir du checkpoint.\nLes systèmes distribués de type “cloud” ou “web” sont à l’opposé :\nils sont trop gros pour tolérer d’éteindre à chaque panne, et ils ne peuvent de toute façon pas tolérer d’arrêter le service\nils utilisent du matériel bon marché pour scaler\nils sont répartis à travers le globe, utilisant le réseau internet qui est très peu fiable comparé à un réseau local.\n\n\n\n\nIl faut que la gestion des problèmes matériels fasse partie du design de notre système.\n\n\nLe réseau :\nLe réseau internet (IP) est construit de manière à être peu fiable de par sa nature asynchrone. Un paquet peut à tout moment être perdu, corrompu, mettre beaucoup plus de temps à arriver etc. pour diverses raisons, parce qu’il passe par des dizaines de nœuds divers et variés qui peuvent être surchargés, débranchés, mal configurés etc.\nOn a des protocoles comme TCP construits par dessus pour corriger ça et renvoyer les paquets perdus ou corrompus.\nQuand on envoie un paquet, on ne sait pas s’il a été reçu ou pas. Au mieux on peut demander au destinataire de répondre, mais s’il ne répond pas on ne sait pas ce qu’il s’est passé. Tout ce qu’on peut faire c’est avoir un timeout, et considérer l’échec après le timeout.\n\n\nC’est le cas d’internet qui est peu fiable, mais le réseau ethernet local est également asynchrone. Donc les messages échangés entre les ordinateurs d’un même datacenter sont aussi prompts aux corruptions et pertes.\nUne étude a trouvé qu’il y a 12 fautes réseau par mois dans un datacenter moyen.\nAjouter de la redondance ne règle pas autant de problèmes qu’on le croit puisqu’il y a aussi les erreurs humaines des ops qui sont nombreuses\n\n\nLa détection des machines en état de faute est difficile, mais il y a des moyens :\nSi le processus applicatif est mort mais que l’OS tourne, la machine répondra peut-être par un message TCP indiquant qu’elle refuse les connexions.\nDans le même cas, la machine peut aussi avertir les autres nœuds que son processus applicatif est mort. HBase fait ça.\nDans le cas spécifique d’un datacenter, on peut avoir accès aux switches réseaux pour avoir certaines informations sur l’état connu de certaines machines qui ne répondent plus depuis un certain temps.\nDe même avec les routeurs qui peuvent immédiatement répondre que telle ou telle machine est injoignable si on les interroge.\n\n\nLa question de la valeur du timeout est une question particulièrement épineuse et pas simple. Une des manières est de tester en environnement réel et d’ajuster en fonction des performances.\nCet ajustement peut être automatique, Akka et Cassandra font ça.\n\n\nLa congestion du réseau est souvent causée par des problèmes de queuing diverses :\nau niveau des switchs\nau niveau des machines si tous les CPU sont occupés\nTCP qui fait du queuing pour éviter la corruption de paquets, et qui retente l’envoie du paquet de manière transparente (ce qui prend du temps)\n\n\nNe pourrait-on pas rendre la communication fiable du point de vue matériel ?\nPour ce faire, il faudrait qu’elle soit synchrone. C’est le cas du réseau téléphonique à commutation de circuit, qui alloue une ligne permettant d’envoyer une quantité fixe de données de manière régulière. Les divers switch et autres éléments réseaux qui établissent cette communication allouent cette quantité pour que le transfert puisse se faire.\nDans le cas des communications autres que stream audio / vidéo, on ne sait pas à l’avance quelle quantité de données on voudra, ni quand on voudra faire le transfert. La commutation par paquets permet de ne rien envoyer quand il n’y a pas besoin, et d’envoyer des paquets de taille variable quand c’est nécessaire. Le prix c’est que le réseau n’est pas en train de nous allouer de la place en permanence, et qu’il y a du queuing.\nC’est donc bien un choix d’allocation dynamique et non pas de réservation statique des ressources réseaux qui fait qu’on utilise toutes nos ressources disponibles mais avec des délais variables. On fait ce genre de choix aussi pour l’allocation dynamique des CPU vis à vis des threads.\n\n\n\n\nLes clocks : les clocks des ordinateurs sont globalement peu fiables, et d’autant moins dans un contexte d’ordinateurs distribués.\nIl y a 2 types de clocks sur un ordinateur :\nLes time-of-the-day clocks : ils renvoient le temps courant, en général sous forme d’entier depuis l’epoch (1er janvier 1970).\nVu qu’ils sont synchronisés par NTP (network time protocol), on peut régulièrement avoir des sauts dans le temps, et donc pour mesurer des durées c’est pas le top.\n\n\nLes monotonic clocks : ils renvoient une valeur arbitraire, mais garantissent qu’après un certain temps, la valeur renvoyée sera l’ancienne + le temps écoulé\n\n\nA propos de la précision :\nGoogle suppose que les clocks de ses machines se décalent de l’équivalent de 17 secondes pour un clock resynchronisé une fois par jour.\nLe protocole de mise à jour des clocks NTP ne peut pas être plus précis que le temps de latence d’envoi/réception des messages (une expérimentation a montré un minimum de 35 ms pour une synchronisation via internet).. Et en cas de congestion du réseau c’est pire.\nDans les machines virtuelles le CPU est partagé, donc on peut se retrouver avec des sauts bizarres dans le clock à cause de ça.\nEn cas de besoin, on peut mettre en place des infrastructures de haute précision qui se mettent à jour par GPS, mais c’est coûteux. C’est ce qui est fait sur les machines de trading à haute fréquence.\nEn fait, il faudrait voir le clock plutôt comme un intervalle que comme un temps. Malheureusement la plupart des API ne le présentent pas comme ça.\nUne exception est constituée par l’API TrueTime de Google Spanner, qui renvoie un groupe de 2 valeurs : [earliest, latest].\nDans le cas particulier de Google, en partant du principe que les intervalles de confiance sont fiables, si deux intervalles pour deux requêtes ne se chevauchent pas, alors on est sûrs que la requête avec l’intervalle plus récent a eu lieu après l’autre. Google utilise ça pour faire de la snapshot isolation dans un environnement distribué, mais pour ça il équipe chaque datacenter d’une réception GPS ou d’une horloge atomique, sans quoi les intervalles seraient trop grands. En dehors de Google cette solution basée sur le temps n’est pour le moment pas viable.\n\n\n\n\n\n\nContrairement à un CPU ou une carte réseau, quand un clock est défectueux la machine peut quand même donner l’impression que tout va bien, et faire des erreurs qui se voient beaucoup plus difficilement.\nC’est en particulier problématique si on se sert des clocks pour faire des timestamps pour vérifier quelle transaction a eu lieu la 1ère dans un système distribué. Et c’est encore plus problématique avec du LWW (last write wins) : si on noeud a son clock qui retarde, tous ses messages finiront par être rejetés en faveur de ceux des autres nœuds parce que considérés comme anciens.\nPlutôt que les clocks physiques, il faut utiliser des clocks logiques, c'est-à-dire des techniques pour détecter l’ordre des choses plutôt que le moment où elles ont eu lieu.\n\n\nUn thread peut se mettre en pause pendant un temps indéterminé pour des raisons très variées : le garbage collector du langage, la machine virtuelle, l’OS qui a besoin de le mettre en pause pour faire autre chose etc. Dans un système distribué “shared nothing” il n’y a pas de mémoire partagée, donc il faut partir du principe qu’un nœud peut se retrouver arrêté pendant que le monde autour de lui aura continué.\nIl existe des systèmes appelés temps réel (real time, ou hard real time pour bien insister sur l’aspect contrainte de temps à respecter absolument). Ces systèmes sont pensés et testés sous tous les angles pour respecter un certain nombre de contraintes de temps de réponse. On les utilise principalement dans les machines où le temps est crucial (par exemple le déclenchement d’un airbag).\nPour le problème spécifique du garbage collector, certains systèmes demandent à leur nœud de prévenir quand il y a un besoin de garbage collection, et au besoin redirigent le trafic vers d’autres nœuds en attendant que ce soit fait. Ça permet de réduire pas mal les problèmes de pause non voulue de l’application.\n\n\n\n\nSavoir, vérité et mensonge :\nDans des conditions aussi difficiles que les systèmes distribués où on ne peut rien savoir de certain sauf à travers les messages qu’on reçoit ou ne reçoit pas, on peut quand même créer des systèmes qui fonctionnent : il est possible d’avoir quelque chose de fiable construit sur des bases offrant peu de garanties, à conditions que le modèle de système qu’on a choisi convienne.\nLa vérité dans un contexte distribué est déterminée par la majorité. Pour éviter la dépendance à un noeud particulier, et étant donné qu’un noeud, quel qu’il soit, ne peut pas faire confiance à sa propre horloge vu qu’il peut entrer en pause à tout moment sans le savoir, on décide de mettre en place des quorums pour qu’une majorité de noeuds décident par exemple si un noeud est mort ou non.\nIl faut bien s’assurer que and on noeud pense être doté d’une responsabilité (il est le leader, il a le lock sur un objet etc.), il se fie quand même à ce que disent la majorité des noeuds : s' ils lui disent qu’il n’a plus la responsabilité en question, alors il faut qu’il accepte de se comporter comme tel, sous peine d’inconsistances dans le système.\nPour garantir qu’un lock soit bien respecté, on peut utiliser un lock service qui fournit un token incrémental à chaque lock. Si le nœud a son temps alloué qui a expiré, et qu’il essaye d’écrire alors qu’un autre a déjà écrit à sa place, son token sera rejeté par le lock service. On parle de fencing token.\nZooKeeper permet de fournir ce genre de fencing token s’il est utilisé comme lock service.\n\n\n\n\nJusqu'ici on est parti du principe que les noeuds peuvent de plus répondre, ne pas savoir qu’ils n’ont plus une certaine responsabilité, ou échouer. Mais qu’ils restent “honnêtes” au sens où ils ne vont pas dire qu’ils ont reçu un message alors qu’ils ne l’ont pas reçu, ou encore falsifier un fencing token. De tels cas de corruption s’appellent une Byzantine fault.\nÇa vient du Byzantine Generals Problem où on imagine dans la ville antique de Byzance, des généraux de guerre essayent de se mettre d’accord, et communiquant par messager, mais où certains généraux mentent sans se faire découvrir.\nOn imagine donc que certains nœuds peuvent être complètement corrompus jusqu’à ne plus suivre le protocole attendu du tout, par exemple dans le cas d’un logiciel dans un contexte aérospatial soumis à des radiations.\nOu alors se mettent carrément à tricher intentionnellement, soit à cause d’un piratage, soit plus classiquement un contexte de communication inter-organisations, où les organisations ne se font pas confiance.\nC’est le cas par exemple pour la blockchain où les participants ne se font pas confiance puisque n’importe lequel pourrait essayer de tricher.\n\n\nDans notre cas habituel de serveurs web, on part du principe que le client final derrière son navigateur pourrait être malicieux, mais sinon les serveurs de l'organisation sont fiables. Et on ne met pas en place de mécanismes contre les problèmes de fautes byzantines, parce que c’est trop compliqué.\nLa plupart des algorithmes contre les fautes byzantines comptent sur le fait que la majorité des nœuds ne vont pas être infectés par le problème, et donc pourront garder le contrôle contre la minorité du réseau corrompue. Donc ça peut être utile dans un contexte d’application peer-to-peer, mais si on charge notre version du logiciel dans tous les nœuds, ça ne nous protégera pas des bugs. Et de même si un hacker prend le contrôle d’un nœud, on imagine qu’il pourra aussi prendre le contrôle des autres nœuds.\nOn peut néanmoins se prémunir contre des formes modérées de mensonges avec quelques astuces :\nFaire un checksum des paquets pour vérifier qu’ils n’aient pas été corrompus. TCP/UDP le font mais parfois laissent passer.\nVérifier la validité de toutes les données entrées par l’utilisateur.\nDans le cas de la mise à jour depuis des serveurs NTP, faire des requêtes auprès de plusieurs serveurs, pour que ceux qui n’ont pas une bonne valeur soient rejetés.\n\n\n\n\nNotre système doit prendre en compte les problèmes matériels qu’on a décrits, mais il ne doit pas non plus être complètement dépendant du matériel exact sur lequel il tourne pour pouvoir changer le matériel. On va donc créer une abstraction qui est le system model.\nConcernant les considérations liées au temps, on en a 3 :\nSynchronous model : on part du principe que les erreurs réseau, de clock ou les pauses de processus sont limités à certaines valeurs définies; En pratique la réalité ne colle pas à de modèle.\nPartially synchronous model : on part du principe que le système se comporte de manière synchrone la plupart du temps, sauf parfois où il déborde. Ce modèle correspond beaucoup mieux à nos systèmes web distribués.\nAsynchronous model : on est beaucoup plus restrictif puisqu’on considère qu’aucune notion de temps ne peut être fiable. Et donc on ne peut pas utiliser de timeouts non plus.\n\n\nConcernant les considérations liées aux échecs de noeuds, il y en a aussi 3 :\nCrash-stop faults : on considère que si un nœud fait une faute, on l’arrête et c’en est fini de lui.\nCrash-recovery faults : les nœuds peuvent être en faute, puis revenir en état correct un peu plus tard. C’est ce modèle qui nous est en général le plus utile pour nos systèmes web.\nByzantine (arbitrary) faults : les nœuds peuvent faire absolument n’importe quoi.\n\n\nPour définir qu’un algorithme d’un system model distribué est correct, il peut avoir deux types de propriétés :\nsafety : il s’agit d’une propriété qui dit que rien de mal ne doit se passer. Par exemple, la uniqueness de quelque chose. Si une telle propriété est rompue, c’est parce que chose a été violée et qu’il y a eu un dommage non réparable.\nliveness : il s’agit d’une propriété qui dit qu’une chose attendue doit arriver. Par exemple l’availability, le fait de recevoir une réponse. Si une telle propriété est rompue, c’est que ce qui était attendu n’a pas eu lieu, mais pourrait avoir lieu plus tard\n\n\nIl est courant de demander à ce que les caractéristiques de safety soient respectées dans tous les cas, même si tous les nœuds crashent, un mauvais résultat ne doit pas être retourné. Pour les caractéristiques de liveness, on peut demander à ce qu’elles soient respectées seulement dans certains cas, par exemple si un nombre suffisant de nœuds est encore en vie.\nEnfin, il faut bien garder en tête qu’un system model n’est qu’un modèle. Dans la réalité, on sera amené à rencontrer des erreurs non prévues. Et à l’inverse, sans raisonnement théorique, on pourrait avoir des erreurs dans nos systèmes pendant longtemps sans s’en rendre compte. Les deux sont aussi importants l’un que l’autre.\nC’est la différence entre le computer science (théorique), et le software engineering (pratique).","9---consistency-and-consensus#9 - Consistency and Consensus":"Pour rendre un système tolérant aux fautes, il faut introduire des abstractions. C'est ce qu’on a fait avec les transactions par exemple en partant du principe qu’une transaction est atomique. Une autre abstraction intéressante est le consensus : faire en sorte que les nœuds se mettent d’accord.\nLa consistency est une question importante à laquelle on peut apporter différents niveaux de garantie. Comme avec l’isolation où il s’agissait de traiter la concurrence entre deux transactions, avec la consistance il s’agit de coordonner l’état des réplicas vis-à-vis des délais (replication lag) et des fautes.\nLa linearizability consiste en une abstraction qui donne l’illusion que le replication lag n’existe pas, qu’il n’y a en fait qu’une seule copie des données : dès qu'une copie a été faite, le système doit se comporter comme si cette donnée la plus récente était lisible depuis partout.\nUne des conséquences c’est qu’il faut que quand une lecture a été faite avec une valeur, ce soit cette valeur qui soit retournée par tous les réplicas à partir de ce moment. Si une écriture a lieu entre temps ça peut être cette nouvelle valeur écrite, mais certainement pas une valeur plus ancienne.\nOn doit pouvoir éviter le cas où une personne recharge la page et voit que le match a été gagné par telle équipe, et juste après une autre personne affiche la page, et voit que le match est toujours en cours.\nEn revanche, il n’y a pas de contraintes de délais : si l’écriture prend du temps c’est pas grave. Et si deux transactions sont concurrentes et que l’une arrive avant l’autre c’est pas grave non plus.\n\n\nLinearizability vs serializability : la serializability est une notion d’isolation pour pouvoir garantir la manipulation de plusieurs objets au sein d’une même transaction, sans être gêné par les autres transactions. La linearizability consiste à renvoyer systématiquement le résultat le plus récent à chaque lecture une fois que celui-ci a été lu au moins une fois.\nLe 2-phase locking et l’actual serialization garantissent aussi la linearizability. En revanche la Serializable snapshot isolation ne la garantit pas puisqu’elle va créer des snapshots pour les transactions, et ne pas inclure les writes récents dans ces snapshots (ce qui peut facilement résulter à ce que certaines transactions aient un write et d’autres pas).\n\n\nParmi les applications de la linearizability :\nL’élection d’un nouveau leader est un problème où dès que le lock a été pris, il faut que personne d’autre ne puisse le prendre.\nApache ZooKeeper et etcd sont souvent utilisés comme système de lock pour implémenter l’élection de leader.\nApache Curator ajoute des choses par-dessus ZooKeeper.\n\n\n\n\nLe fait de garantir qu’un nom d’utilisateur ne sera pas pris deux fois, ou encore qu’un compte en banque ne va pas en dessous de 0.\nDans le cas où on a 2 canaux de communication, l’un des canaux peut être plus rapide que l’autre : par exemple si on écrit une image, et qu’on enqueue un message pour qu’une version thumbnail de l’image soit générée. Si le traitement du message dans la queue est plus rapide que le temps qu’on met à écrire l’image entière, la thumbnail risque d’être faite à partir d’un fichier partiel. Il faut donc s’assurer de l’ordre de ce qui est fait dans ces 2 canaux.\n\n\nLa linearizability parmi les systèmes de réplication connus :\nSingle-leader replication : ça pourrait être linearizable si la BDD n’utilise pas de snapshot isolation. Mais il reste le problème de savoir qui est le leader, et dans le cas de réplication asynchrone on peut perdre des données au failover.\nConsensus algorithms : ces algorithmes permettent d’implémenter la linearizability en répondant aux problèmes soulevés dans la single-based replication. On va y revenir.\nC’est comme ça que fonctionnent ZooKeeper et etcd.\n\n\nMulti-leader replication : ces systèmes ne sont pas linearizable puisqu’il y a des écritures concurrentes qui sont résolues après coup.\nLeaderless replication : certains affirment qu’en respectant la règle du quorum consistency, on peut obtenir une linearizability sur des BDD Dynamo-style. Mais ce n’est en général pas vrai.\nRiak ne fait pas de read repair à cause du manque de performance de cette technique, et il la faudrait pour la linearizability.\nCassandra fait le read repair, mais il perd la linearizability à cause de son algo last write wins qui cause des pertes de données.\n\n\n\n\nSi on part de l’exemple de la multi-leader replication, on constate que c’est pratique parce que si la connexion est rompue entre deux datacenters, les deux peuvent continuer indépendamment, et se resynchroniser dès que la connexion est rétablie. On a alors une grande availability du système, mais on ne respectera pas la linearizability. A l’inverse si on reste en single-leader, le datacenter déconnecté du datacenter leader se verra inopérant jusqu’à rétablissement du réseau. Mais on garde la linearizability.\nLe CAP theorem décrit cette problématique et a permis en son temps d’ouvrir la discussion, mais il est fondamentalement inutile de nos jours.\n\n\nDans la pratique, de nombreuses BDD n’implémentent pas la linearizability parce que ça coûte trop cher en performance. Il n’y a malheureusement pas d’algorithme qui permette d’avoir de la linearizability sans ce problème de performance qui est d’autant plus grand qu’il y a beaucoup de délais dans le réseau.\n\n\nGaranties d’ordre d’exécution :\nLa causal consistency (causalité) est au cœur des problématiques des systèmes distribués faisant fonctionner des applications qui ont du sens.\nRespecter la causalité n’implique pas forcément un total order (ordonnancement total) de tous les éléments, mais uniquement de ceux liés entre eux par une relation cause / conséquence.\nLa linearizability quant à elle, implique un total order. Elle est donc une contrainte plus forte que la causal consistency.\nC’est trop récent à l’époque du livre pour être dans des systèmes en production, mais il y a de la recherche sur des techniques permettant de détecter la causalité sans total order. Par exemple une généralisation des version vectors.\n\n\n\n\nLa causal consistency coûte quand même cher s’il faut traquer toutes les transactions et leurs relations. On peut sinon utiliser des sequence numbers pour créer un clock logique permettant de définir un ordre total.\nSur une configuration single-leader, il suffit d’incrémenter un compteur à chaque opération au niveau du leader.\nDans le cas où il y a plusieurs leaders, on a d’autres solutions:\nGénérer des sequence numbers différents pour chaque nœud (par exemple pair pour l’un, impairs pour l’autre).\nUtiliser un clock physique.\nAllouer des plages à chaque nœud, par exemple 0 à 1000 pour l’un, 1000 à 2000 pour l’autre etc.\nMalheureusement certains nœuds peuvent aller plus vite que d’autres, et ces techniques ne garantissent pas la causalité dans le système.\n\n\nLa causalité peut être assurée dans un environnement multi-leader grâce aux Lamport timestamps. Il s’agit d’une idée de Leslie Lamport dans un des papiers les plus cités des systèmes distribués.\nLe principe est d’avoir un compteur normal par nœud, et pour le rendre unique on l’associe à un chiffre représentant le nœud lui-même. Et l’astuce de la technique consiste à ce que chaque nœud et chaque client garde en mémoire la valeur la plus élevée de compteur qu’il connaisse. Et quand il a connaissance de la valeur d’un autre compteur plus élevé que celui qu’il connaissait au détour d’une opération, il met immédiatement à jour le compteur du nœud sur lequel il fera la prochaine opération avec cette valeur-là.\nCette technique permet de respecter la causalité, mais aussi un total ordering.\nMalheureusement ça ne règle pas tous nos problèmes : même avec un total order, on ne peut pas savoir sur le moment si un nom d’utilisateur unique est en passe d’être pris par un autre nœud ou non pour savoir sur le moment s’il faut l’autoriser soi-même ou non. Avec le temps et les opérations, on finira par avoir un ordonnancement total, mais pour le moment non.\nC’est l’objet du total order broadcast.\n\n\n\n\n\n\nLe total order broadcast nécessite qu’aucun message ne soit perdu, et que tous les messages soient délivrés à tous les nœuds dans le même ordre.\nLa connexion peut être interrompue, mais les algorithmes de total order broadcast doivent réessayer et rétablir l’ordre des messages dans tous les nœuds quand le réseau est rétabli.\nZooKeeper et etcd implémentent le total order broadcast.\nA noter aussi que le total order broadcast maintient l’ordre tel qu’il est au moment de l’émission des messages, donc c’est plus fort que le timestamp ordering.\nOn peut voir ça comme un log de messages transmis à tous les nœuds dans le bon ordre.\nOn peut ainsi implémenter la linearizability à partir d’un système respectant le total order broadcast.\nPour l’écriture :\nOn ajoute un message au log disant qu’on voudrait écrire\nOn lit le log et on attend que notre message nous parvienne\nSi le premier message concernant ce sur quoi on voulait écrire est le nôtre, alors on peut valider l’écriture dans le log.\n\n\nPour la lecture :\nOn peut faire pareil qu’avec l’écriture : ajouter un message indiquant qu’on veut lire, attendre de le recevoir, puis faire la lecture en fonction de l’ordre indiqué dans le log.\nC’est comme ça que ça marche dans les quorum reads, dans etcd.\n\n\nOn peut demander à avoir tous les messages de log liés à une lecture puis faire la lecture à partir de là.\nC’est comme ça que fonctionne la fonction sync() de ZooKeeper.\n\n\nOn peut lire à partir d’un réplica synchrone avec le leader (en cas de single-leader), dont on est sûr qu’il a les données les plus récentes.\n\n\n\n\nA l’inverse, on peut aussi implémenter un système total order broadcast à partir d’un système linéarisable : il suffit d’avoir un compteur linéarisable qu’on attache à chaque message envoyé via total order broadcast.\n\n\nOn peut enfin noter qu’à la fois la linearizability et le total order broadcast sont tous deux équivalents au consensus.\n\n\nLe consensus consiste en la possibilité pour les nœuds de se mettre d’accord sur quelque chose (on pense par exemple à l’élection de leader, ou à l’atomic commit problem où il faut choisir entre garder ou non une transaction présente sur certains nœuds), alors même que des noeuds peuvent être en faute à tout moment.\nC’est un sujet très subtil et complexe.\nLe FLP result est un résultat théorique montrant que le consensus est impossible dans un system model asynchrone. Dans la pratique, à l’aide de timeouts (même s’ils peuvent être parfois faussement positifs), on arrive à atteindre le consensus.\nQuand une transaction est écrite en BDD, il est hors de question de la retirer par la suite parce qu’elle a pu être prise en compte par d’autres transactions. Il faut donc bien réfléchir avant d’entrer définitivement l’écriture en BDD.\nLe two-phase commit (2PC) est un algorithme de consensus implémenté dans certaines BDD.\n2PC n’est pas très bon, des algorithmes plus modernes existent chez ZooKeeper (Zab) et etcd (Raft).\nAttention à ne pas confondre 2PC avec 2PL (2 phase lock) qui permet l’isolation pour la sérialisation, le mieux est d’ignorer le rapprochement de leur nom.\nFonctionnement :\non a besoin d’un nouveau composant : le coordinator.\nLors d’une transaction, après les lectures / écritures, quand on veut inscrire vraiment tout ça en BDD, le coordinator va procéder en 2 étapes :\ndemander successivement à chaque nœud si il est prêt à faire un commit et attendre leur réponse.\nsi oui, faire le commit, sinon annuler la transaction.\n\n\nL’idée c’est que lors de la 1ère phase, quand le coordinator demande si les nœuds sont prêts, en fait il leur demande aussi de tout préparer pour que même en cas de crash rien ne soit perdu de leur côté. La seule chose qui leur resterait à faire alors serait de valider les données déjà mises en forme pour aller dans la BDD.\nLorsque le coordinator prend sa décision finale de faire s’exécuter ou d’annuler la transaction en phase 2, alors il l’écrit localement et passe un point de non-retour. A partir de là il réessayera en permanence de faire finaliser la transaction auprès de tout nœud qui deviendrait indisponible à partir de ce moment-là.\nDans le cas où le coordinator crash juste après avoir demandé aux noeuds de se préparer fait que les noeuds doivent rester en attente. Ils ne peuvent pas unilatéralement prendre de décision de valider ou annuler une transaction chacun de leur côté. La solution est d’attendre que le coordinator revienne, lise ce qu’il avait décidé sur son fichier de log, et envoie les messages qui conviennent.\nIl existe un autre algorithme appelé 3-phase commit (3PC) qui résout le problème de l'aspect bloquant lié à l’attente du commit du coordinator, mais il implique des temps de réseaux bornés. Or nos réseaux habituels sont imprévisibles. Pour cette raison, c'est le 2PC qui continue d’être utilisé.\nLe souci de ce cas c’est surtout le lock au niveau de la BDD, souvient sur les entrées concernées par la transaction. Si le coordinator ne revient jamais ou que les logs sont perdus, alors on peut se retrouver face à des locks orphelins, et un administrateur humain devra manuellement résoudre ces conflits, puisque les locks sont censés survivre même à un redémarrage de la BDD.\n\n\n\n\n\n\nEn pratique, les transactions distribuées sont souvent décriées parce qu’elles coûteraient trop par rapport à ce qu’elles apporteraient.\nLes transactions distribuées utilisant MySQL sont connues pour être 10 fois plus lentes que les mêmes transactions sur un seul nœud.\nIl y a deux types de transactions distribuées : celles qui sont implémentées par une même BDD qui tourne sur plusieurs nœuds, et celles qui consistent à faire communiquer des technologies hétérogènes d’un nœud à un autre. Les dernières sont bien plus compliquées.\nLes transactions hétérogènes peuvent faire communiquer par exemple une BDD et un message broker, et ne commiter que si tout a marché, et annuler tout sinon.\nXA (eXtended Architecture) est justement un protocole qui permet d’implémenter le 2PC dans des technologies hétérogènes. Il s’agit d’une API en C qui se connecte aux programmes qui s’exécutent sur une machine.\nIl est supporté par de nombreuses BDD : PostgreSQL, MySQL, DB2, SQL Server, Oracle.\nEt par plein de message brokers : ActiveMQ, HornetQ, MSMQ, IBM MQ.\nXA a cependant des limitations, par exemple il ne permet pas de détecter les deadlocks, et ne supporte pas le SSI (serializable snapshot isolation).\n\n\n\n\nIl faut noter quand même que le coordinateur est souvent un single point of failure vu qu’il contient lui-même des données persistantes cruciales pour le fonctionnement du système. Mais étonnamment les possibilités de le rendre réplicable sont en général rudimentaires.\n2PC a quand même un point problématique aussi, c’est qu’il a tendance à amplifier les failures, puisque dès qu’un nœud ne répond pas on va annuler la transaction. C’est pas très “fault tolerant” tout ça.\n\n\nLe consensus tolérant les fautes :\nOn peut citer 4 propriétés définissant le consensus :\n3 de safety :\nUniform agreement : tous les nœuds doivent arriver au même choix.\nIntegrity : aucun nœud ne décide deux fois.\nValidity : le choix décidé est valide.\n\n\nEt une de liveness :\nTermination : les nœuds ne se retrouvent pas bloqués, même en cas de crash de certains d’entre eux. Ils évoluent vers la terminaison du processus de choix.\n2PC ne remplit pas cette condition puisque le coordinator peut bloquer le système en cas de faute.\n\n\n\n\n\n\nLes algorithmes de consensus tolérants aux fautes sont difficiles à implémenter (donc on ne va pas les implémenter nous-mêmes mais utiliser des outils qui les implémentent).\nCe sont les suivants :\nViewstamped replication (VSR)\nPaxos\nRaft\nZab\n\n\nCes algorithmes sont de type total order broadcast.\nA chaque tour les nœuds décident du prochain message à traiter, et décident par consensus.\nPour le remplacement des leaders les nœuds utilisent des timeouts, et lancent une élection avec un quorum. Et c’est seulement quand le nœud a bien reçu le message de la majorité qu’il sait qu’il est bien le leader.\nCes algorithmes sont encore un sujet de recherche, et ont parfois des edge cases problématiques qui basculent le leader entre 2 nœuds, ou qui forcent en permanence le leader à renoncer.\n\n\n\n\nServices de coordination :\nLes services comme ZooKeeper sont rarement utilisés directement par les développeurs. On va plutôt les utiliser à travers d’autres services comme HBase, Hadoop YARN, OpenStack Nova et Kafka.\nCe sont en gros des stores de clé-valeur qui tiennent en RAM.\nZooKeeper a notamment ces caractéristiques :\nLinearizable atomic operations : à l’aide d’un lock, une seule opération parmi les opérations concurrentes peut réussir.\nTotal ordering of operations : les fencing tokens permettent de préserver l’ordre des transactions.\nFailure detection : les nœuds ZooKeeper et les autres nœuds s’envoient des messages régulièrement, et en cas de timeout déclarent le nœud échoué.\nChange notifications : les clients (les autres nœuds) peuvent s’abonner à des changements spécifiques des autres nœuds à travers ZooKeeper, ce qui évite de faire des requêtes pour voir où ça en est.\n\n\nZooKeeper est pratique pour des informations qui changent toutes les minutes ou heures comme l’association d’une adresse ip à un leader.\nSi on veut répliquer l’état d’une application qui peut nécessiter des milliers ou millions de changements par seconde, on peut utiliser des outils comme Apache BookKeeper.\n\n\nZooKeeper fait partie des membership services, issu d’une longue recherche depuis les années 80. En couplant le consensus avec la détection de fautes, ils permettent d’arriver à une certaine connaissance de qui composent les membres du réseau.\n\n\n\n\nIntégrer des systèmes disparates ensemble est l’une des choses les plus importantes à faire dans une application non triviale.\nLes données sont souvent classées en 2 catégories qu’il est de bon ton d'expliciter :\nLes systems of record qui sont les données de référence.\nLes derived data systems qui sont en général des données dénormalisées, par exemple stockées dans un cache.","10---batch-processing#10 - Batch Processing":"Il existe 3 types de systèmes :\nServices (online systems) : un client envoie un message et reçoit une réponse. En général, le temps de réponse et la disponibilité (availability) sont très importants.\nBatch processing systems (offline systems) : des tâches de fond, souvent exécutées périodiquement, durant plusieurs minutes voire plusieurs jours. La performance se mesure par la quantité de données traitées.\nStream processing systems (near-real-time systems) : il s’agit d’une forme particulière de batch processing. On ne répond pas à une requête d’un client humain, mais on réagit à un événement assez rapidement après qu’il ait eu lieu.\n\n\nLe batch processing avec les outils Unix :\nL’outil sort d’Unix va automatiquement prendre en charge des données plus grandes qu’il n’y a de mémoire vive, mettre ça en disque pour faire les opérations, et paralléliser au niveau CPU.\nLa philosophie unix est très proche de l’agile et du devops. On casse les gros problèmes en petits, on fait de petits programmes qui font une chose et la font bien. On fait des itérations courtes.\nUne des clés de la puissance des outils unix est l’interface uniforme, permettant de les composer ensemble. De nos jours c’est plutôt l’exception que la norme parmi les programmes.\nOn a également une séparation entre la logique et le câblage des données grâce à stdin et stdout.\nLes outils unix sont très pratiques pour l’expérimentation : les entrées sont immuables, et on peut envoyer la sortie vers un less par exemple.\nMais le plus souci c’est que les outils unix ne marchent que sur une machine, pas sur des architectures distribuées.\n\n\nMapReduce est un modèle assez bas niveau de batch processing, connu pour être l’algorithme qui permet à Google d’être aussi scalable.\nIl ressemble aux outils unix mais sur des architectures distribuées.\nIl prend des inputs, et envoie le résultat dans des outputs.\nLes inputs ne sont normalement pas modifiés et il n’y a pas de side-effects autre que les outputs.\nLes fichiers d’output sont écrits de manière séquentielle.\nAlors que les outils unix écrivent dans stdout, MapReduce écrit dans un système de fichiers distribués.\nHadoop utilise HDFS (Hadoop distributed File System), qui est une implémentation open source de Google File System.\nIl en existe d’autres comme GlusterFS, Quantcast File System (QFS).\nD’autres services sont similaires : Amazon S3, Azure Blob Storage, OpenStack Shift.\nFonctionnement de HDFS :\nHDFS est basé sur une approche shared-nothing, c'est-à-dire qu’il lui suffit d’ordinateurs connectés par un réseau ip classique.\nUn démon tourne sur chaque nœud et expose les fichiers qui sont sur ce nœud. Et un serveur central appelé NameNode contient des références vers ces fichiers.\nIl y a de la réplication entre les nœuds.\nDe cette manière HDFS est capable de faire fonctionner des dizaines de milliers de machines et des petabytes de données.\n\n\n\n\nLe fonctionnement se fait en 4 étapes :\n1- On lit des fichiers et on les structure sous forme d’entrées.\nC’est le parser qui s’en charge.\n\n\n2- on appelle la fonction mapper pour extraire des clés-valeurs\nIl s’agit ici d’une fonction où on peut ajouter du code à nous. La fonction est appelée une fois par entrée et permet d’extraire de la manière souhaitée les clés-valeurs.\n\n\n3- on trie les clés-valeurs par clés\nC’est fait automatiquement.\n\n\n4- on appelle la fonction reducer pour faire notre action sur les clés-valeurs\nLà encore on peut ajouter du code à nous. On a en paramètre toutes les valeurs associées à une clé et on peut en faire ce qu’on veut en sortie.\n\n\n\n\nOn peut aussi enchaîner plusieurs MapReduce, l’un préparant les données en entrée pour l’autre.\nMapReduce permet aussi de paralléliser les opérations de manière transparente pour le code. Comme il y a de nombreuses entrées à traiter, chacune peut s’exécuter localement sur la machine du réplica où elle est. Cela permet aussi d’éviter les transferts réseau en localisant les calculs.\nConcernant le code custom des fonctions mapper et reducer, dans Hadoop elles sont écrites en Java, alors que dans MongoDB et CouchDB elles sont écrites en Javascript.\nContrairement aux outils Unix, MapReduce ne permet pas de chaîner directement ses jobs. Il faut plutôt écrire le résultat d’un job dans un dossier, puis donner ce dossier comme entrée au MapReduce suivant. C’est du moins comme ça que ça se passe dans Hadoop.\nDu coup tout un tas d’outils permettent de coordonner les jobs MapReduce dans Hadoop : Oozie, Azkaban, Luigi, Airflow, Pinball.\nD’autres outils haut niveau autour de Hadoop permettent également de gérer ce genre de choses : Pig, Hive, Cascading, Crunch, FlumeJava.\n\n\nA propos des reduce-side joins avec MapReduce :\nOn ne va envisager les jointures que sur des tables entières pour notre cas qui concerne les batchs, typiquement quand on traite des BDD destinées à l’analyse des données.\nPar exemple : si on a d’un côté des événements avec un user id, et de l’autre côté la table des users avec certaines de leurs caractéristiques. On va vouloir corréler les deux pour ne sélectionner que les faits d’un certain type d’utilisateurs.\nPour des raisons de performance, on va opter pour le plus de localité possible, et donc on ne va pas faire des accès random en traitant chaque entrée une par une là où elle est. On va plutôt copier la table des users dans le même filesystem HDFS que la table des faits, puis on va lire les deux conjointement.\nLa technique des sort-merge joins permet à plusieurs mappers de trier des données par la même clé (par exemple l’id de l’utilisateur pour des événements dont il est l’objet, et pour des données personnelles sur l’utilisateur), puis à un reducer de récupérer ces données et de les merger ensemble pour faire l’action qu’on voulait avec cette jointure.\nUne fois que les mappers ont fait leur travail, chaque clé agit comme une adresse au sens où les valeurs d’une même clé vont être envoyées au même nœud pour que le reducer soit exécuté avec ces valeurs-là. Il y a bien une localité des données pour l’exécution du traitement.\nD’une certaine manière on a séparé l’obtention des données du traitement des données, ce qui contraste avec la plupart des applications où on fait des requêtes en BDD en plein milieu du code.\n\n\nDans certains cas on peut se retrouver avec des hot keys par exemple des données liées aux followers de célébrités. Ceci peut donner trop de charge à un nœud de reducer, et les autres devront alors l’attendre pour que l’opération de MapReduce soit terminée.\nPour éviter ça on va détecter les hot keys et les traiter différemment des autres clés. On va les séparer dans plusieurs reducers différents sur plusieurs nœuds, et ensuite on fusionnera le résultat final.\nPig fait d’abord une opération pour déterminer les hot keys, et ensuite fait le traitement de la manière décrite.\nCrunch a besoin qu’on lui dise explicitement les hot keys.\nHive a aussi besoin que les hot keys soient spécifiés explicitement dans une table de metadata séparée.\n\n\n\n\n\n\nA propos des map-side joins :\nLes reduce-side joins sont pratiques parce que les mappers lisent les données quelles qu’elles soient, préparent et trient et donnent ça au réducers. Mais tout ceci coûte en terme de copies au niveau du DD.\nSi nous avons des informations sur la structure des données, nous pouvons faire des map-side joins, où il s’agit simplement de tout faire dans les mappers et se débarrasser des reducers.\n\n\nUn cas où c’est utile est quand on doit faire une jointure entre un grand dataset et un petit dataset, suffisamment petit pour que ça puisse être chargé dans la RAM de chaque mapper. Chaque mapper aura alors à disposition l’ensemble du petit dataset pour chercher les entrées qui l’intéressent.\nOn appelle cet algorithme le broadcast hash join.\nCette méthode est supportée par Pig, Hive, Cascading et Crunch, ainsi que la data warehouse Impala.\n\n\nDans le cas où on a deux tables partitionnées de la même manière et qu’on veut faire une jointure dessus, on peut faire le map-side join sur chacune des partitions, ce qui permet de ne charger en mémoire qu’une faible quantité de données.\nSi les deux datasets sont en plus triés selon la même clé, alors on n’a pas besoin que l’une des deux entre en mémoire. Les mappers pourront chercher les données qui les intéressent du fait qu’elles sont triées de la même manière.\nLe choix d’un map-side join ou d’un reduce-side join a un impact sur les données résultant du MapReduce : avec le map-side les données seront partitionnées de la même manière qu’elles l’étaient à l’input, alors qu’avec le reduce-side, les données seront partitionnées selon la clé de la jointure.\n\n\nLe batch se rapprocherait plus des OLAP que des OLTP dans la mesure où il scanne une grande quantité de données, mais le résultat d’un batch sera une forme de structure et non pas un rapport à destination de data analysts.\nUn exemple de batch est l’utilisation initiale de MapReduce par Google pour faire des indexes pour son moteur de recherche. Encore aujourd’hui MapReduce est un bon moyen de créer des indexes pour Lucene/Solr.\nUn autre exemple est de construire des BDD key-value pour du machine learning, ou pour des systèmes de recommandation.\nOn pourrait penser que la bonne solution serait d’orienter la sortie du MapReduce vers notre BDD, entrée par entrée, mais c’est une mauvaise idée, à la fois pour des raisons de performance (localité des données, pas d’utilisation réseau, parallélisation des tâches) et d’atomicité du batch job.\nLa bonne solution consiste plutôt à créer une toute nouvelle BDD sur le filesystem distribué, de la même manière qu’on crée le fichier d’index.\nPlusieurs systèmes de BDD supportent le fait de créer des fichiers de BDD à partir d’opérations MapReduce : Voldemort, Terrapin, ElephantDB, HBase.\nCes fichiers sont écrits une fois et demeurent ensuite read-only.\nLes systèmes de BDD qui les supportent vont servir les anciennes données, commencer à copier ce fichier depuis le filesystem distribué vers le disque local, et dès que c’est fait switcher vers le fait de servir ces données-là.\n\n\n\n\n\n\nMapReduce suit la philosophie Unix :\nOn peut rejouer une opération MapReduce autant de fois qu’on veut sans dommages pour les données d’entrée.\nSi les données sont corrompues pour une raison éphémère, on retente l’opération.\nSi c’est un bug logiciel, on le résout, et on refait la même opération encore.\n\n\nOn a une séparation de la logique, et du câblage pour décider où vont les données.\nPar contre là où les outils unix font beaucoup de parsing parce que le format est le texte, Hadoop et compagnie peuvent utiliser Avro et Parquet pour permettre une évolutivité des schémas.\n\n\nComparaison entre Hadoop et les BDD distribuées :\nLes BDD distribuées implémentant le massively parallel processing (MPP) avaient déjà la capacité de faire des jointures distribuées en parallèle depuis 10 ans quand MapReduce est sorti. La différence c’est qu’elles obligent les données à respecter un schéma prédéfini.\nPar contraste, le modèle MapReduce a permis de collecter n’importe quelles données, y compris du texte, des images etc. et de les mettre tels quels, transférant alors le problème de l’interprétation de ces données au consommateur.\nÇa s'appelle le sushi principle : raw data is better. Et ça permet par exemple de consommer la même donnée différemment selon les contextes.\nOn peut par exemple collecter les données, et dans une étape séparée utiliser un MapReduce pour réorganiser ces données de manière à les transformer en data warehouse.\n\n\nLes BDD MPP sont efficaces pour le cas d’utilisation qu’elles prévoient : la manipulation des données via des requêtes SQL. En outre, ça fournit un bon niveau d’abstraction pour ne pas avoir à écrire de code.\nD’un autre côté, tout ne peut pas être traité avec des requêtes SQL. Si on a des utilisations particulières comme du machine learning, des systèmes de recommandation, de recherche dans du texte etc. alors on a probablement besoin d’exécuter du code custom sur ces données. C’est ce que permet MapReduce.\nSi on a MapReduce, on peut construire un modèle SQL par dessus. C’est ce qu’a fait Hive.\n\n\nLa versatilité permise par les raw data dans du Hadoop permettent d'implémenter du SQL, du MapReduce, mais aussi d’autres modèles encore.\nOn a des BDD OLTP comme HBase\nOn a des BDD analytiques comme Impala\nLes deux utilisent HDFS mais pas MapReduce.\n\n\nDeux autres différences :\nLa manière de gérer les fautes n’est pas la même : les systèmes MPP annulent la requête en cas de faute, alors que MapReduce va annuler une partie du job, peut être le mapper ou le reducer, et réessayer pour le terminer.\nLa gestion de la mémoire n’est pas la même : les systèmes MPP vont avoir tendance à stocker beaucoup en mémoire vive, alors que MapReduce va plutôt écrire sur disque dès que possible.\nCeci est en partie dû au fait que MapReduce a été fait par Google dans un contexte où les jobs de grande priorité et de faible priorité tournent sur les mêmes machines. En moyenne un job batch a 5% de chances d’être arrêté parce que ses ressources sont préemptées par un processus plus prioritaire. C’est aussi pour cette raison qu’on écrit sur disque dès que possible et qu’on tolère beaucoup les fautes.\nSans ce genre de contraintes de préemption, MapReduce pourrait se révéler moins pertinent dans sa manière de fonctionner.\n\n\n\n\n\n\n\n\nMalgré le succès de MapReduce dans les années 2000, il y a d’autres modèles intéressants.\nMapReduce, bien que simple à comprendre, n’est pas simple à mettre en œuvre. Par exemple, le moindre algorithme de jointure a besoin d’être refait from scratch.\nIl existe un ensemble d’outils construits par-dessus MapReduce, et qui fournissent d’autres abstractions (Pig, Hive, Cascading, Crunch).\nIl existe aussi des modèles complètement différents de MapReduce, et qui permettent d’obtenir de bien meilleures performances pour certaines tâches.\nContrairement aux programmes Unix, MapReduce fait de la matérialisation des états intermédiaires, c'est-à-dire que la sortie d’un MapReduce doit être complètement écrite avant de pouvoir être consommée par un autre processus. A contrario les programmes Unix mettent en place un buffer sous le forme du pipe qui permet au programme suivant de démarrer en consommant la sortie du précédent bout par bout au fur et à mesure.\nCeci a plusieurs désavantages :\nLe fait de devoir attendre qu’un job MapReduce soit complètement terminé avant d’entamer le suivant est source de lenteur.\nSouvent, le mapper ne sert qu’à lire le code déjà formaté correctement et est donc inutile. On pourrait alors chaîner plusieurs reducers.\nLe fait que les états intermédiaires matérialisés soient sur le filesytem distribué veut dire qu’ils sont aussi répliqués, ce qui est plutôt overkill pour l’usage qu’on en fait;\n\n\nPour répondre à ces problèmes, des dataflow engines ont été développés.\nParmi les plus connus il y a Spark, Tez et Flink.\nTez est relativement petit, alors que Spark et Flink sont des frameworks plus gros, avec leurs propres couches réseau, scheduler, API.\n\n\nIls permettent :\nde ne pas nécessairement faire l’étape de tri, ce qui permet de faire des économies quand l’ordre des entrées n’importe pas.\nde chaîner les operators (qui remplacent les mappers et reducers) dans l’ordre souhaité, ce qui permet aussi d’éviter les mappers inutiles.\ndes optimisations locales, sans faire appel au réseau, et sans écrire dans le filesystem distribué HDFS quand ce n’est pas nécessaire. On ne matérialise donc pas forcément les états intermédiaires.\nde commencer la prochaine opération dès que des données sont disponibles, et sans attendre que la précédente soit terminée.\n\n\nOn peut les utiliser pour faire les mêmes opérations qu’avec MapReduce, et comme les operators sont une généralisation des mappers et reducers, on peut switcher de MapReduce vers Spark ou Tez dans Pig, Hive ou Cascading.\n\n\nAlors qu’avec MapReduce on avait une bonne tolérance aux fautes, avec Spark, Flink et Tez on doit trouver d’autres astuces :\nSi la machine qui faisait le calcul est perdue, on trouve d’autres données qui permettent de reconstruire la donnée perdue : la liste des opérations appliquées, et un état précédent, ou au pire la donnée originale qui est sur HDFS.\nConcernant le problème du déterminisme, si une opération était non déterministe et que la donnée a été transmise à un autre acteur alors qu’on a une faute, alors il faut tuer l’acteur en question. Et de manière générale il faut éviter les opérations non déterministes.\n\n\n\n\nOn peut également utiliser les batch pour des données sous forme de graphs.\nPageRank est un exemple connu de système sous forme de graph.\nPour les parcourir et y faire des opérations, un MapReduce ne suffit pas puisqu’il ne peut faire qu’une lecture/écriture. Mais on peut répéter ce genre d’opérations sous forme itérative, tant qu’on n’a pas atteint le but recherché.\nCependant MapReduce n’est pas très efficace pour itérer plusieurs fois avec de petits changements.\nOn a alors un modèle appelé bulk synchronous parallel (BSP), aussi connu sous le nom de Pregel model, popularisé par un papier de Google.\nIl est implémenté par Apache Giraph, Spark GraphX API, Flink Gelly API.\nC’est la même chose qu’avec MapReduce sauf que les données sont conservées en mémoire, et en cas de faible changement, il n’y a que peu de choses à recréer.\nIl est résistant aux fautes, en vérifiant l’état de tous les vertices régulièrement, et en l’écrivant sur disque dur.\nLe calcul est parallélisé, et ça cause beaucoup de communication réseau. Dans la mesure du possible, si les données peuvent tenir en RAM sur un seul nœud, ou même sur son DD, il vaut mieux tenter l’approche non distribuée qui sera plus rapide, sinon le Pregel model est inévitable.\n\n\n\n\n\n\nA mesure que le temps passe, des couches sont construites par dessus MapReduce, permettant d’avoir des abstractions.\nLes jointures peuvent ainsi être faites par des opérateurs relationnels, permettant à l’outil de décider de la manière de l’implémenter. C’est supporté par Hive, Spark et Flink.\nGrâce à ces diverses abstractions, les batch processings se rapprochent des BDD distribuées d’un point de vue performance, tout en permettant quand c’est nécessaire, d’exécuter du code arbitrairement pour plus de flexibilité.","11---stream-processing#11 - Stream Processing":"L’idée derrière les streams c’est de faire la même chose que les batchs, mais de manière beaucoup plus récurrente, et jusqu'à la plus petite unité possible : plutôt que de faire le traitement une fois par jour on fait le traitement dès qu’on a des données nouvelles.\nLes données dans le stream processing sont des events. Ils sont mis à disposition par un producer, à destination de consumers. Ils sont groupés dans un stream d’events.\nComment transmettre les event streams :\nOn peut imaginer un mécanisme de polling où le producer met à disposition et les consumers vérifient régulièrement s’il n’y a pas de nouveaux events. Mais ça fait beaucoup de messages à envoyer si on veut être réactif. Il vaut mieux que les consumers soient notifiés à chaque event.\nEn général les BDD supportent mal cette technique. On a bien les triggers qui permettent d’exécuter du code à chaque requête, mais ça reste assez limité. Les BDD ne sont pas conçues pour ça.\n\n\nLa bonne solution est d’utiliser un messaging system.\nPour différencier ces systèmes, il faut regarder deux points :\nQue se passe-t-il si le producer crée plus d’events que les consumers ne peuvent consommer ?\nSoit les consumers sautent ces messages.\nSoit les messages sont mis dans un buffer qui grossit, et dans ce cas que se passe-t-il si ça continue de grossir jusqu’à dépasser la RAM ?\nSoit les consumers empêchent le producer de produire tant qu’ils n’ont pas fini les events déjà produits.\n\n\nQue se passe-t-il si le système est down ou que des nœuds crashent ? est-ce qu’on perd des events, ou est-ce qu’ils sont persistés / dupliqués ?\n\n\nUne première possibilité est la communication directe entre producer et consumers :\nDes librairies de messaging brokerless comme ZeroMQ et nanomsg utilisent TCP/IP pour communiquer.\nUDP multicast est un protocole qui permet d’envoyer des events sans garantie de réception.\nStatsD et Brubeck utilisent UDP pour envoyer des métriques en tolérant des pertes.\nLe consumer peut exposer une API REST ou RPC appelée par le producer. C’est l’idée des webhooks. Dans le cas où les consumers sont HS, il se peut simplement qu’ils ratent l’event.\n\n\nUne autre solution est l’utilisation de message brokers (ou message queues).\nCe sont en fait des BDD, soit in memory, soit avec une forme de persistance, qui mettent en relation les producers et consumers en général de manière asynchrone.\nIls tolèrent donc les crashs côté consumer, puisque le message pourra être traité plus tard.\n\n\nPar rapport aux BDD :\nIls ont des similarités, et peuvent même participer à des protocoles 2PC utilisant XA.\nMais il y a des différences :\nLes BDD gardent les données, alors que les brokers les effacent quand ça a été traité.\nLes brokers partent du principe que le nombre de messages à avoir en mémoire est faible. S’il grossit les performances peuvent se dégrader.\n\n\n\n\n\n\nLorsqu’il y a plusieurs consumers, on peut trouver deux stratégies pour leur envoyer les events :\nLoad balancing : si les messages coûtent cher à traiter, on donne chaque message à un consommateur.\nLes protocoles d’encapsulation AMQP et JMS supportent tous deux cette pratique.\n\n\nFan-out : Chaque consumer reçoit le message et peut le traiter indépendamment des autres.\nLà encore AMQP et JMS supportent cette pratique.\n\n\n\n\nPour que le broker sache quand il faut enlever le message de la queue et éviter de l’enlever en cas de crash du consumer, le consumer qui a traité le message doit faire un acknowledgement. Sinon le message reste et devra être traité.\nCes crashs peuvent causer un traitement des messages dans un ordre différent de celui d’arrivée. Si on veut éviter ça, on peut faire une queue par consumer.\n\n\n\n\nLes brokers traditionnels se distinguent des BDD ou des batches par le fait que les données sont détruites une fois traitées. Mais on peut très bien combiner la faible latence de traitement des messages (streaming) avec de la persistance durable : on a alors les log-based message brokers.\nIl s’agit d’écrire les events dans un fichier de log, comme on le ferait pour les LSM-Tree, ou les write-ahead logs. Les consumers peuvent alors traiter le fichier séquentiellement, et une fois à la fin être notifiés à chaque nouveau message.\nPour pouvoir scaler avec ce modèle au-delà de ce que peut supporter la lecture d’un seul disque, on peut utiliser le partitionnement : les messages sont partitionnés sur différentes machines représentant des producers, et des consumers viennent traiter les messages sur chaque partition.\nAu sein de chaque partition, on peut avoir un identifiant séquentiel indiquant l’ordre. Par contre, ça ne marche pas à travers les partitions.\nGrâce au partitionnement, ce type de log-based brokers, malgré le fait d’écrire sur disque, arrivent à traiter plusieurs millions de messages par seconde.\n\n\nCe type de broker est implémenté par Apache Kafka, Amazon Kinesis Streams et Twitter’s Distributed Log. Google Cloud Pub/Sub est architecturé de cette façon, mais expose une JMS-style API.\nLes log-based brokers supportent le fan-out messaging puisque les logs sont conservés et peuvent être lus un grand nombre de fois.\nPour ce qui est du load-balancing messaging, c’est mis en place à l’aide des partitions, qui sont assignés à des consumers spécifiques.\nIl y a des désavantages :\nOn n’a qu’un consumer par partition.\nLes messages lents d’une même partition vont ralentir les autres messages de cette partition.\n\n\n\n\nQuand utiliser les brokers classiques vs log-based :\nQuand le processing des messages peut être coûteux, et qu’on a envie de paralléliser message par message (et quand l’ordre des messages n’est pas très important), on peut utiliser les brokers de type JMS / AMQP.\nQuand en revanche les messages sont rapides à traiter, et que l’ordre importe, alors les log-based brokers sont pertinents.\nVu que l’ordre est respecté seulement au sein des partitions, on peut très bien choisir comme clé de partitionnement la chose dont on veut que les événement liés gardent le bon ordre. Par exemple l’id d’un utilisateur.\n\n\n\n\nÉtant donné que l’ordre est respecté au sein de chaque partition, on n’a plus besoin d'acknowledgement quand le traitement est fait pour chaque event. On sait que ce sera fait dans l’ordre et on peut regarder régulièrement le log offset de chaque consumer.\nSi un consumer échoue, un autre reprendra au dernier log offset connu. Et si des messages avaient été traités mais dont le log offset n’était pas connu, ils seront traités deux fois (il va falloir régler ce problème).\n\n\nA propos de l’espace disque :\nA force d’écrire des logs sur le DD, il finit par être plein, et il faut alors supprimer des données ou les bouger vers un espace d’archivage.\nCela veut donc dire que si on consumer est vraiment trop lent, il pourrait finir par ne plus avoir accès aux messages non lus qui ont été déplacés.\nIl faut quand même relativiser ça : un DD typique fait 6To, et en écrivant séquentiellement à la vitesse max on écrit en moyenne à 150 Mo/s. Ce qui fait 11 heures pour remplir le disque dur. Et sachant qu’on n’écrit pas en permanence à la vitesse max, en général des events de plusieurs jours vont pouvoir être stockés sur une même machine productrice.\nSi un consumer est trop en retard, on peut aussi lever une alerte pour qu’un être humain gère. Vu les délais, il aura normalement le temps de régler la situation.\n\n\nOn peut noter aussi que pour les log-based brokers, vu qu’ils écrivent toujours sur DD, le temps de traitement reste à peu près constant, alors que pour les brokers plus classiques, si on dépasse la RAM et qu’on doit écrire sur DD, les performances se dégradent.\n\n\nOn peut remarquer que les log-based brokers sont plus proches des batches que les brokers classiques. Les données anciennes étant conservées, on peut les rejouer à loisir pour faire des tâches dessus.\n\n\n\n\nLes streams et les bases de données :\nLes principes des streams peuvent aussi être utiles pour les BDD.\nPar exemple, le replication log envoyé par le leader n’est rien d’autre qu’un stream.\nOn peut aussi considérer que chaque opération d’écriture en BDD est un événement, et qu’on peut reconstruire la BDD à partir du log d’events déterministes.\n\n\nOn se retrouve souvent avec des copies des données sous différents formats pour différents usages (cache, data warehouse etc.). Mais comment garder ces données synchronisées ?\nUne solution est d’utiliser les batches. Mais c’est lent, et on n’aura pas de données à jour rapidement.\nUne autre solution serait d’écrire en même temps dans la BDD principale et dans ces autres copies. Mais dans des systèmes distribués il peut survenir des inconsistances entre ces copies.\nPour régler ce problème, on pourrait transformer les copies en suiveuses de la BDD principales comme avec le modèle leader / follower.\nMalheureusement pendant longtemps les logs des messages allant dans la BDD ont été considérés comme des API privées. Récemment on a un intérêt vers le fait de les exploiter comme des streams qu’on appelle change data capture (CDC).\nLa solution est d’utiliser un log-based broker pour transporter les events d’écriture de la BDD (leader) vers les datasets qui sont des followers (search index, data warehouse etc.).\nC’est utilisé par Databus de LinkedIn, Wormhole de Facebook et Sherpa de Yahoo.\nBottled Water le fait pour PostgreSQL en lisant son write-ahead log.\nMaxwell et Debezium le font pour MySQL.\nMongodriver le fait pour MongoDB.\nGoldenGate le fait pour Oracle.\nKafka Connect Framework offre des connecteurs CDC pour divers BDD.\nRethinkDB, Firebase, CouchDB, MongoDB et VoltDB permettent aussi d’avoir un mécanisme pour exporter le stream des données hors de la BDD.\nEn général, cette solution est utilisée dans un mode de réplication asynchrone.\n\n\nCertains outils permettent de commencer un dataset suiveur avec un snapshot initial des données, plutôt que de récupérer la totalité des logs pour reconstruire la BDD.\nCertains outils comme Apache Kafka permettent aussi de récupérer les logs compactés, au sens de la compaction des LSM-Tree : seules les logs représentant la dernière version des entrées sont gardées. Si une entrée est supprimée à un moment, toutes les logs précédentes de cette entrée peuvent être supprimées aussi par la compaction par exemple.\n\n\n\n\nEvent sourcing : c’est une idée développée par la communauté domain-driven design (DDD).\nCela consiste à stocker tous les changements d’état d’une application sous forme de logs de change events\nLa différence entre le change data capture de la BDD et l’event sourcing c’est que le change data capture permet d'ajouter / enlever / modifier des choses dans la BDD et d'en faire un log, alors que l’event sourcing décourage ou interdit la modification, mais consiste plutôt à accumuler des events qui représentent des choses qui se produisent plutôt que de simples changements d’état qui s’annuleraient entre eux.\nLa conséquence est qu’on ne peut pas vraiment faire de compaction pour les events de l’event sourcing, parce qu’ils ne s’annulent pas entre eux à proprement parler. Il faut garder ces events immuables.\n\n\nL’event sourcing est un modèle très puissant pour représenter clairement ce qui se passe dans l'application, et permet aussi d’avoir des facilités pour débugger.\nIl existe des BDD spéciales pour l’event sourcing comme Event Store, mais en réalité n’importe quelle BDD ou message broker serait adapté.\nL’event sourcing sépare bien les events des commands. Quand une requête arrive de l’utilisateur c’est d’abord une command. Elle doit être traitée et validée, et c’est seulement quand on est sûr qu’elle l’est qu’elle devient un event immuable. Elle est alors transmise à divers systèmes consommateurs et ne peut pas être supprimée, mais seulement changée par un autre event d’annulation par exemple.\n\n\nLes streams et les états vis à vis de l’immuabilité :\nOn peut voir la BDD comme étant un sous ensemble, ou une version cache la plus récente des données que sont les logs d’events. Avec le mécanisme de compaction des SSTables c’est encore plus évident puisqu’on a les logs, et on vient enlever ce qui est “inutile” pour obtenir la BDD qui est l’état le plus actuel des données.\nUn des avantages à avoir les logs des changements immuables comme source de vérité principale à partir de laquelle on peut construire diverses formes de dataset est que même si on fait une opération malheureuse qui corrompt les données, si c’est juste sous forme de log il suffira de revenir en arrière dans les logs et c’est réglé. Avec une vraie BDD si on a corrompu les données on risque de ne pas pouvoir défaire.\nOn peut dériver diverses formes de données à partir des logs :\nPar exemple, Druid ingère les données de Kafka, de même pour Pistachio qui utilise Kafka comme un commit log, et Kafka Connect peut exporter les données de Kafka vers diverses BDD ou indexes.\n\n\nStocker les données est facile si on n’a pas à se préoccuper de le faire dans un format qui permettra une lecture optimisée en fonction de notre contexte. On peut donc séparer l’écriture de la lecture, en créant de nouveaux dataset dérivés quand on a besoin des données pour faire quelque chose de spécifique.\nCette idée de séparer les données d’écriture et de lecture est connue sous le nom CQRS (Command Query Responsibility Segregation).\nDans cette approche la question de “faut-il vraiment dénormaliser ?” ne se pose plus vraiment : il est logique de dénormaliser pour optimiser en lecture, vu que de toute façon les données seront présentes sous une forme plus canonique dans la version écrite.\n\n\nAvantages et inconvénients de l'event sourcing et du change data capture :\nUn des inconvénients est que si l’écriture se fait de manière asynchrone pour gagner du temps (ce qui est souvent le cas), on risque de ne pas avoir la garantie de read after your writes par exemple. Pour remédier à ça on pourrait rendre la copie synchrone, ou utiliser des transactions distribuées, ou un total order broadcast.\nUn des avantages est que ça peut faciliter la concurrency control : à chaque fois qu’une requête a besoin de modifier plusieurs objets, on peut très bien écrire un event dans le log qui implique l’ensemble de ces objets. Et donc on aurait des opérations atomiques écrites en une fois.\n\n\nA propos de l'immuabilité :\nElle est utile si les données ne changent pas tant que ça, mais si elles changent beaucoup on risque de se retrouver avec des logs énormes pour pas beaucoup de données.\nOn a aussi des contraintes légales qui imposent parfois de supprimer certaines données.\nOn peut alors réécrire l’historique pour enlever certaines données. Datomic appelle ça l’excision.\nIl faut savoir aussi qu’étant donné les diverses copies de dataset, backups et autres, c’est assez difficile de complètement supprimer les données.\n\n\n\n\n\n\n\n\nTraitement des streams.\nOn peut faire 3 choses avec un stream :\nL’écrire en BDD ou autre forme de persistance.\nLe donner directement à l’utilisateur en lui affichant.\nLe modifier pour fabriquer un nouveau stream à travers un operator comme avec les batchs, dont le résultat ira à nouveau dans une persistance ou chez l’utilisateur.\n\n\nTout ceci est assez similaire à ce qui se passe avec les batchs.\nLa différence c’est que le stream ne se finit pas, et donc on ne peut pas faire de sort ou de jointures sort-merge comme avec les batchs.\nLa tolérance aux erreurs aussi n’est pas la même : on peut difficilement rejouer un stream qui tourne depuis des années comme on rejouerait un batch qui vient d’échouer.\n\n\nA propos des usages du streaming :\nOn l’utilise pour du monitoring quand on veut être averti de choses particulières, par exemple avec la détection de fraudes, le statut des machines d’une usine etc.\nLes complex event processing (CEP) permettent de déclarer des patterns à trouver (souvent avec du SQL), et créent des complex events à chaque fois que ça match, il s’agit de trouver une combinaison d’events.\nC’est implémenté dans Esper, IBM InfoSphere Streams, Apama, TIBCO StreamBase, SQLstream.\n\n\nLes stream analytics qui ressemblent aux CEP mais sont plus orientés vers le fait de trouver des résultats agrégés à partir des données streamées. Par exemple calculer une moyenne, une statistique.\nOn utilise souvent des fenêtres de données pour faire les calculs dessus.\nOn utilise parfois des algorithmes probabilistes comme les Bloom Filters pour savoir si un élément est dans un set et d’autres. Ces algorithmes produisent des résultats approximatifs mais utilisent moins de mémoire.\nParmi les outils on a Apache Storm, Spark Streaming, Flink, Concord, Samza et Kafka Streams. Et parmi les outils hostés on a Google Cloud Dataflow et Azure Stream Analytics.\n\n\nLes dataset dérivées des logs comme dans l’event sourcing peuvent être vus comme des materialized views, dans ce cas il faut prendre en compte l’ensemble des logs et pas juste une fenêtre.\nSamza et Kafka Streams font ça.\n\n\nOn peut faire aussi un peu pareil que les CEP mais en recherchant un seul event qui match un critère de recherche. Alors que d’habitude on doit indexer avant de faire une recherche, là il s’agit de rechercher en plein streaming.\nLa feature percolator d’Elasticsearch permet de faire ça.\n\n\n\n\nLa notion de temps dans la gestion des stream processing :\nAlors que dans les batch processing ce qui compte c’est éventuellement le timestamp des events analysés, et pas le temps pendant le quel le batch s’exécute (ce qui rend la réexécution du batch transparente d’ailleurs), dans le cadre du stream processing le temps pendant lequel le processing s’exécute peut être pris en compte, par exemple pour faire des fenêtres.\nAttention cependant aux lags : il est possible que lors du stream processing un event soit processé bien après avoir été émis. Et dans ce cas on peut se retrouver avec des events traités dans un ordre qui n’est pas le bon vis-à-vis de leur émission.\n\n\nQuand on stream avec des fenêtres de temps contenant des events pour y faire des opérations, on ne peut jamais être sûr que tous les events de la fenêtre sont arrivés : ils ont peut être été retardés (qu’on appelle straggler)\nDans ce cas, soit on dit tant pis et on annule les events retardataires, en levant éventuellement une alerte s’il y en a trop.\nSoit on publie plus tard un correctif avec les events retardataires.\n\n\nQuand on veut prendre en compte le temps, le temps de la création de l’event est souvent plus précis (par exemple un event peut être créé offline par un mobile, et envoyé seulement quand il est connecté), mais aussi moins fiable vu que la machine n’est pas sous notre contrôle contrairement au serveur.\nUne des solutions est de relever (1) le temps de l’event indiqué par le client, (2) le temps de l’envoi de l’event indiqué par le client, et (3) le temps de la réception de l’event par le serveur. De cette manière on peut comparer les horloges du client et du serveur vu que le (2) et le (3) doivent être très proches.\n\n\nIl y a plusieurs types de fenêtres temporelles :\nTumbling window : Les fenêtres sont fixes, et chaque event appartient à une fenêtre.\nHopping window : Les fenêtres font la même taille mais se chevauchent, certains events qui sont entre les deux sont dans les deux fenêtres.\nSliding window : Les fenêtres font la même taille mais se déplacent dans le temps, et donc les events les plus anciens sont exclus au fur et à mesure, remplacés par des events plus récents.\nSession window : Les fenêtres n’ont pas la même taille, elles regroupent des events proches dans le temps où un même utilisateur a été actif.\n\n\n\n\nLes streams étant une généralisation des batchs, on a ici le même besoin des jointures.\nOn peut dénombrer 3 types :\nLe stream-stream join (window join) consiste à joindre deux ensemble streams d’events ensemble. Par exemple dans le cadre d’une recherche, joindre les events de recherches faites aux events clics qui s’en sont suivis (ou à l’absence de clics après timeout).\nLe stream-table join (stream enrichement) consiste à “enrichir” les events issus d’un stream avec le contenu d’une BDD. Par exemple les actions d’un utilisateur enrichis (complétés ou triés) avec des infos issus de son profil.\nPour ce faire il nous faut une copie de la BDD sur le disque local de préférence, et si suffisamment petit on peut même la mettre en RAM. C’est très similaire aux Map-side joins des batchs.\nVu que les données de la table risquent d’être mises à jour, on peut utiliser le change data capture pour récolter les mises à jour de la table régulièrement.\n\n\nLe table-table join (materialized view maintenance) consiste à matérialiser une requête de jointure entre deux tables, à chaque fois qu’il y a un changement dans ces deux tables qui risque d’affecter le résultat de cette jointure.\nOn peut prendre l’exemple de twitter qui, en même temps qu’il stocke les tweets et followers, construit une timeline en cache au fur et à mesure.\n\n\n\n\nOn remarque que dans la plupart des cas, le temps est important, et que deux événements, ou un événement et une mise à jour en BDD pourraient arriver avant ou après l’autre (du fait du partitionnement). Ceci rend la jointure non déterministe (si on la refait on risque d’avoir un résultat différent).\nDans les data warehouses ce problème s’appelle _slowly changing dimension (SCD) _et la solution à ça peut être d’ajouter un identifiant qui est changé à chaque event. Mais la conséquence c’est qu’on ne peut plus faire de compaction.\n\n\n\n\nA propos des fautes dans le cadre des streams :\nL’avantage avec les batchs c’était le fait de pouvoir réexécuter en cas d’erreur, et d’avoir au final le job exécuté comme s’il l’avait été une seule fois.\nUne des solutions est le microbatching : on fait des petites fenêtres de données (souvent d’1 seconde) et on les traite comme des batchs.\nSpark Streaming fait ça.\nUne variante consiste à faire des checkpoints réguliers sur DD, et en cas de crash on recommence à partir du checkpoint.\nFlink fait ça.\n\n\nAttention cependant au moment où on fait autre chose avec ces données, comme écrire en BDD ou envoyer un email. Dans ce cas, il s’agit de side effects qui pourront être réexécutés en cas de réexécution du microbatch.\nPour régler ce problème, il faut tout préparer, et exécuter tout ce qui est validation des opérations, side-effects et autres en une seule fois et de manière atomique.\nC’est un peu de la même manière que le 2PC (two phase commit) des transactions distribuées.\nC’est utilisé par Google Cloud Dataflow, VoltDB et Apache Kafka.\n\n\nUne autre solution pour ce problème est de créer de l’idempotence, c’est-à-dire faire en sorte qu’une chose faite plusieurs fois donne le même résultat.\nOn peut le faire par exemple en retenant un offset qui fera en sorte de ne rien faire si on tente de refaire l’opération.\nAttention au fait que cela implique qu’il faut rejouer les messages dans le même ordre (un broker log-based permet ça), de manière déterministe, et sans concurrence.\n\n\n\n\n\n\n\n\nOn peut aussi vouloir que des states (par exemple compteurs, moyennes etc.) soient reconstruites en cas de faute.\nDans certains cas ça peut être fait à partir des events, par exemple parce qu’il s’agit d’un état qui porte sur peu d’entre eux.\nSinon une solution peut être de les sauvegarder régulièrement quelque part pour aller les chercher en cas de besoin.\nFlink capture régulièrement ces valeurs et les écrit sur du HDFS.\nSamza et Kafka Streams répliquent les changements des states vers un stockage persistant avec compaction.\nVoltDB réplique les states en faisant le processing des messages sur plusieurs nœuds.\nIl faut voir que la sauvegarde en local avec accès au disque ou la sauvegarde via le réseau peuvent chacun être plus ou moins performants en fonction des cas.","12---the-future-of-data-systems#12 - The Future of Data Systems":"Chaque outil a ses avantages et inconvénients, et il n’y a pas d’outils parfaits.\nCertaines personnes disent que tel ou tel type d'outil n’a aucune utilité, mais ça reflète surtout le fait qu’eux ne l’utilisent pas, et qu’ils ne voient pas plus loin que le bout de leur nez.\n\n\nIl convient souvent de combiner plusieurs outils pour plusieurs usages :\nParmi ceux-ci on peut trouver :\nUne BDD relationnelle pour la persistance de données structurées. (ex : PostgreSQL)\nUn index de recherche pour une recherche performante, mais qui est moins bon sur la persistance des données (ex : Elasticsearch)\nUn système d’analyse du type data warehouse ou batch / stream processing.\nParmi les batchs / streams on pourrait vouloir alimenter un système de machine learning, de classification, de ranking, de recommandations, de notification basée sur le changement de données.\n\n\nUn cache ou des données dénormalisées issues des données initiales.\n\n\nPar exemple, on peut avoir une BDD et un search index, avec les données écrites d’abord dans la BDD, puis propagées dans le search index via change data capture (CDC).\nSi on décide qu’on veut écrire à la fois dans la BDD, et dans le search index, alors on risque d’avoir des latences qui causent des différences d’ordre d’écriture entre les deux.\nUne solution à ça c’est d’utiliser un système d’entonnoir qui force l’ordre, dans l’idée d’un total order broadcast.\n\n\n\n\nQue choisir entre les données dérivées (CDC, event sourcing) et les transactions distribuées (2PC) pour faire communiquer plusieurs outils entre eux ?\nSelon l’auteur, XA, le protocole qui permet de faire communiquer les outils via les transactions distribuées a une mauvaise tolérance aux fautes et une faible performance. Et en l’absence d’un autre protocole aussi répandu (ce qui ne risque pas d’arriver rapidement), il est plus pertinent d’opter pour les datasets dérivés.\nCependant, les transactions distribuées supportent la linearizability et donc permettent par exemple le “read your own writes”, alors que les données dérivées sont en général asynchrones et donc n’apportent pas ces garanties. Cette eventual consistency est à mettre dans la balance.\nPlus tard on parlera d’un moyen de contourner ce problème.\n\n\n\n\n\n\nAttention au fait de vouloir du total ordering :\nPour avoir du total ordering il faut que les données passent par une seule machine (par exemple single leader). Sinon on peut créer des partitions mais on aura des ambiguïtés entre partitions.\nDans le cas de plusieurs datacenters on a en général besoin de 2 leaders => on n’aura donc pas de total ordering.\nQuand on fait du micro-service, il est courant de déployer le code sur des machines avec chacune son stockage et sans que ce stockage soit donc partagé => on se retrouve là aussi donc à ne pas respecter le total ordering.\nPour être clair : le total ordering implique le total order broadcast, qui est équivalent au consensus. Et la plupart des algorithmes de consensus ne sont pas faits pour marcher si le throughput dépasse les données que peut gérer un seul serveur. Le fait de pouvoir gérer un tel throughput avec des datacenters distribués dans le monde est un sujet de recherche.\n\n\nA propos de l’ordre causal :\nPour les événements qui touchent le même objet, celui-ci étant sur la même partition on peut ordonner ces actions et respecter la causalité.\nEn revanche, pour les événements qui portent sur plusieurs objets il n’y a pas de solution facile. Quelques pistes :\nLes clocks logiques peuvent aider.\nSi on log des events pendant pour les lectures, alors les autres évents peuvent les utiliser pour identifier le moment où un événement ne s’était pas produit et créer un ordre comme ça.\nLes structures de résolution automatique de conflit (fusion des objets par exemple) peuvent aussi aider.\n\n\n\n\nA propos des batches et streams :\nUne des raisons pour lesquelles il est pratique d’avoir des dataset dérivés par batch/stream plutôt que transactions distribuées est qu’on peut fauter quelque part et ne pas tout annuler, mais seulement retenter la construction du batch/stream.\nUn des avantages des batchs/streams c’est qu’avec les datasets dérivés, on peut changer le schéma de nos données pour un dataset, et reprocesser le tout, ou continuer pour le stream. On n’a pas à faire d’opérations destructives pour faire évoluer notre code.\nOn peut d’ailleurs faire les changements graduellement, blocs de données par bloc de données.\n\n\nLa lambda architecture consiste à avoir un batch et un stream qui vont processer la même chose pour avoir la donnée immédiatement, mais avoir un process mieux tolérant aux erreurs plus tard. Le stream fait une approximation, alors que le batch fait un calcul plus précis régulièrement.\nIl y a cependant plusieurs problèmes :\nMaintenir la logique dans le batch et le stream est difficile.\nIl faut merger les deux régulièrement, et ça peut être difficile si les opérations appliquées sont difficiles.\nReprocesser toutes les données avec le batch est très coûteux, donc on peut à la place reprocesser une seule heure de données et y ajouter le stream. Cependant, rendre le batch incrémental le fragilise.\n\n\nMais plus récemment on a d’autres solutions pour rendre la lambda architecture plus utilisable grâce à certaines features qui sont de plus en plus supportés par les outils.\n\n\n\n\n\n\nA propos de BDD :\nLes BDD et les filesystem font la même tâche.\nMais ils ont certaines différences : les filesystem Unix offrent une API bas niveau pour traiter avec les fichiers, alors que les BDD offrent une API plus haut niveau avec SQL.\nD’une certaine manière certaines BDD NoSQL tentent d’ajouter la philosophie Unix aux BDD.\n\n\nLes BDD et les batchs / streams ont des choses en commun :\nPar exemple, la construction de search indexs dans les batchs/streams sont un peu la même chose que la construction d’index secondaires.\nEt du coup on en arrive à la conclusion qu’en fait les batchs/streams ne sont que la continuation d’une même base de données transformée pour l'adapter aux besoins, distribuée sur d'autres machines et administrée éventuellement par d’autres équipes.\nL’auteur spécule que les données seront organisées en deux grands axes, qui sont en fait deux faces de la même pièce :\nFederated databases (unifying reads) : il s’agit de fournir une API de lecture pour accéder à toutes les données existantes du système, tout en laissant les applications spécialisées accéder directement aux datasets spécifiques dont elles ont besoin. L’idée est donc de connecter toutes les données ensemble.\nPostgreSQL et son foreign data wrapper permet de faire ça.\n\n\nUnbundled databases (unifying writes) : il s’agit de traiter les écritures pour qu’on puisse écrire dans n’importe quelle version des données, et qu’elles soient quand même synchronisées avec le reste. Alors que les BDD supportent les indexes secondaires, ici on a différents datasets interconnectés et donc on doit en quelque sorte maintenir nos indexes à la main.\nOn est en plein dans la tradition Unix où des petits outils font une chose bien, et peuvent s’interconnecter.\nAlors que la fédération des données n’est pas trop difficile, maintenir les données synchronisées est plus compliqué à faire.\nPour accomplir ces données synchronisées on recourt traditionnellement aux transactions distribuées, mais selon l’auteur c’est la mauvaise approche. L’approche sous forme de données dérivées depuis un event log asynchrone, et l’utilisation de l’idempotence est bien plus solide.\nUne des raisons déjà évoquée est que faire communiquer des systèmes de données hétérogènes via un mauvais protocole marche moins bien que via une meilleure abstraction avec des logs d’event et de l’idempotence.\nLe gros plus de l’approche avec les event logs est le couplage faible entre les composants :\nLa nature asynchrone de cette approche permet de tolérer bien mieux les fautes (par exemple, un consommateur fautif va rattraper son retard plus tard via les messages accumulés) alors qu’avec les transactions distribuées synchrones par nature, les fautes ont tendance à être amplifiées.\nAu niveau des équipes, chacune peut se spécialiser dans un type de dataset pour un usage, et le faire indépendamment des autres.\n\n\n\n\nEntre utiliser un système de BDD intégré et un système composé de datasets dérivés, le choix des datasets dérivés n’est pas forcément systématique. Ça peut être une forme d’optimisation prématurée, et d’ailleurs si un système de BDD répond à nos besoins, autant l’utiliser lui seul.\nCe qui manque dans l’histoire c’est une manière simple et haut niveau d’interconnecter ces systèmes, par exemple “declare mysql | elasticsearch” comme équivalent de “CREATE INDEX” dans une BDD.\nIl y a des recherches à ce sujet mais pour le moment rien de tel, on doit faire beaucoup de choses à la main.\n\n\n\n\n\n\n\n\nPour continuer sur l’idée de l’unbundling databases, et des applications autour du dataflow :\nOn peut trouver des ressemblances avec le concept d’unbundling des BDD et des langages de dataflow comme Oz, Juttle, les langages fonctionnels réactifs comme Erlang, et les langages de programmation logique comme Bloom.\nL’idée de l’unbundling est aussi présente dans les tableurs quand ils mettent à jour toute une colonne dès qu’une donnée est écrite. Il faut juste faire la même chose mais dans un contexte distribué, et avec des technologies disparates.\nOn a différentes formes de données dérivées, mais en gros dès que la dérivation est spécifique à notre métier, il faut écrire du code applicatif pour gérer ce dataset-là. Et les BDD ont en général du mal à permettre ça. Il y a bien les triggers / stored procedures, mais ça reste une feature secondaire.\nIl est devenu un pattern courant et une bonne pratique de séparer le code applicatif du state (ie. la persistance), en ayant des serveurs stateless qui accèdent à une BDD commune.\nLes développeurs fonctionnels disent qu’ils sont pour “la séparation de l’église et de l’état”.\nCependant, de même que dans la plupart des langages il n’y a pas de système de souscription (sauf à le faire avec le pattern observer), dans les BDD il n’y en a pas non plus sauf récemment avec les CDC par exemple.\n\n\nVu qu’on veut sortir la logique de mise à jour automatique par exemple d’un index dans la BDD hors de celle-ci, on peut partir du principe que la donnée n’est pas une chose passive utilisée par l’application, mais que les changements dans un dataset peuvent déclencher du code applicatif pour créer un autre dataset.\nA cet effet, on peut utiliser des log message brokers (et non pas des message brokers traditionnels qui servent à exécuter des jobs de manière asynchrone).\nL’ordre des messages est souvent important pour maintenir des datasets dérivés.\nOn doit être tolérant aux fautes et ne pas perdre de messages, sous peine d’inconsistance.\nLes message brokers permettent au code applicatif de s’exécuter sous forme d’operators, ce qui est pratique.\n\n\n\n\nLe stream processing et les services :\nL’architecture sous forme de services est plutôt à la mode, son avantage principal est de permettre une forme de scalabilité dans l’entreprise, en permettant à plusieurs équipes de déployer séparément.\nIl y a cependant une différence entre les services qui vont envoyer un message pour recevoir une réponse du service qui a les données, et le stream processing qui va construire et maintenir à jour un dataset local à la machine qui a le code applicatif, qui n’aura plus de requête réseau à faire => la méthode avec le stream processing est donc plus performante.\n\n\n\n\nLecture des données dérivées :\nLes données dérivées sont construites et mises à jour en observant la donnée initiale et la faisant passer à travers des operators, tout ceci pendant la phase d’écriture. On a ensuite du code exécuté qui lit ces données dérivées et qui répond à une requête d’un client, pendant la phase de lecture donc.\nCe point de rencontre représente en quelque sorte le point d’équilibre entre la quantité de travail qu’on souhaite faire à l’écriture, et la quantité de travail qu’on souhaite faire à la lecture.\nOn peut déplacer ce point de rencontre pour faire plus de travail à l’écriture, ou plus à la lecture.\nPar exemple pour la recherche, on peut très bien ne pas créer de search index, et tout faire à la lecture.\nOu alors on peut non seulement créer un search index à l’écriture, mais aussi créer tous les résultats de recherche possibles, comme ça à la lecture on n’aura plus qu’à lire un cache (aussi appelé materialized view).\nSi créer l’ensemble des résultats de recherche serait sans doute excessif, on peut très bien imaginer mettre en cache les résultats des recherches les plus fréquentes.\n\n\nOn voit qu’on retrouve aussi notre exemple de twitter qui avait choisi de mettre en cache toutes les timelines, sauf pour les célébrités où il faisait la recherche en BDD.\n\n\nAutre exemple de lecture de données dérivées : les applications web sur mobile qui gagnent de plus de capacité d’autonomie, y compris offline, peuvent stocker une forme de dataset dérivé au sein même du mobile, permettant au code sur le client d’en faire quelque chose offline.\nLes outils frontend comme le langage Elm et le framework React / Redux permettent de souscrire à des events de l’utilisateur, en mode event sourcing.\nIl serait tout à fait naturel de faire la même chose dans la relation client / serveur : permettre au client de faire une requête, puis de réceptionner non pas une réponse mais un stream de messages réguliers.\n\n\n\n\nLes log message brokers passent en général leur contenu à une forme ou une autre de BDD spécialisée, mais il y a aussi une certaine persistance des events eux-mêmes (les logs) dans le message broker. En général seuls les events d’écriture y sont consignés, ce qui est raisonnable mais n’est pas la seule manière de faire possible.\nIl est possible qu’en fonction du besoin applicatif, on ait aussi intérêt à consigner les events de lecture. Ça permet notamment de faire un stream-table join entre les lectures et les données existantes.\nC’est utile en particulier dans le cas où on a plusieurs partitions qu’il faut traverser pour obtenir notre résultat.\nLa feature de distributed RPC de Storm implémente cette fonctionnalité.\n\n\nÇa prend bien sûr plus de place donc il faut y réfléchir.\nUn des avantages est que ça permet aussi de régler le problème de causalité vis-à-vis d’écritures sur des objets différents.\n\n\n\n\n\n\nA la recherche des données correctes :\nOn a tendance à avoir un mouvement vers une plus grande performance, availability et scalability, avec une consistency qui est parfois délaissée.\nExemple : la réplication leaderless.\nOn peut aussi noter le rapport hasardeux à l’isolation et l’implémentation de faibles niveaux d’isolation dans beaucoup de BDD.\n\n\nOn peut répondre à certaines problématiques de corruption de données à l’aide de la serializability et des atomic commits, mais c’est vraiment coûteux et ça marche surtout sur un seul datacenter, avec des limites de scalabilité.\nIl y a aussi les transactions qui permettent de régler certains problèmes, mais ce n’est pas la seule solution.\n\n\nN’oublions pas non plus les erreurs et bugs applicatifs qui peuvent endommager les données de manière définitive, même en présence de serializability…\nPour lutter contre ces problèmes voici quelques solutions :\nL’immutabilité des données (du genre event sourcing et autres) permet d’être sûr que même en écrivant des données corrompues, on pourra toujours les annuler pour retrouver l’état d’avant.\nRendre les opérations idempotentes pour qu’elles ne puissent être exécutées qu’une seule fois au plus est une forme de protection contre la corruption de données.\nDe manière générale, il est intéressant d’implémenter des mécanismes end-to-end qui vont suivre la requête de bout en bout. Par exemple TCP fournit ce genre de garanties à son niveau, mais une connexion TCP peut sauter et on peut en établir une autre pour refaire la même transaction, on a alors besoin de quelque chose qui suit notre transaction.\nMalheureusement, implémenter de tels mécanismes end-to-end au niveau applicatif n’est pas simple. Pour l’auteur, il faudrait qu’on trouve la bonne abstraction pour rendre ça facile, mais il y a de la recherche à faire.\n\n\n\n\nAppliquer des contraintes :\nLa contrainte de uniqueness dans un système distribué nécessite le consensus, et donc une forme de fonctionnement synchrone. Si les writes se faisaient de manière asynchrone, alors on ne saurait pas immédiatement si on peut écrire en respectant cette contrainte ou pas, et on aurait le conflit plus tard.\nUn bon moyen pour garantir cette contrainte est de partitionner en fonction de la clé qui doit avoir la contrainte d’unicité. Mais là aussi bien sûr on ne pourra pas écrire dans la BDD de manière asynchrone.\n\n\nPour les contraintes au sein des log-based message brokers il s’agit aussi de faire en sorte que les requêtes avec possibilité de conflit soient dans la même partition, et de vérifier séquentiellement, message par message, que la requête respecte bien la contrainte vis-à-vis de la BDD locale.\nDans le cas où les entrées qui sont l’objet de contraintes sont localisées dans des partitions différentes ça se complique un peu.\nOn peut utiliser un atomic commit (par exemple 2PC).\nMais on peut aussi faire sans (exemple de débit / crédit d’un compte) :\nOn attribue un id à la requête.\nLe stream processor crée 2 messages : un pour décrémenter le compte qui a un débit, et un autre pour incrémenter l’autre compte (qui sont chacun sur leur partition).\nLes processors suivants consomment les messages, appliquent le débit ou le crédit, et dédupliquent en fonction de l’id de la transaction initiale.\nVu qu’on est dans un log based broker avec l’ordre des messages préservés et de la persistance, en cas de crash de l’un des consommateurs, il redémarre et réapplique les messages non processés dans l’ordre prévu.\nNous avons donc réussi à réaliser une transaction multi-partition sans utiliser de protocole de type atomic commit, en cassant la transaction en plusieurs morceaux s’exécutant chacun sur leur partition, et en ayant un mécanisme end-to-end (ici l’id) assurant l’intégrité du tout (le fait qu’un bout ne sera pas exécuté 2 fois).\n\n\n\n\n\n\n\n\nA propos de l’intégrité et de la relation au temps :\nLe terme consistency englobe en réalité deux enjeux :\nLa timeliness qui consiste à s’assurer que l’observateur voit une donnée à jour. C’est tout l’objet du terme eventual consistency quand la timeliness n’est pas respectée.\nL’integrity qui consiste à préserver les données d’une corruption permanente des données, y compris dérivées. Pour la régler il faudrait réparer et non pas juste attendre ou réessayer.\nSi le non-respect de la timeliness est embêtant, le non-respect de l’integrity peut être catastrophique.\n\n\nAlors que dans les transactions ACID la timeliness et l’integrity sont confondues, on vient de voir que dans le stream processing on peut les décorréler, et arriver à garantir l’integrity tout en ayant un fonctionnement asynchrone et donc ne garantissant pas la timeliness. Et le tout sans utiliser les transactions distribuées coûteuses;\nSelon l'auteur, cette technique est particulièrement prometteuse.\n\n\nOn peut aussi se demander si toutes les applications ont vraiment besoin d’un respect intransigeant de la timeliness, et donc d’un respect de la linearizability (dès qu’une écriture est faite, elle impacte les lecteurs) :\nOn peut très bien faire une compensating transaction dans le cas où on a accepté une transaction côté client mais qu’il se révèle qu’elle ne respecte pas les contraintes.\nD’ailleurs un processus d’excuse et de compensation peut être pertinent dans de nombreux cas, par exemple pour la réservation, souvent on propose plus de places que disponibles en partant du principe qu’il y aura des désistations. Et dans le cas où on a mal prévu, il faut pouvoir avoir un processus de compensation.\n\n\nOn peut créer un système qui pour l’essentiel évite la coordination :\n1 - on préserve l’intégrité des données dans les systèmes dérivés sans atomic commit, linearizability, ou coordination synchrone entre partitions.\n2 - la plupart des applications peuvent se passer de contraintes temporelles fortes pour la timeliness.\nSelon l'auteur, ce type de système sans coordination a beaucoup d’avantages. On peut très bien utiliser la coordination synchrone pour certaines opérations importantes qui ne permettent pas de retour en arrière, et garder le reste sans cette coordination.\nFinalement on peut voir la chose de cette manière : avoir de fortes garanties synchrones du type transactions distribuées réduit le nombre d’excuses qu’il faudra faire pour les données inconsistantes présentées, mais ne pas les utiliser réduit le nombre d’excuses qu’il faudra faire pour toutes les indisponibilités du système dues à la faible performance induite par la coordination.\n\n\n\n\nVis-à-vis des erreurs matérielles et logicielles :\nIl y a des corruptions probables contre lesquelles notre système model prévoit des parades, et des corruptions contre lesquelles non, comme par exemple faire confiance aux opérations du CPU. Pourtant tout peut arriver avec plus ou moins de probabilité.\nIl ne faut pas oublier non plus que les BDD ne sont que des logiciels qui peuvent avoir des bugs, et pour nos codes applicatifs c’est encore pire.\nLa corruption des données finit par arriver qu’on le veuille ou non. Il faut une forme d'auto-auditabilité. Il faut vérifier régulièrement que nos données sont bien là et intègres, de même que nos backups.\nL’approche représentée par l’event sourcing permet d’auditer plus facilement les données.\nEt si on a bien fait la séparation entre les données sources et dérivées c’est encore plus clair.\nOn peut faire un checksum sur le log d’events pour le vérifier, et on peut rejouer les batchs pour recréer des données dérivées propres.\n\n\nUne bonne pratique dans la vérification des données est de le faire sur des flows end-to-end. Cela permet d’inclure tout le hardware et le software dans ce qui est vraiment vérifié.\nLes techniques cryptographiques de vérification de l’intégrité introduites par la blockchain est un mécanisme très intéressant pour l’avenir de l’intégrité des données."}},"/notes/get-your-hands-dirty-on-clean-architecture":{"title":"Get Your Hands Dirty on Clean Architecture","data":{"":"","1---whats-wrong-with-layers#1 - What’s Wrong With Layers?":"La layered architecture est tout à fait classique : 3 couches successives (web -> domain -> persistance).\nElle peut même permettre une bonne architecture qui laisse les options ouvertes (par exemple remplacer la persistance en ne touchant que cette couche-là).\n\n\nLe problème de la layered architecture c’est qu’elle se détériore rapidement et encourage les mauvaises habitudes.\nElle promeut le database-driven design : vu que la persistance est à la base, on part toujours depuis la modélisation de la structure de la DB.\nAu lieu de ça on devrait mettre au centre le comportement, c’est-à-dire le code métier, et considérer la persistance comme périphérique.\nUn des éléments qui pousse au database-driven design aussi c’est l’ORM, qui peut être utilisé depuis le layer domain, et introduit des considérations techniques dedans.\n\n\nElle encourage les raccourcis : quand un layer du dessus a besoin d’un élément du dessous, il suffit de le pousser vers le bas et il y aura accès.\nLa couche de persistance finit par grossir et devenir une énorme couche “utilitaire” qui contient la logique et les aspects techniques entremêlés.\n\n\nElle devient de plus en plus difficile à tester :\nLe logique métier a tendance à fuiter vers la couche web, parce que la persistance y est directement utilisée.\nTester la couche web devient de plus en plus difficile parce qu’il faut mocker les deux autres, et parce que la logique y grossit.\n\n\nElle cache les use cases : la logique fuitant vers les autres layers, on ne sait pas où ajouter un nouveau use-case, ni où chercher un existant.\nOn peut ajouter à ça que vu qu’il n’y a pas de limite, la couche domain a des unités (services) qui grossissent au fil du temps, ce qui rend plus difficile de trouver où va chaque fonctionnalité.\n\n\nElle rend le travail en parallèle difficile :\nComme on fait du database-driven design, on doit toujours commencer par la couche de persistance, et on ne peut pas être plusieurs à la toucher.\nSi en plus les services du domaine sont gros, on peut être en difficulté pour être plusieurs à modifier un gros service pour plusieurs raisons.","2---inverting-dependencies#2 - Inverting dependencies":"Le Single Responsibility Principle (SRP) dit qu’un composant doit avoir une seule** raison de changer**.\nÇa veut dire que si on change le logiciel pour n’importe quelle autre raison, il ne devrait pas changer.\nQuand nos composants dépendent les uns des autres, ça leur donne autant de raisons à chaque fois de changer, si un des composants dont ils dépendent change lui aussi.\n\n\nLa layered architecture fait que la couche web et la couche domain ont des raisons de changer liées à la couche de persistance. On n’a pas envie que la couche domain change pour d’autres raisons qu’elle-même, donc on va inverser les dépendances qu’elle a.\nC’est le Dependency Inversion Principle (DIP).\nOn va copier les entities depuis la persistance vers la couche domain qui en a besoin aussi.\nEt on va créer une interface de persistance dans le domaine, à laquelle va adhérer la couche persistance qui aura donc une dépendance vers le domaine plutôt que l’inverse.\n\n\nSelon Robert Martin la clean architecture doit garder le domaine séparé du reste (frameworks, infrastructure etc.), et les dépendances doivent être tournées vers le code du domaine pour que celui-ci n’en ait pas d’autres que lui-même.\nLes entities du domaine sont au centre.\nIls sont utilisés par les use cases, qui représentent les services, mais impliquent d’avoir une granularité fine.\nCette séparation a un coût, qui est qu’il faut dupliquer les entités entre le domaine et l’infrastructure, notamment pour éviter que les entités du domaine soient polluées par la technique.\n\n\nL’hexagonal architecture est similaire à la clean architecture mais un peu moins “abstraite”, c’est la version d’Alistair Cockburn.\nL’hexagone contient les entities et les use cases, et en dehors on trouve des adapters pour intégrer la communication avec l’extérieur.\nOn a deux types d’adapters :\nLes adapteurs de gauche drivent l’hexagone, parce qu’ils appellent des fonctions exposées par l’hexagone.\nExemple : handlers HTTP.\n\n\nLes adapters de droite sont drivées par l’hexagone, parce que l’hexagone appelle des méthodes sur eux.\nExemple : communication avec la DB.\n\n\n\n\nPour permettre la communication, l’hexagone définit des ports (interfaces), qui doivent être implémentés par les adapters.\nC’est pour ça qu’on parle de Ports & Adapters.\n\n\n\n\nQuel que soit leur nom, tout l’intérêt de ces architectures c’est de permettre d’avoir un domaine isolé, dont on pourra gérer la complexité sans qu’il ait d’autres raisons de changer que lui-même.","3---organizing-code#3 - Organizing Code":"On peut organiser le code par couches : le classique web, domain et persistance, mais avec une inversion de la persistance vers le domain.\n- web\n- AccountController\n- domain\n- Account\n- AccountService\n- AccountRepositoryPort\n- persistance\n- AccountRepositoryImpl\n\nMais cette organisation est sous-optimale pour 3 raisons :\nIl n’y a pas de séparation sous forme de dossiers ou de packages pour les fonctionnalités. Donc elles vont vite s’entre-mêler au sein de chaque couche.\nComme les services sont gros, on peut difficilement repérer la fonctionnalité exacte qu’on cherche tout de suite.\nOn ne voit pas au premier coup d'œil quelle partie de la persistance implémente quel port côté domain. L’architecture ne saute pas aux yeux.\n\n\n\n\nOn peut ensuite organiser le code par features : les limites de dossier/package sont définies par les features qui contiennent un fichier par couche.\n- account\n- Account\n- AccountController\n- AccountRepository\n- AccountRepositoryImpl\n- SendMoneyService\n\nOn a nos features visibles immédiatement (Account -> SendMoneyService), ce qui fait qu’on est dans le cadre d’une screaming architecture.\nPar contre, nos couches techniques sont très peu protégées, et le code du domaine n’est plus protégé du reste par des séparations fortes.\n\n\nOn peut enfin organiser le code dans une architecture expressive, reprenant le meilleur des deux autres :\nUne séparation initiale par features majeures.\nPuis une séparation par couches à l’intérieur de ces features majeures.\nEt enfin la séparation explicite des ports et adapters, en explicitant leur nature entrante ou sortante.\n- account\n- adapter\n- in\n- web\n- AccountController\n- out\n- persistance\n- AccountPersistanceAdapter\n- domain\n- Account\n- application\n- SendMoneyService\n- port\n- in\n- SendMoneyUseCase\n- out\n- LoadAccountPort\n- UpdateAccountStatePort\n\nLe fait que l’architecture soit alignée avec la structure en packages fait que nous avons moins de chances d’en dévier. Elle est incarnée de manière très concrète dans le code.\nLe domaine étant isolé, on peut très bien en faire ce qu’on veut, y compris y appliquer les patterns tactiques du DDD.\nCôté visibilité des packages :\nLes adapters peuvent rester privés, puisqu’ils ne sont appelés qu’à travers les ports.\nLes ports doivent être publics pour être accessibles par les adapters.\nLes objets du domaine doivent être publics pour être accessibles depuis les services et les adapters.\nLes services peuvent rester privés parce qu’ils sont appelés à travers les ports primaires.\n\n\n\n\nConcernant la manière dont fonctionne l’inversion de dépendance ici :\nPour les adpaters entrants il n’y a pas besoin d’inversion puisqu’ils sont déjà entrants vers l’hexagone. On peut, au besoin, quand même protéger l’hexagone derrière des ports quand même.\nPour les adapters sortants par contre il faut inverser la dépendance, en les faisant respecter le port de l’hexagone, puis en les instanciant et les donnant à l’hexagone.\nIl faut donc un composant tiers neutre qui instancie les adapters sortants pour les donner à l’hexagone, et instancie l’hexagone pour le donner aux adapters entrants.\nIl s’agit de l’injection de dépendance.","4---implementing-a-use-case#4 - Implementing a Use Case":"Comme on a une forte séparation hexagone/adapters, on peut implémenter l’hexagone de la manière dont on veut, y compris avec les patterns tactiques du DDD, mais pas forcément.\nDans ce chapitre on implémente un use-case dans l’hexagone de l’exemple buckpal qui est une application de gestion de paiement.\nLa couche domain se trouve dans buckpal -> domain, et contient une entité Account, qui a des méthodes pour ajouter et retirer de l’argent.\nChaque ajout ou retrait se fait en empilant des entités Activity qui contiennent chaque transaction dans un tableau interne à Account.\nLe tableau interne ne contient qu’une fenêtre d’Activity pour des raisons de performance, et une variable permet de connaître la valeur du compte avant ce nombre restreint d’Activity.\n\n\nLes use-cases se trouvent dans la couche applicative, dans buckpal -> application -> service.\nUn use-case va :\nRécupérer l’input (qu’il ne valide pas par lui-même pour laisser cette responsabilité à un autre composant).\nValider les règles business, dont la responsabilité est partagée avec la couche domain.\nManipuler l’état du domaine :\nÇa se fait en instanciant des entités et appelant des méthodes sur elles.\nEt en général en passant les entités à un adapter pour que leur état soit persisté.\nAppeler éventuellement d’autres use-cases.\n\n\nRetourner l’output.\n\n\nLes use-cases vont être petits pour éviter le problème de gros services où on ne sait pas quelle fonctionnalité va où.\n\n\nLa validation des inputs se fait dans la couche applicative pour permettre d’appeler l’hexagone depuis plusieurs controllers sans qu’ils aient à valider ces données, et pour garantir l’intégrité des données dans l’hexagone.\nOn va faire la validation dans une classe de type command. Cette classe valide les données dans son constructeur, et refuse d’être instanciée si les données sont invalides.\nL’auteur déconseille d’utiliser le pattern builder et conseille d’appeler directement le constructeur.\nExemple de builder :\nnew CommandBuilder().setParameterA(value1).setParameterB(value2).build();\n\n\n\nCette classe va se trouver dans buckpal -> application -> port -> in.\nElle constitue une sorte d’anti-corruption layer protégeant l’hexagone.\n\n\nOn pourrait être tenté de réutiliser des classes de validation d’input entre plusieurs use-cases ressemblants, par exemple la création d’un compte et la modification d’un compte. L’auteur le déconseille.\nSi on réutilise, on va se retrouver avec quelques différences (par exemple l’ID du compte) qui vont introduire de potentielles mauvaises données.\nOn va se retrouver à gérer les différences entre les deux modèles dans les use-cases alors qu’on voulait le faire dans un objet à part.\nGlobalement, faire des modèles de validation d’input permet, au même titre que le fait de faire des petits use-cases, de garder l’architecture maintenable dans le temps.\n\n\nLa validation des business rules doit quant à elle être faite dans les use-cases.\nLa différence avec la validation des inputs c’est que pour les inputs il n’y a pas besoin d’accéder à l’état du modèle de données, alors que pour les business rules oui.\nLe use-case peut le faire directement en appelant des fonctions, ou alors le déléguer à des entities.\nDans le cas où l'essentiel de la logique est fait dans les entities et où le use-case orchestre juste des appels et passe les données, on parle d’un rich domain model. Dans le cas où c’est le use-case qui a l’essentiel de la logique et les entities sont maigres, on parlera d’un anemic domain model.\n\n\nLe use-case lève une exception en cas de non-respect des règles business comme pour les règles d’input. Ce sera à l’adapter entrant de décider de ce qu’il fait de ces exceptions.\n\n\nConcernant l’output renvoyé par le use-case, il faut qu’il soit le plus minimal possible : n’inclure que ce dont l’appelant a besoin.\nSi on retourne beaucoup de choses, on risque de voir des use-cases couplés entre eux via l’output (quand on ajoute un champ à l’objet retourné, on a besoin de changer tous ceux qui le retournent).\n\n\nLes use-cases qui font uniquement de la lecture pour renvoyer de la donnée peuvent être distinguées des use-cases qui écrivent.\nPour ça on peut les faire implémenter un autre port entrant que les use-cases d’écriture, par exemple le port GetAccountBalanceQuery.\nOn pourra à partir de là faire du CQS ou du CQRS."}},"/notes/learning-domain-driven-design":{"title":"Learning Domain-Driven Design","data":{"":"","introduction#Introduction":"Le problème principal qui met en échec la plupart des projets de dev c’est la communication. Et c’est le cœur de ce qu’est le DDD.\nLe but du DDD c’est d’aligner le Design logiciel avec le Domaine business.\nLe DDD se décompose en 2 parties :\nLe design stratégique : créer une compréhension commune du domaine, et prendre des décisions haut niveau sur le projet.\nC’est les questions “Quel logiciel on crée ?” et “Pourquoi on le crée ?”\n\n\nLe design tactique : écrire du code qui épouse le domaine.\nC’est la question “Comment on crée chaque partie du logiciel ?”","part-i--strategic-design#Part I : Strategic Design":"","1---analyzing-business-domains#1 - Analyzing Business Domains":"Pour développer un bon logiciel qui réponde au besoin de notre entreprise, il faut que nous les devs connaissions la stratégie du business : différencier les parties importantes des parties moins importantes pour adapter nos techniques.\nLe business domain est le domaine d’activité principal de l’entreprise.\nExemple : FedEx fait de la livraison.\nPour une grande entreprise qui fait plusieurs choses très différentes (comme Google ou Amazon), on peut parler de plusieurs domains.\n\n\nUn subdomain est une activité que l’entreprise fait pour mener à bien son business.\nExemple : Starbucks fait des cafés, mais il doit aussi recruter, gérer la tréso etc.\nIl existe 3 types de subdomains :\nLes core subdomains sont les seules activités qui donnent à l’entreprise un avantage concurrentiel par rapport à ses concurrents. Soit par une innovation produit, soit par une optimisation qui permet de produire à moindre coût.\nC’est une activité par nature complexe étant donné qu’il faut qu’elle ne soit pas facilement reproductible par les concurrents.\nLe statut de “core” est aussi par nature temporaire : dès que les concurrents rattrappent ce n’est plus core.\nA noter aussi qu’un core subdomain peut très bien ne pas résider dans du logiciel. Par ex une bijouterie va vendre mieux que les concurrents grâce au style donné par les artisans bijoutiers (qui donne l’avantage compétitif et qui est donc le core subdomain), la boutique en ligne dans ce cas est un generic subdomain.\n\n\nLes generic subdomains sont des problèmes communs déjà résolus et largement disponibles et utilisés par les concurrents (soit en open source, soit sous forme de service payant).\nIls sont complexes comme les core subdomains, mais ne donnent juste pas d’avantage compétitif.\nExemple : un mécanisme d’authentification est complexe, mais des solutions open source et payantes existent, et personne ne recode son système à soi.\n\n\nLes supporting subdomains sont des activités simples mais nécessaires et dont on n’a pas de solution générique à réutiliser (ou alors ça coûte moins cher de le faire soi-même) : c’est du ETL (extract, load, transform), ou encore CRUD.\nVu la simplicité, ils ne peuvent pas procurer d’avantage compétitif.\nVu qu’il n’y a pas d’avantage compétitif, on préfère appliquer l’effort sur les core subdomains qui apporteront plus de valeur business.\n\n\n\n\nCôté stratégie :\nLes core subdomains doivent être développés au sein de l’entreprise, par les développeurs les plus expérimentés, et en appliquant le maximum de qualité et les techniques d’ingénierie les plus avancées.\nOn ne peut pas les acheter sinon on perd la notion d’avantage compétitif, et il ne serait pas très malin de les sous-traiter.\n\n\nLes generic subdomains étant difficiles mais déjà résolus, il est plus rentable de les acheter (ou de faire appel aux services d’un consultant spécialisé), ou d’utiliser une solution open source.\nLes supporting subdomains sont simples et changent peu, donc ils peuvent être implémentés avec moins de qualité, ou moins de techniques sophistiquées (design patterns, techniques d’architecture etc.). Un simple framework rapide suffit.\nOn peut laisser les débutants se charger de ça, ou alors on peut le sous-traiter.\n\n\n\n\n\n\nPour trouver les subdomains, il va falloir faire l’analyse nous-mêmes.\nOn peut déjà partir des départements qui composent l’entreprise, mais ça nous donne des subdomains grossiers.\nOn peut alors “distiller” les subdomains en plus petits subdomains : on prend un département et on liste les sous-activités, puis on analyse pour chacune d’entre elles si elles sont core, generic ou support.\nOn peut distiller au maximum jusqu’à arriver à “un subdomain comme un ensemble cohérent de cas d’usages” : un même acteur qui fait plusieurs tâches précises.\nIl faut faire la distillation maximale pour les subdomains core, pour pouvoir éliminer les petits bouts génériques ou support à l’intérieur et se concentrer uniquement sur ce qui a le plus de valeur.\nPour les autres on s’arrête de distiller à partir du moment où une distillation donne des activités du même type (generic ou support), aller au-delà ne nous permettra de toute façon pas de prendre des décisions plus stratégiques.\n\n\n\n\nLes devs vont devoir collaborer avec les domain experts, mais qui sont-ils ?\nCe ne sont pas les analystes qui recueillent le besoin, ni les ingénieurs qui créent le système. Ces derniers transforment le modèle mental des domain experts pour en faire du logiciel.\nCe sont soit ceux qui arrivent avec les besoins (qui sont là depuis le début, qui ont créé l’activité etc.), soit les utilisateurs finaux dont on résout le problème.\nA noter que les domain experts peuvent très bien n’être experts que d’un sous-domaine seulement.","2---discovering-domain-knowledge#2 - Discovering Domain Knowledge":"Généralement, les gens du business (les domain experts) communiquent les besoins à des intermédiaires (system/business analysts, product owners, project managers), qui vont ensuite communiquer ça aux ingénieurs logiciel qui créent le logiciel.\ndomain experts -> gens au milieu -> software engineers\nOn assiste aussi à une ou plusieurs transformations :\ndomain knowledge -> analysis model -> software design model\n\n\nLe DDD propose d’arrêter les transformations, et que tous les acteurs utilisent le même langage pour se parler : l’Ubiquitous Language.\nIl doit pouvoir être compris par les domain experts, donc il va se baser sur les termes qu’ils utilisent déjà.\nChaque terme doit être précis et sans ambiguïté, et il ne doit pas y avoir de synonymes.\nLa raison est que si dans les conversations courantes on arrive à se comprendre même avec des ambiguïtés en fonction du contexte, dans le cadre du logiciel ça marche moins bien.\n\n\nLe but de l’ubiquitous language est d’être un modèle du business domain, qui va représenter le modèle mental des domain experts.\nUn modèle n’a pas à représenter toute la réalité mais seulement une partie de manière à être utile. Et donc ici aussi il ne s’agit pas de représenter tout le business domain, ni de faire de tout le monde un domain expert.\n\n\nCôté outils :\nIl faut un glossaire sous forme de wiki, que tous font évoluer au fur et à mesure. On y met les termes de l’ubiquitous language.\nPour la logique il faut d’autres outils comme des description de cas d’usage, ou des tests Gherkin.\nExemple Gherkin :\nScenario: Notifier l'agent d'un nouveau cas\nGiven: Jules qui crée un nouveau cas \"Nouveau cas\"\nWhen: Le ticket est assigné à M. Wolf\nThen: L'agent reçoit une notification\n\nCeci dit, le plus important est l’usage quotidien de l’ubiquitous language par les acteurs. Les outils ne sont qu’un plus.\n\n\n\n\nPour que la compréhension du business domain soit bonne, il faut une communication régulière entre les domain experts et ceux qui sont en charge de réaliser le logiciel.\nCette communication se fait bien sûr dans l’ubiquitous language.\nÇa implique de poser beaucoup de questions aux domain experts.\nA force, on va se rendre compte qu’on pourra aider les domain experts à mieux comprendre leur champ d’expertise sur certains points, par exemple en pensant à des edge cases et pas seulement aux “happy paths”.\nOn peut alors aller vers une co-construction de la compréhension du domaine (même si les experts en sauront toujours plus).\n\n\n\n\nA propos de la langue de l’ubiquitous language, le conseil de Vlad est d’utiliser l’anglais au moins pour les noms d’entités du business domain.","3---managing-domain-complexity#3 - Managing Domain Complexity":"On se rend parfois compte qu’un même mot est utilisé par plusieurs domain experts avec une signification différente.\nExemple : le mot lead peut désigner un simple prospect intéressé dans le département marketing, alors que ça peut désigner une entité plus complexe avec des histoires de process de vente dans le département sales.\n\n\nEn général on a naturellement tendance à agrandir notre modèle pour que chaque version du mot puisse être représentée, mais ça finit par donner un modèle très complexe et difficile à maintenir.\nLa solution DDD est de créer deux bounded contexts qui auront chacun leur ubiquitous language.\n\n\nUn bounded context doit avoir des limites (boundaries).\nUne manière de les trouver est de repérer les incohérences de terminologies utilisées par les domain experts : un bounded context ne pourra pas être plus grand que ce que ces incohérences permettent.\nMais on peut diviser ces ensembles en bounded contexts plus petits. Il s’agit là d’une décision stratégique, qui dépend du contexte.\nAvoir des bounded context trop grands impliquera une plus grande complexité du modèle, alors que s’ils sont trop petits on aura un plus grand effort d’intégration à faire.\nLes raisons qui peuvent pousser à découper en bounded contexts plus petits peuvent être :\nd’ordre organisationnels : par exemple une nouvelle équipe qui va prendre en charge un morceau du logiciel qui va se développer sur un rythme différent du reste.\nd’ordre non fonctionnels : par exemple le fait de devoir scaler certaines parties du logiciel indépendamment.\n\n\nDe manière générale, on va essayer de trouver des fonctionnalités cohérentes qui opèrent sur un même jeu de données, et les laisser dans un même bounded context.\n\n\n\n\nLa différence entre subdomains et bounded contexts c’est que les subdomains sont là de fait et ressortent avec l’analyse. Ils sont de la responsabilité du business. Alors que les bounded contexts sont le résultat d’un choix stratégique, et sont de la responsabilité des ingénieurs logiciel.\nChaque bounded context a une séparation physique : il devra être implémenté avec sa codebase, en tant que projet autonome, et éventuellement service autonome.\nDans le cas où il s’agit d’un service autonome, on peut aussi choisir le langage, la stack, indépendants des autres.\n\n\nUn bounded context ne doit être géré que par une seule équipe de dev.\nUne équipe peut par contre en avoir plusieurs en charge.\n\n\nSi on se tourne vers la vie de tous les jours, on peut remarquer plein d’exemples où des mots ont des significations différentes selon les contextes, et où un modèle différent serait utile pour chaque :\nUn carton rectangulaire qu’on met au sol permet de représenter un réfrigérateur pour savoir s'il va bien s’insérer. Ce modèle est partiel mais il est utile pour ce qu’on veut faire, et c’est tout ce qui compte.\nEt si on a besoin de vérifier aussi la hauteur, on peut utiliser un autre modèle du réfrigérateur qui suffit : un simple mètre. => On a là une vision de l’approche DDD des modèles.","4---integrating-bounded-contexts#4 - Integrating Bounded Contexts":"Chaque bounded context permet de représenter un modèle au sens DDD, c'est-à-dire une abstraction utile pour résoudre un problème particulier.\nLes bounded contexts doivent quand même avoir des points de contact pour que le logiciel forme un tout, ces points sont appelés contracts.\nOn parle ici de points de contact au niveau du modèle (terminologie, concepts), mais le modèle se traduit à chaque fois en implémentation aussi.\n\n\nLa nature de ces contracts dépend de la nature de la relation entre les équipes qui gèrent les bounded contexts, il y en a de 3 types :\nCooperation : il s’agit soit de la même équipe, soit d’équipes qui ont une bonne communication et collaboration (par ex le succès de l’une dépend du succès de l’autre).\nPartnership : aucune équipe ne dicte sa loi à l’autre. Elles collaborent pour faire des changements dès qu’il y en a besoin d’un côté ou de l’autre.\nIl faut une intégration continue pour que les problèmes soient vite résolus.\nIl faut une communication vraiment au top entre équipes, et pas d'histoires de rivalités.\n\n\nShared Kernel : il s’agit d’un cas à part où les boundaries des bounded contexts sont violés : deux bounded contexts partagent une partie de leur modèle.\nPar exemple, ça peut être la partie d’authentification maison qu’ils auront en commun.\nLa partie partagée peut être changée par chaque bounded context, et affectera l’autre immédiatement. Il faut donc que des tests d’intégration soient déclenchés à chaque changement.\nBien réfléchir avant de l’utiliser : on l’utilise quand le coût pour appliquer les changements du modèle sous forme d’implémentation dépasse le coût de coordination entre équipes. C’est donc pertinent pour des modèles qui changent beaucoup.\nCas d’usage courants :\nQuand deux équipes n’ont pas une assez bonne communication, et que le modèle partnership donnerait lieu à des problèmes d’intégration.\nQuand on a un système legacy qu’on essaye de découper en bounded contexts, la partie pas encore découpée peut être temporairement partagée par les autres bounded contexts.\nQuand une même équipe a deux bounded contexts en charge, il est possible qu’avec le temps les frontières de ces deux bounded contexts s’estompent. Pour éviter ça on peut introduire une partie partagée qui va bien définir les frontières des deux autres.\n\n\n\n\n\n\nCustomer-Supplier : contrairement au modèle de type cooperation, la réussite de chaque équipe en charge de chaque bounded context n’est pas liée. On a donc un rapport de force qui va pencher vers l’une ou l’autre partie (le customer qui consomme ou le supplier qui fournit).\nConformist : la provider n’a pas vraiment de motivation à satisfaire le customer. Soit parce que c’est un provider externe, soit il est interne et c’est la politique de l’entreprise qui fait ça. Le customer va donc se conformer au modèle et aux changements de modèle du provider.\nLe supplier le fait soit parce que c’est un standard bien établi, soit simplement parce que ça lui convient.\n\n\nAnticorruption Layer : le provider a toujours le pouvoir, mais cette fois le consumer ne va pas se conformer, il va vouloir se protéger par une couche d’abstraction entre ce qui lui arrive et son propre modèle.\nLe consumer peut vouloir cette couche quand :\nIl contient un core subdomain, et que le modèle du provider pourrait être gênant.\nLe modèle du provider est bordélique, et on veut s’en protéger.\nLe modèle du provider change souvent, et on veut s’en protéger.\n\n\nGrâce à cette couche filtrante, le modèle du consumer n’aura pas à être pollué par des concepts dont il n’a pas besoin.\n\n\nOpen-Host Service : cette fois c’est le consumer qui a l’avantage parce que le provider a une motivation à satisfaire le consumer. Le provider va donc mettre en place une couche d’abstraction de son côté pour découpler son modèle interne du modèle public qu’il expose.\nC’est comme l’anticorruption layer, mais côté provider.\nLe provider peut éventuellement maintenir plusieurs OHS le temps que les customers migrent au nouveau modèle public.\n\n\n\n\nSeparate ways : les deux bounded contexts n'interagissent pas du tout. Si elles ont des fonctionnalités communes, elles les dupliquent de leur côté.\nÇa peut être quand la communication entre équipes est impossible.\nOu encore quand ça coûte plus cher de partager une fonctionnalité que de la refaire chacun de son côté. Parce que la fonctionnalité est trop simple, ou que les deux modèles sont trop différents.\n\n\n\n\nLe context map est une représentation graphique de chaque bounded context, avec les relations que chacun entretient avec les autres.\nÇa peut permettre de faire des choix stratégiques haut niveau.\nÇa peut faire apparaître des problèmes organisationnels :\nPar exemple si une équipe provider a tous ses consumers qui mettent en place un anticorruption layer. Ou encore si une équipe n’a que des relations separate ways avec les autres.\n\n\nLe context map doit être maintenu par l’ensemble des équipes.\nOn peut utiliser des outils comme Context Mapper, qui consiste à avoir un fichier source dans un dépôt de code, et permet de visualiser le graphique résultant avec une extension VSCode.","part-ii--tactical-design#Part II : Tactical Design":"","5---implementing-simple-business-logic#5 - Implementing Simple Business Logic":"Ce chapitre présente deux patterns tactiques simples. Ce sont des patterns connus, décrits notamment dans Patterns of Enterprise Application Architecture de Martin Fowler.\n1 - Transaction Script : il s’agit pour le provider de fournir une interface publique avec des endpoints qui ont un comportement transactionnel : soit une requête réussit entièrement, soit elle est entièrement annulée.\nMartin Fowler écrit à propos de ce pattern : Organizes business logic by procedures where each procedure handles a single request from the presentation.\nNotre fonction peut accéder à la base de données pour faire des manipulations, soit à travers une mince couche d’abstraction, soit directement. (Donc forte dépendance avec la BDD).\nDans le cas simple où on a une BDD relationnelle, il s’agit simplement d’utiliser la fonctionnalité native de transaction de sa BDD : on crée la transaction au début de la fonction, et on commit ou rollback à la fin.\nLa chose se complique quand notre BDD ne supporte pas les transactions, qu’on doit mettre à jour des données dans plusieurs BDD, ou encore qu’on est dans un contexte distribué où les transactions ne sont pas effectives immédiatement (eventual consistency).\n\n\nPotentiels problèmes qu’on rencontre souvent :\nOubli de transaction : parfois on oublie tout simplement d’utiliser le mécanisme de transaction au début de notre fonction. En exécutant plusieurs écritures en BDD on peut avoir la 1ère qui réussit et la 2ème qui échoue. On se retrouve alors potentiellement dans un système incohérent.\nSystème distribué : souvent dans les systèmes distribués on met à jour une donnée, et on prévient d’autres serveurs en leur envoyant un message. Il suffit que l’envoi de message ne marche pas pour que le système devienne incohérent parce qu’une partie du travail ne sera pas faite.\nIl n’y a pas de solution simple à ce problème, mais de nombreuses solutions qui font chacune du mieux qu’elles peuvent, en fonction du contexte.\n(pas dit dans le livre, mais c’est l’objet de Designing Data-Intensive Applications de Martin Kleppmann)\n\n\n\n\nTransaction Script est le pattern le plus simple, et convient pour les opérations simples de type ETL (extract, transform, load) où la logique business est elle-même simple et change peu souvent.\nIl convient bien :\nDans les supporting subdomains.\nComme couche d’adapter pour intégrer un generic subdomain.\nDans un anticorruption layer vis-à-vis d’un autre bounded context.\n\n\nPar contre sa simplicité fait que plus la logique business est complexe, plus il y a de la duplication de logique entre les diverses fonctions, et donc un risque d’incohérence au fil du temps. Donc il ne conviendra pas aux core subdomains.\n\n\nTous les autres patterns plus avancés, sont d’une manière ou d’une autre basés sur Transaction Script.\n\n\n2 - Active Record : on garde les caractéristiques transactionnelles du Transaction Script, mais on va ajouter des objets (les fameux Active Records) qui vont être en charge de transformer les données depuis la BDD vers des structures en mémoire qui conviennent mieux à ce qu’on veut faire dans nos fonctions.\nMartin Fowler écrit à propos de ce pattern : An object that wraps a row in a database table or view, encapsulates the database access, and the domain adds domain logic on that data.\nLa conséquence est que les fonctions appelées par les endpoints ne vont plus faire d’appel à la BDD, mais vont passer uniquement par les Active Records pour manipuler les données.\nL’Active Record va fournir des méthodes pour accéder aux données et les modifier, et va faire la traduction à chaque fois.\nOn encapsule la complexité de la transformation des données pour éviter de dupliquer cette logique-là un peu partout.\n\n\nActive Record est utile pour les cas d’usage simples, mais qui impliquent des transformations de données potentiellement complexes.\nIl convient donc lui aussi seulement aux supporting subdomains, ou à des couches d’adapter pour intégrer un generic subdomain ou un autre bounded context.\nQuand il est utilisé pour les logiques business complexes comme pour les core subdomains, on parle parfois d’un anemic domain model antipattern (et c’est déconseillé).\n\n\n\n\nIl s’agit cependant de rester pragmatique : bien qu’il soit souhaitable de protéger les données, il faut se demander à chaque fois quelles seraient les implications de la corruption de données dans tel ou tel projet.\nPar exemple, si on ingère des millions d’entrées venant de devices IOT dans notre base, risquer d’en corrompre 0.001% peut ne pas forcément être très grave.","6---tackling-complex-business-logic#6 - Tackling Complex Business Logic":"Ce chapitre présente le pattern du DDD pour les logiques business compliquées (à utiliser dans les core subdomains donc) : le domain model pattern.\nCe pattern est aussi dans le bouquin de Fowler, mais il a été développé plus en détail dans celui d’Eric Evans : Domain Driven Design: Tackling Complexity in the Heart of Software, sorti un an après (2003).\nAu niveau de l’implémentation :\nLe code du domain model est là pour représenter le domaine qui est déjà complexe, donc il ne doit pas introduire de complexité supplémentaire liée aux technos ou à l’infrastructure. => on utilisera donc du code pur (par exemple du TypeScript pur), pas de framework, d’ORM ou autre chose d’extérieur dans ce code.\nDébarrassé de tout ce qui est lié aux technos, le code peut utiliser l’ubiquitous language et épouser le modèle mental des domain experts.\nA noter que ce livre présente les choses d’un point de vue orienté objet, mais une approche fonctionnelle serait tout aussi valide, d’autres ressources dans la communauté du DDD vont dans ce sens.\n\n\nLes blocs qui composent le domain model sont :\n1- Value objects :\nCe sont des objets immutables identifiés par les valeurs de chacune de leurs propriétés. Si on veut le même objet avec une valeur différente pour une de ses propriétés, on crée de fait un autre objet.\nPar exemple, dans Python ou JavaScript, le string est un value object : on ne peut pas modifier une valeur string en elle-même, mais on peut créer de nouveaux strings avec des valeurs différentes (et qui auront bien une référence en mémoire différente).\nLes value objects n’ont pas d’attribut ID, puisque leur identification se fait par la composition des valeurs de leurs propriétés qui les rend uniques.\nEn revanche, il faut implémenter un opérateur d’égalité pour pouvoir comparer deux value objects entre eux.\n\n\n\n\nLe value object sera utilisé comme type pour les données à la place des types primitifs (comme string, number etc).\nL’utilisation systématique de types primitifs est connue comme “the primitive obsession”.\nPar exemple, plutôt que d’avoir email: string, on peut avoir email: Email, avec le value object Email qui va faire la validation et refuser toute valeur qui ne correspond pas.\nÇa évite de dupliquer la logique de validation partout, mais aussi d’oublier de faire cette validation à certains endroits.\n\n\nEn plus de la simple validation de données, le value object peut encapsuler en un endroit unique du code business lié au concept précis qu’il décrit. (Et c’est là qu’il “brille” vraiment)\nConcept qui est normalement dans le lexique de l’ubiquitous language de notre bounded context.\nPar exemple, un value object représentant une couleur peut avoir une méthode pour mixer la couleur avec une autre, et en obtenir une 3ème, selon des règles business bien particulières.\n\n\nEt ça augmente la clarté du code en rendant plus explicite l’intention.\n\n\nOn utilise les value objects dès que possible. Typiquement comme type des attributs des entities.\n\n\n2.1- Entities :\nCe sont des objets dont les propriétés peuvent changer (de type value object ou de type primitif), identifiés par leur ID qui lui ne change pas.\nExemple d’entity : une personne représentée par son nom, son email etc.\nLes entities sont importants, mais ils n’ont vraiment d'existence que dans le cadre d’un aggregate.\n\n\n2.2 - Aggregates :\nUn aggregate est une entity qui en contient d’autres.\nOn choisit un aggregate root qui est l’entity principale, et contient les autres.\nL’idée c’est de regrouper des entities qui sont amenées à changer ensemble.\n\n\nL’aggregate est responsable de garantir la consistance des données en son sein.\nLes objets extérieurs pourront donc seulement lire ses propriétés, mais pour les changer il faudra toujours passer par l’interface publique de l’aggregate.\nLes fonctions de cette interface publique s’appellent les commands.\nIl y a 2 manière d'implémenter ces commands :\nSoit comme méthodes publiques de l’aggregate root.\nSoit la logique se trouve dans des objets de commande spécifiques à l’aggregate et contenant la logique à appliquer. On instancie un tel objet command avec les infos qu’on veut changer, et on le passe à une méthode “execute” de l’aggregate root chargée d’appliquer la commande.\n\n\nLe fait que l’aggregate soit le seul responsable de modifier ses données fait que la logique business le concernant ne peut qu’être en son sein, et pas disséminée ailleurs.\n\n\nLa couche applicative (aussi appelée souche “service” : celle qui forward les actions de l’API publique au domain model) n’a donc plus qu’à :\nCharger l’état actuel de l’aggregate.\nExécuter la commande requise.\nEnregistrer le nouvel état en BDD.\nRenvoyer le résultat à l’appelant.\n\n\nAttention à la concurrence. Pour éviter les problèmes, la couche applicative devra s’assurer que l’état lu au début n’a pas changé entre-temps avant de valider le nouvel état de l’aggregate en BDD.\nPour ce faire, le moyen le plus classique est que l’aggregate ait une propriété “version” incrémentée à chaque modification.\nSi la version a changé entre-temps par une autre action concurrente, on pourra choisir d’annuler notre action, ou d’appliquer un mécanisme de résolution approprié à notre logique business (si on en est capable).\n\n\n\n\nL’aggregate doit avoir un comportement transactionnel : toute commande devra soit être intégralement exécutée, soit annulée.\nEn revanche, aucune opération ne pourra modifier plusieurs aggregates de manière transactionnelle : c’est une transaction par action sur un seul aggregate.\nSi on se retrouve à avoir besoin de modifier plusieurs aggregates en une transaction, c’est que les limites de nos aggregates ont été mal pensées : il faut les revoir.\nExemple d’aggregate : un ticket de support, qui peut contenir un ou plusieurs messages, chacun pouvant contenir une ou plusieurs pièces jointes.\n\n\nLa règle de base pour délimiter son aggregate c’est de le choisir le plus petit possible, et d’y inclure toutes les données qui doivent rester fortement consistantes (strong consistency) avec l’aggregate.\nDonc si en prenant en compte la logique business, certaines données liées à l’aggregate peuvent être un peu désynchronisées (et se mettre à jour un peu plus tard) sans causer de problèmes ( => eventual consistency), alors elles doivent être hors de l’aggregate.\nDans ce cas la bonne pratique est de placer dans l’aggregate une propriété représentant leur ID, là où pour une entity faisant partie de l’aggregate on placera en propriété une instance de celle-ci :\n// le customer ne fait pas partie de l'aggregate\nprivate customer: UserID;\n// les messages en font partie\nprivate messages: Message[];\n\nExemple d’aggregate qu’on délimite : imaginons qu’on a notre ticket de support, avec une règle business qui implique de le réassigner à un autre agent s’il n’y a pas eu de nouveau messages alors qu’il reste moins de 50% du temps restant pour le traiter.\nSi les messages mettent du temps à être à jour par rapport au ticket (c’est-à-dire qu’il y a une eventual consistency entre les messages et leur ticket), on aura du mal à respecter la règle business parce qu’on ne saura pas si il y a eu un message récemment ou pas. => Donc les entities “messages” vont dans le même aggregate.\nEn revanche, si les données des agents ou des produits sont légèrement en retard, ça ne nous empêchera pas de respecter notre règle business. => Donc ceux-là ne vont pas dans le même aggregate.\n\n\n\n\n\n\n2.3 - Domain Events :\nLes domain events sont des événements importants que chaque aggregate peut publier, et auquel d’autres aggregates peuvent souscrire.\nLeur nom est important, et il doit être formulé au passé.\nPar ex : Ticket assigned ou Ticket escaleted\n\n\nLeur contenu doit décrire précisément ce qui s’est passé :\nExemple :\n{\n\"ticketId\": \"7bfaaf0c-128a-46f8-99b3-63f0851eb\",\n\"eventId\": 146,\n\"eventType\": \"ticket-escalation\",\n\"escalationReason\": \"missed-sla\",\n\"escalationTime\": 65463113312\n}\n\n\n\n\n\n3 - Domain services :\nQuand on a de la logique business qui n’appartient à aucun aggregate ou qui semble être pertinente pour plusieurs aggregates, on peut la placer dans un domain service.\nLe mot service ici ne fait pas référence à autre chose qu’on appellerait habituellement des services, c’est simplement des objets sans état qui contiennent de la logique business.\nDans les domain services on peut toucher à plusieurs aggregates, mais seulement pour de la lecture : la contrainte de ne modifier qu’un aggregate par transaction tient toujours.\nExemple de domain service : le temps pour répondre aux tickets de support doit être calculé en fonction de plusieurs paramètres, dépendant de l’agent en charge, d’éventuelles escalations, des shifts horaires etc. On peut donc avoir une fonction qui va lire dans plusieurs aggregates des informations, calcule le temps et le renvoie.\n\n\n\n\nAu fond, l’idée des value objects et des aggregates c’est de réduire le degré de liberté du système en encapsulant des morceaux de logique dans des endroits uniques, pour en réduire la complexité.","7---modeling-the-dimension-of-time#7 - Modeling the Dimension of Time":"On peut appliquer l’event-sourcing au domain model pattern pour créer un event-sourced domain model pattern.\nAvec l’event sourcing on introduit la dimension du temps dans notre modèle.\nIl s’agit de reprendre les mêmes blocs que le domain model pattern, mais avec une différence au niveau de la manière de stocker les informations en base de données (et c’est ça qui caractérise l’event sourcing) : on va stocker les domain events des aggregates comme source de vérité, au lieu de l’état actuel de l’aggregate.\nOn appelle cette BDD d’événements l’event store.\nLa BDD doit au minimum permettre de :\nrécupérer tous les événements d’un même aggregate\najouter des événements à un aggregate\n\n\n\n\n\n\nIl s’agit de rejouer l’ensemble des domain events d’un aggregate à chaque fois qu’on le charge en mémoire pour obtenir son état.\nUne fois qu’on a son état, on peut exécuter la logique business comme avant, en jouant la bonne commande sur l’aggregate.\nNotre aggregate génèrera alors des domain events, mais ne modifiera jamais son state autrement que par ces events.\nLa persistance se fera en persistant les domain event dans l’event store.\n\n\nAvec le domain model pattern classique on avait aussi des domain events qui informaient les composants extérieurs des changements importants, mais là la différence c’est que ces events doivent être émis pour chaque changement d’état de l’aggregate.\nCôté code, on a notre objet aggregate root avec ses propriétés de type value object ou entity, et cet objet a une méthode apply(event: EventType) qui est surchargée plusieurs fois pour chaque type d’event qui peut survenir.\npublic class Person {\npublic id: PersonId;\npublic name: Name;\npublic email: Email;\npublic version: number;\n\nfunction apply(event: PersonCreated) {\nthis.id = event.id;\nthis.name = event.name;\nthis.email = event.email;\nthis.version = 0;\n}\nfunction apply(event: EmailUpdated) {\nthis.email = event.email;\nthis.version += 1;\n}\n}\n\nLe constructeur de la classe prend la liste d’events et la rejoue dans l’ordre à chaque instanciation.\n\n\nOn peut rejouer la projection des événements avec des variantes :\nEn rejouant les X premiers événements on remonte à n’importe quel état précis dans le temps.\nEn nous intéressant seulement à certains événements et en implémentant des actions particulières dans les fonctions apply de ceux-ci, on peut rechercher des données particulières et faire de l’analyse de données.\nPar exemple, si on cherche toutes les personnes qui ont pu avoir une modification de leur email plus de 3 fois en une journée, il suffit de créer une classe qui surcharge apply pour l’event EmailUpdated, et de rejouer les événements en comparant les dates de changement dans notre méthode apply.\n\n\n\n\nAvantages et inconvénients :\nAvantages de la version event-sourced par rapport à la version classique :\nOn peut remonter dans le temps. Par exemple pour analyser, ou encore pour débugger en allant à l’état exact qui a causé le bug.\nOn a du deep insight sur ce qui se passe. Particulièrement utile pour les core subdomains qui sont complexes.\nOn a une forte traçabilité de tout ce qui se passe, très utile dans un contexte d’audit, par exemple si la loi l’oblige comme avec les transactions monétaires.\nQuand deux transactions écrivent la donnée de manière concurrente, on va habituellement annuler une des deux transactions. Ici on a des infos plus fines sur les deux changements concurrents, et donc on peut plus facilement prendre des décisions business sur la manière de les concilier au lieu de les annuler.\n\n\nDésavantages :\nCourbe d’apprentissage pour l’équipe : il faut du temps et la volonté pour l’équipe d’apprendre ce paradigme.\nL’évolution du modèle est plus compliquée : on ne change pas un schéma d’events comme on change la structure d’une BDD relationnelle.\nA ce propos il y a le livre Versioning in an Event Sourced System de Greg Young.\n\n\nLa complexité globale de l’architecture est plus grande.\n\n\nRDV au chapitre 10 pour avoir des règles basiques sur le type de design à choisir.\n\n\nA propos de la performance :\nOui rejouer les events à chaque fois qu’on instancie un objet affecte la performance, mais dans les faits :\nIl faut bien noter que les events sont propres à chaque aggregate, et que ceux-ci ont un cycle de vie.\nPar ex un ticket de support passera par plusieurs étapes, et finira par être fermé, donc n’aura plus de changements d’état.\n\n\nEn deçà de 10 000 events par aggregate, l’impact de performance ne se ressent pas.\nLa moyenne du cycle de vie des objets dans la plupart des systèmes est de l’ordre de 100 events.\n\n\n\n\nDans les rares cas où la performance est un problème, il y a la technique du snapshot.\nIl s’agit de générer de manière asynchrone une projection sous forme de BDD servant de cache, et ayant les données des états de chaque aggregates, mais aussi le numéro du dernier event pris en compte.\nQuand l’application a besoin de charger un aggregate, elle le fait depuis ce cache, puis lit le numéro du dernier event pris en compte, et va chercher les events manquants dans l’event store pour les appliquer elle-même et obtenir l’état à jour de l’aggregate.\nAinsi donc, même si l’aggregate a 100 000 events, l’application n’a à chaque fois qu’à appliquer les quelques events à rattraper qui n’avaient pas encore été appliqués sur la BDD dénormalisée servant de cache.\n\n\nAttention cependant, avant d’utiliser une technique avancée comme celle du snapshot il faut :\nS’assurer qu’on a un vrai problème de performance (+ de 10 000 events par aggregate).\nVérifier les limites de notre aggregate et voir si il n’est pas trop gros et qu’on ne pourrait pas plutôt le scinder.\n\n\n\n\nL’event sourcing scale très bien : vu qu’on a une séparation claire des données d’event par aggregate, on peut très bien sharder par identifiant d’aggregate.\nLe CQRS (décrit au chapitre suivant) peut répondre à la problématique de performance de par sa nature.\n\n\nComment supprimer des données de l’event store (par ex pour des considérations légales) ?\nL’idée de l’event sourcing c’est qu’on ne supprime pas les données d’event, c’est la condition pour toujours pouvoir rejouer, mais aussi la condition pour qu’elles ne soient jamais corrompues.\nOn peut utiliser la technique du forgettable payload : on stocke dans notre event une version chiffrée des donnée sensibles, et on stocke une paire clé de chiffrement / ID de l’aggregate dans un stockage key-value à part. Quand on a envie d’effacer une des données sensibles, il suffit de supprimer sa clé.\n\n\nNe pourrait-on pas continuer à utiliser une BDD relationnelle comme BDD principale, et écrire en plus les events soit dans un log, soit en utilisant un database trigger pour écrire dans une table en même temps ?\nC’est une mauvaise idée parce que le log qui ne serait pas la source de vérité finirait par se dégrader au fil du temps, parce qu’on y ferait moins attention.\nEt en plus il est difficile de se donner du mal à bien représenter le sens des events dans ceux-ci si ils ne sont pas la source de vérité et que cette info n’est pas destinée à la BDD principale.","8---architectural-patterns#8 - Architectural Patterns":"Jusqu’ici on s’est intéressé aux patterns tactiques pour la logique business, ici on dézoome un peu et on s’intéresse à la relation entre les diverses parties du code du système (la logique business est une de ces parties).\nSi on ne maintient pas une limite claire entre le code business et le reste, on risque d’avoir une logique diffuse un peu partout.\nÇa rend le code difficile à changer parce qu’il faut passer partout.\nCa fait qu’on peut oublier certains endroits et avoir des inconsistances.\n\n\nOn voit dans ce chapitre 3 architectural patterns :\nLayered architecture\nC’est un des patterns les plus courants, il s’agit d’avoir 3 couches.\n1- la couche de présentation représente l’interface utilisateur, mais de nos jours c’est plus large : Interface graphique, interface en ligne de commande, API programmatique ou réseau.\nOn l’appelle parfois aussi la user interface layer.\n\n\n2- la couche business : c’est là où on a les patterns décrits avant : active record, domain model pattern etc.\nOn l’appelle parfois aussi le domain layer ou le model layer.\n\n\n3- la couche data access où on stocke et manipule des données dans des bases de données, dans du cloud etc.\nOn l 'appelle parfois aussi l’infrastructure layer.\n\n\nCôté dépendance, la couche de présentation n’a accès qu’à la couche business, et la couche business qu’à la couche data access : presentation -> business -> data access\nCe pattern est souvent étendu avec une couche additionnelle appelée service : elle va venir se placer entre la présentation et le business. presentation -> service -> business -> data access\nLe service layer permet de faire la logique d’orchestration autour de la couche business, et permet de découpler davantage les couches présentation et business.\nPar exemple, on peut avoir un controller (du pattern MVC) qui réceptionne un appel API, et qui va simplement faire appel au bon service, puis renvoyer la réponse de celui-ci. Le service quant à lui va commencer une transaction, faire appel par exemple à un objet Active Record, et soit valider soit annuler la transaction avant de renvoyer le résultat.\nLe service layer peut être utile dans le cas où la partie business est un Active Record, mais si c’est un Transaction Script alors il n’y a rien à abstraire vis-à-vis de la couche présentation.\nOn l’appelle parfois aussi l’application layer.\n\n\nAttention à ne pas confondre la layered architecture avec une architecture N-Tier :\nN-Tier fait référence au fait d’avoir des composants déployables indépendamment et donc qu’on considère séparés “physiquement”, qui n’ont pas le même cycle de vie : par exemple ce qui tourne dans un navigateur, ce qui tourne sur un serveur applicatif, le serveur de base de données etc.\nQuand on parle de layered architecture, on parle bien des couches logicielles au sein d’une même entité physique, avec le même cycle de vie.\n\n\nEtant donné la dépendance entre couche business et couche de data access, la layered architecture est mal adaptée à du code business sous forme de domain model pattern. Elle est par contre adaptée pour du transaction script ou de l’active record.\n\n\nPorts and adapters\nElle est bien plus adaptée au domain model pattern parce que le code business va être découplé du reste.\nOn va mettre le business layer d’abord, sans qu’il ne dépende de rien, et ensuite on va regrouper tout ce qui est communication avec la BDD, frameworks, providers externes etc. dans une couche qu’on appellera infrastructure. Et enfin on ajoute une couche application layer (c’est plus parlant que service layer) au milieu.\nBusiness layer <- Application layer <- infrastructure layer\n\n\nGrâce au Dependency Inversion Principle, on va à chaque fois injecter les dépendances des couches supérieures dans les couches du dessous : la couche business ne connaît pas la nature concrète de la couche application, et la couche application ne connaît pas la nature concrète de la couche infrastructure.\nC’est ici que la notion de “ports and adapters” prend son sens : On a des ports qui sont des interfaces mises en place par les couches du dessus, et des adapters qui sont des implémentations concrètes faites par les couches du dessous, et passés en paramètre des fonctions des couches du dessus.\nExemple :\n// le port (couche business)\ninterface IMessaging {\nvoid publish(Message payload);\n}\n\n// l'adapter (couche infrastructure)\npublic class SQBus implements Imessaging {\nvoid publish(Message payload) {\n// ...\n}\n}\n\nIci la classe adapter qui respecte le contrat (le port) établi par la couche business pourra être donnée en paramètre dans les fonctions de la couche business où elle accepte un objet Imessaging. Elle pourra utiliser l’objet sans savoir exactement ce qu’il fait (publier dans un bus, mettre l’info simplement dans une variable etc..).\n\n\n\n\nOn l’appelle aussi (avec quelques variations) : hexagonal architecture, onion architecture ou encore clean architecture.\nDans ces variantes, l’application est parfois appelée service layer ou encore use case layer.\nEt le business layer est parfois appelé domain layer, ou encore core layer.\n\n\n\n\nCQRS (Command-Query Responsibility Segregation)\nCette architecture est similaire au ports & adapters pour ce qui est de la place centrale du code business indépendant du reste. La différence se trouve dans la manière de gérer les données : on va vouloir adopter un “polyglot modeling”, c’est-à-dire plusieurs formes dénormalisées de la donnée pour la lecture.\nA l’origine le CQRS a été pensé pour répondre au problème de l’event sourcing, en fournissant la possibilité de lire le state actuel des données directement, au lieu d’avoir à repartir de la forme primaire (qui est la liste d’événements) et de rejouer tous les events pour obtenir le state actuel.\nMais il est utile aussi sans event sourcing, c’est-à-dire dans le cas classique où la BDD “source de vérité” est par exemple relationnelle.\n\n\nComme l’indique le nom, on va faire une ségrégation (une séparation) entre les commands (les écritures) et les queries (les lectures).\nD’un côté les commands sont faites vers la base de données principale, source de vérité, et fortement cohérente (les invariants sont respectés, les règles business validées etc.).\nDe l’autre les queries sont faites vers des bases de données dérivées, utiles pour chaque cas particulier.\nPar exemple des data warehouses sous forme de colonnes pour l’analyse, des BDD adaptées à la recherche, des BDD relationnelles pour avoir le state des aggregates tout de suite etc.\nLes BDD de lecture peuvent aussi être des fichiers, ou encore des BDD in-memory. Et on peut normalement les supprimer et les reconstituer à nouveau à partir de la BDD principale.\n\n\n\n\nIl faut que les BDD secondaires soient mises à jour à chaque fois que la BDD principale est modifiée. On appelle ça les projections.\nIl y a les projections synchrones :\nIl s’agit d’un modèle de catch-up subscription.\nQuand une requête est faite à la BDD de lecture, le système de projection va faire une requête vers la BDD principale pour obtenir tous les changements de la donnée voulue jusqu’au précédent checkpoint.\nLes nouvelles données sont utilisées pour mettre à jour la BDD de lecture.\nLe système de projection retient le nouveau checkpoint à partir duquel il faudra faire la prochaine mise à jour de la donnée en question.\n\n\nPour que ça puisse fonctionner, il faut que la BDD principale note toutes les modifications faites à l’ensemble des entrées qu’elle possède, avec des checkpoints (des versions) pour chacune.\nNDLR : Ca ressemble à des domain events parce que chaque changement est noté en BDD. Mais pour que ça en soit il faut aussi que la signification métier de chaque changement soit noté, et pas juste “telle entrée est modifiée à telle valeur”. C’est dans cette mesure que le CQRS peut être utilisé même sans event sourcing.\n\n\n\n\nEt les projections asynchrones :\nIl s’agit d’un mécanisme de souscription :\nLa BDD principale publie chacun de ses changements dans un message bus, auquel souscrivent les projection engines de chaque BDD secondaire..\nA chaque message chaque BDD secondaire est mise à jour.\n\n\nCe modèle est plus scalable et plus performant, mais il est soumis aussi à des problématiques de consistance des données des BDD secondaires, et une plus grande difficulté à les reconstruire.\nIl est donc conseillé de rester sur le modèle synchrone et éventuellement construire une projection asynchrone par dessus.\n\n\n\n\n\n\nLes queries ne peuvent pas écrire de données, par contre les commands qui écrivent les données peuvent aussi retourner des valeurs, notamment le statut de succès ou d’échec de la commande, avec l’erreur spécifique.\nLe CQRS est adapté au domain model pattern, il est utile quand on veut avoir plusieurs modèles de données adaptés à plusieurs besoins. Il est aussi adapté à l’event sourced domain model pattern pour lequel il est quasi obligatoire.\n\n\n\n\nCes patterns doivent être appliqués au cas par cas pour les besoins de chaque brique logicielle. Il ne faut pas choisir un seul pattern pour tout le système, ni même forcément un seul pattern pour un même bounded context, dans la mesure où plusieurs subdomains peuvent exister dans un même bounded context.","9---communication-patterns#9 - Communication Patterns":"Dans ce chapitre il est question des patterns de communication entre briques logicielles distinctes et ayant chacune leur bounded context. C’est la mise en application technique du chapitre 4.\nPour implémenter un anticorruption layer ou un open-host service, on peut procéder de plusieurs manières :\nEn mode stateless : chaque requête est autonome. Dans ce cas, le bounded context contient le layer de transformation.\nOn va utiliser le pattern Proxy pour que la requête passe d’abord par le layer (le proxy), puis soit traduite correctement vers la target.\nLa traduction peut être synchrone, auquel cas le proxy est dans le même composant.\nOn peut aussi vouloir utiliser un API gateway pattern, en ayant un composant externe tenant le rôle du proxy et permettant d’intégrer plusieurs flux vers une même target.\nDans ce cas, on pourra utiliser une solution open source comme Kong ou KrakenD, ou une solution payante comme AWS API Gateway, Google Api-gee ou Azure API Management.\n\n\n\n\nLa traduction peut aussi être asynchrone, auquel cas le proxy est un composant qui souscrit à des events, et envoie des messages.\nIl pourrait en plus filtrer certains messages non pertinents.\nDans le cas d’un Open-Host Service, le modèle asynchrone est très pratique parce qu’il permet de traduire correctement les événements du langage privé vers les événements du langage public.\nEt même de filtrer certains événements qui n’ont d’utilité qu’au sein du bounded context et pas pour l’intégration avec les autres.\n\n\n\n\n\n\nEn mode statefull où le layer possède sa BDD et implémente une logique complexe.\nOn a le cas où on veut agréger les données entrantes. Soit parce qu’on veut les traiter par batchs, soit parce qu’on veut créer un plus gros message à partir de plus petits messages arrivant.\nLe BDD propre au proxy sert alors à gérer ce processus d'agrégation.\nParfois on peut utiliser un outil tout fait comme Kafka, AWS Kinesis, ou une solution de batch comme Apache Nifi, AWS Glue, Spark.\n\n\nOn a aussi le cas où on veut unifier des sources de données. Par exemple un backend for frontend qui va chercher des données chez plusieurs autres bounded contexts pour les afficher.\n\n\n\n\nA propos de l’implémentation technique des domain events émis par les aggregates et consommé par les autres composants.\nUne façon de faire triviale mais mauvaise serait de laisser l’aggregate faire l’ajout de l’event dans le message bus.\nL’event serait émis avant même que la transaction (qui a lieu dans le layer applicatif, après que le code de l’aggregate ait été exécuté) n'ait été commitée. On pourrait alors avoir des composants avertis d’une chose qui n’est pas encore vraie en DB.\nVoir même qui ne sera jamais vraie en DB si la transaction échoue.\n\n\n\n\nUne variante serait d’émettre l’event dans le layer applicatif juste après le commit dans la BDD.\nMais là encore que se passerait-il si la publication de l’event ne marchait pas ? Ou encore si le serveur crashait juste avant la publication de l’event ? On aurait la transaction validée mais pas d’event pour prévenir les autres.\n\n\nLe pattern Outbox permet d’adresser ces cas.\n1- Le state et l’event sont ajoutés dans la BDD en une seule transaction.\nL’event peut être ajouté dans une table différente si c’est possible. Si c’est une database NoSQL qui ne le permet pas, alors on peut le rajouter dans l’état de l’aggregate.\n\n\n2- Un message relay va s’occuper de traiter les events non publiés de la BDD en envoyant le message dans le message bus.\nLe relay peut soit poller régulièrement la BDD (il faut faire attention à ne pas la surcharger), soit on peut utiliser les fonctionnalités proactives de la BDD pour déclencher le relay.\n\n\n3- Une fois que l’event est envoyé, le message relay va enlever l’event de la BDD, ou le marquer comme publié.\nA noter que le relay garantit la publication du message au moins une fois : si il crash lui-même après avoir publié l’event mais avant de l’avoir enlevé de la BDD, il va le republier une 2ème fois.\n\n\n\n\n\n\nLe pattern Saga permet d’adresser les cas où on doit écrire dans plusieurs aggregates, vu que par principe on a décidé qu’une transaction ne devait écrire que dans un seul aggregate.\nLe saga va écouter des events émis par certains composants, et déclencher des commandes sur d’autres. Si la commande échoue, le saga est responsable de déclencher des commandes pour assurer la logique business.\nExemple : imaginons deux entities qu’on ne veut pas mettre dans le même aggregate parce qu'elles sont trop peu liées entre elles : une campagne de publicité et un publisher.\nOn veut que l’activation d’une campagne déclenche sa publication auprès du publisher, et qu’en fonction de la décision du publisher, elle soit soit validée soit annulée.\nLe saga va écouter l’event d’activation de l’aggregate de la campagne, et déclencher la commande de publication auprès de l’aggregate du publisher. Et il écoute aussi les events de validation ou d’annulation auprès de l’aggregate du publisher, pour déclencher une commande auprès de celui de la campagne en retour.\n\n\nLa logique de la saga peut aussi nécessiter de garder en mémoire son état, par exemple pour gérer correctement les actions de compensation. Dans ce cas les events de la saga peuvent être mis en BDD, et la saga peut être implémentée elle-même comme un event-sourced aggregate.\nDans ce cas il faut séparer la logique d’exécution des commandes de la mise à jour du state de la saga, et utiliser le même principe que pour l’outbox pattern. On aura un message relay qui garantira l’exécution de la commande même si on échoue à quelque étape que ce soit.\n\n\nAttention tout de même à ne pas utiliser les saga pour compenser des limites d’aggregates mal pensées. L’action du saga étant asynchrone, les données seront eventually consistent entre-elles. Les seules données fortement consistantes sont celles qui sont dans un même aggregate.\n\n\nLe Process Manager se base sur le même principe que la saga, mais là où la saga associe juste un event à une commande, le process manager va implémenter une logique plus complexe liée à plusieurs events pour choisir les commandes à déclencher.\nIl est implémenté sous forme de state-based ou event-sourced aggregate avec une persistance de son état en BDD.\nLà où la saga est déclenchée implicitement quand un événement qu’elle doit écouter se produit, le process manager est instancié par l’application et s’occupe de mener à bien un flow en plusieurs étapes.\nExemple : la réservation d’un voyage business commence par la sélection du trajet le plus optimal, puis la validation par l’employé. Dans le cas où l’employé préfère un autre trajet, le manager doit valider. Puis l’hôtel pré-approuvé doit être réservé. Et en cas d’absence d’hôtels disponibles, l’ensemble de la réservation est annulée.\nLà encore, vu qu’on a des commandes et un état, il vaut mieux implémenter l’outbox pattern pour jouer les commandes et mettre à jour l’état de manière sûre.","part-iii--applying-domain-driven-design-in-practice#Part III : Applying Domain-Driven Design in Practice":"","10---heuristics#10 - Heuristics":"Ce chapitre donne des heuristiques, c'est-à-dire des règles à appliquer qui marchent dans la plupart des cas.\nBounded contexts\nRecomposer différemment des bounded contexts quand on se rend compte que le découpage n’est pas bon coûte cher. Il vaut donc mieux commencer par des bounded contexts grands, et les redécouper plus finement par la suite.\nCette règle s’applique en particulier pour les bounded contexts contenant un core subdomain, qui change par nature beaucoup. Il peut être sage de laisser d’autres subdomains avec lesquels il réagit beaucoup dans le même bounded context.\n\n\nBusiness logic patterns\nSi le subdomain trace des transactions monétaires, ou doit permettre une traçabilité, ou encore des possibilités d’analyse approfondies des données, alors il faut choisir l’event sourced domain model pattern.\nSinon, si la logique business est quand même complexe (core subdomain), alors il faut choisir le domain model pattern classique.\nSinon, si la logique business inclut des transformations de données complexes, alors il faut choisir l’active record pattern.\nEt sinon, il faut se rabattre sur le transaction script pattern.\nConcernant le fait de savoir si une logique business est complexe :\nElle l’est si elle contient des règles business compliquées, des invariants et des algorithmes compliqués. On peut s’en rendre compte à partir de la validation des inputs (?).\nElle l’est aussi si l’ubiquitous language est compliqué. Et à l’inverse s’il décrit surtout des opérations CRUD, alors elle ne l’est pas.\nSi on se retrouve à avoir une différence entre le pattern qu’on veut utiliser pour répondre à la nature de la logique business, et la catégorie de notre subdomain, alors c’est l’occasion de s’interroger sur la pertinence de notre catégorisation.\nMais il ne faut pas oublier non plus que l’avantage compétitif peut ne pas résider dans la technique.\n\n\n\n\n\n\nArchitectural patterns\nL’event sourced domain model nécessite le CQRS. Dans le cas contraire, on est vraiment limité : on ne pourra qu’obtenir les entities par leur ID.\nLe domain model classique nécessite le ports & adapters, sinon on va avoir du mal à isoler la logique business de la persistance.\nL’active record va bien avec une layered architecture à 4 couches, avec la couche service qui va contenir la logique qui manipule l’active record.\nLe transaction script peut être implémenté avec une simple layered architecture à 3 couches.\nEnfin, même dans le cas où on n’a pas choisi l’event sourcing pour notre logique business, on peut quand même adopter le CQRS dans le cas où on aurait besoin d’avoir la donnée sous plusieurs formes différentes.\n\n\nTesting strategy\nLa testing pyramid (beaucoup de unit tests, moins de tests d’intégration, et peu de tests end to end) est bien adaptée aux domain model patterns (event sourced ou classique). Les value objects et les aggregates font de parfaits units autonomes.\nLe testing diamond (peu de test unitaires, beaucoup de tests d’intégration, et peu de tests end to end) est bien adapté à l’active record pattern. La logique business étant éparpillée à travers le layer service et business, il est pertinent de les tester ensemble. (avec la BDD du coup ?)\nLa reversed testing pyramid (peu de unit tests, plus de tests d’intégration, et beaucoup de tests end to end) est bien adaptée au transaction script. La logique business étant simple, et le nombre de couches faible, on peut directement tester de bout en bout.\n\n\nL’auteur a déjà rencontré des équipes qui utilisaient par exemple l’event sourced domain model pattern partout. Pour autant il ne le conseille pas, et a rencontré beaucoup plus d’équipes qui s’en sortaient bien avec ces règles d’heuristiques.","11---evolving-design-decisions#11 - Evolving Design Decisions":"Identifier les subdomains a des implications importantes. Mais il faut régulièrement questionner leur statut et éventuellement le réévaluer.\nCore -> supporting : Si l’avantage compétitif apporté par le subdomain n’est plus justifié, l’entreprise peut décider de réduire la complexité du subdomain au minimum pour se concentrer sur ceux qui ont plus de valeur.\nCore -> generic : On peut avoir un procédé qui constituait un avantage compétitif, qui se fait dépasser par un acteur qui se met à fournir le fournir sous forme de service sur le marché. Dans ce cas on se retrouve contraint d’utiliser sa solution pour rester à la page, et notre core subdomain devient un generic subdomain.\nSupporting -> core : Il s’agit de cas où un supporting subdomain dont la logique se complexifie. Si elle n’apporte pas d’avantage compétitif, alors il n’y a pas de raison à cette complexification et il faut la corriger. Sinon c’est qu’on se retrouve avec un core subdomain.\nSupporting -> generic : On peut imaginer qu’on ait créé un système simple pour gérer quelque chose. Puis il apparaît un système open source qui fait la même chose, mais a aussi des fonctionnalités que l’entreprise n’avait pas priorisé étant donné la faible valeur apportée par ce système. Mais si c’est à faible coût alors pourquoi pas : l’entreprise abandonne la solution maison pour la solution open source.\nGeneric -> core : Une entreprise utilisant une solution externe se retrouve limitée par celle-ci. Elle décide finalement d’implémenter une solution maison qui réponde vraiment à son besoin, et grâce à ça de mieux rendre son service. Un exemple est Amazon qui a créé une solution d’infrastructure web maison pour ses besoins, et a fini par la commercialiser sous le nom AWS.\nGeneric -> supporting : Pour la raison inverse de passer de supporting à generic, on peut décider que la solution open source ne vaut plus le coup parce qu’elle coûte trop cher à intégrer, et revenir à une solution maison simple.\n\n\nQuand ajouter des fonctionnalités devient douloureux, c’est un signe qu’on se retrouve avec des patterns tactiques qui ne sont plus adaptés à la complexité de notre logique business.\nPar exemple, si un supporting subdomain se met à avoir une logique de plus en plus complexe, il est peut être temps de le transformer en core.\n\n\nSi on a fait nos choix en conscience, et qu’on a connaissance des différents patterns existants, migrer n’est pas si difficile.\nTransaction script -> active record :\nIl s’agit dans les deux cas de logique procédurale. Quand on a des structures de données compliquées, il faut les repérer et les encapsuler la logique de lecture/écriture dans des objets.\n\n\nActive record -> domain model :\nOn pense à le faire quand on remarque que la logique business qui manipule les active records devient complexe, et qu’on se trouve face à des duplications qui mènent à des inconsistances à travers la codebase.\nOn identifie d’abord les value objects immuables dans notre logique. On identifie aussi la logique qui pourrait aller dans ces objets et on la migre dedans.\nEnsuite on essaye de délimiter les données qui doivent être mises à jour ensemble.\nOn peut mettre les setters des active records privés, et constater où ils sont appelés dans le code.\nOn peut alors déplacer tout le code qui est cassé à l’intérieur de l’active record. On obtient un bon candidat pour un aggregate.\nOn va ensuite examiner la hiérarchie d’entities qu’il faut construire, et extraire éventuellement plusieurs aggregates du code : un aggregate sera la plus petite unité dont les données doivent être fortement consistantes.\nEnfin on se débrouille pour que les méthodes de l’aggregate ne soient appelables que par l’aggregate root, et que seule l’interface publique soit appelable de l’extérieur.\n\n\n\n\nDomain model -> event sourced domain model :\nIl faut commencer par modéliser les domain events.\nLa partie la plus compliquée va être de gérer les events passés qui n’existent pas. On a deux manières de le faire :\nGénérer de fausses transitions passées.\nOn va analyser chaque aggregate et on va imaginer une manière réaliste d’obtenir l’état actuel par une succession de domain events successifs.\nOn va alors générer ces events en base et faire comme si notre aggregate avait toujours été event-sourced.\nL’avantage c’est qu’on pourra toujours faire des projections, y compris en partant de zéro.\nLe désavantage c’est que les events avant la migration seront inventés, et pourraient aussi induire en erreur sur une analyse.\n\n\nModéliser des events de migration.\nL’autre solution c’est d’accepter explicitement qu’avant un certain point on n’a pas de données. On va alors créer un event initial pour chaque aggregate, qui va simplement avoir pour effet de mettre la valeur actuelle de l’état de toutes les valeurs de l’aggregate.\nL’avantage c’est qu’on ne pourra pas avoir de fausses projections.\n\n\n\n\n\n\n\n\nLes changements organisationnels peuvent affecter les patterns d’intégration des bounded contexts :\nPar exemple, si une seule équipe gérait un bounded context avec plusieurs subdomains, l’arrivée d’une 2ème équipe mènera à la séparation du bounded context en deux, parce qu’un bounded context ne doit pas être géré par deux équipes différentes.\nAutre exemple : si un des bounded contexts est affecté à une autre équipe, et que la communication n’est pas bonne, on va passer du partnership à du customer-supplier.\nEt quand on a encore plus de problèmes de communication, il vaut parfois mieux dupliquer la fonctionnalité et passer sur du separate ways.\n\n\nIl faut prendre soin de la connaissance du domaine.\nEn particulier pour les core subdomains, la modélisation du domaine est complexe et change souvent. Il faut donc régulièrement revoir les value objects, les aggregates etc.\nParfois la connaissance du domaine se perd, la documentation n’est plus à jour, les gens bougent etc. Il faut alors la retrouver, par exemple avec des sessions d’event storming.\n\n\nA mesure que le code grossit, les décisions de design deviennent obsolètes et doivent être adaptées. Si on ne le fait pas, on va obtenir un big ball of mud.\nIl faut identifier et régulièrement éliminer l’accidental complexity résultant des décisions design obsolètes, et n’adresser que l’essential complexity inhérente au domaine avec les outils tactiques du DDD.\nLes subdomains qui deviennent de plus en plus gros peuvent être distillés pour en mettre en évidence d’autres. Il faut en particulier le faire avec les core subdomains pour que la partie core soit circonscrite à ce qui est vraiment nécessaire.\nLes bounded contexts peuvent aussi être revisités quand ils grossissent trop :\nOn peut simplifier le modèle du bounded context en extrayant un bounded context chargé d’un problème spécifique qui a grossi.\nParfois on se rend compte qu’un bounded context n’arrive pas à faire une action sans être dépendant d’un autre bounded context. On peut alors revoir ses limites pour augmenter son autonomie.\n\n\nLes aggregates doivent rester des unités incluant le plus petit set de données possible qui doivent être fortement consistantes entre elles.\nAu fil du temps on peut être amené à ajouter des choses dans des aggregates existants parce que c’est plus pratique.\nIl faut donc régulièrement revérifier le contenu des aggregates. Et ne pas hésiter à extraire des fonctionnalités dans un nouvel aggregate pour simplifier le premier.\nOn se rend souvent compte que le nouvel aggregate révèle un nouveau modèle et amène à la création d’un nouveau bounded context.","12---eventstorming#12 - EventStorming":"L’event storming est un atelier qui permet de modéliser ensemble un process business particulier.\nIl s’agit de construire la story du business process concerné, à travers une timeline d’events qu’on fait apparaître sur un tableau au cours de l’atelier.\n\n\nLes membres participant à l’atelier doivent être le plus divers possible (devs, domain experts, product owners, UI/UX, CSM etc.). Mais il vaut mieux ne pas dépasser 10 personnes.\nCôté matériel :\nÇa se passe sur un mur, avec un gros rouleau de papier sur lequel on va pouvoir coller des post-its.\nIl faut des post-its de couleur, chaque couleur représentant un concept particulier. Et des marqueurs pour écrire dessus.\nL’atelier dure 2 à 4 heures, donc il faut prévoir de quoi grignoter.\nIl faut une salle spacieuse, et rien qui ne gène pour que chaque participant accède librement au tableau.\nIl ne faut pas de chaises, les participants sont debout.\n\n\n\n\nL’atelier se passe en 10 étapes, pendant lequel le modèle est enrichi collectivement :\nÉtape 1 - Unstructured Exploration : il s’agit d’une étape en mode brainstorming : tous les participants prennent des post-its oranges, et écrivent dessus des events liés au process business auquel on s’intéresse.\nLes events sont collés sur le tableau sans se soucier de l’ordre ou de la redondance.\nLes events sont formulés au passé.\nExemple d’events : “Notification sent”, “Destination chosen”, “Order shipped”.\n\n\nÉtape 2 - Timelines : Les participants vont organiser les post-its dans le bon ordre, en commençant par le “happy path scenario”, puis par les cas d’erreur.\nOn peut tracer des flèches à partir d’un post-it pour mener à plusieurs flows de posi-its possibles.\nC’est aussi le moment d’enlever les doublons.\n\n\nÉtape 3 - Pain Points : On va marquer les points qui requièrent une attention particulière : des bottlenecks, des étapes manuelles à automatiser, de la doc ou du domain knowledge manquant etc.\nOn fait ça avec des post-its roses, tournés de 45° pour être “en diamant”.\nCette étape y est dédiée spécifiquement, mais le facilitateur doit aussi faire attention aux pain points levés tout au long de l’atelier et les marquer.\n\n\nÉtape 4 - Pivotal Events : On va s’intéresser à des events marquant une étape importante dans le processus, et tracer une barre verticale le long du tableau, pour délimiter l’avant et l’après cet event.\nExemple : dans un flow d’achat, “order initialized”, “order shipped” et “order delivered” peuvent être des events importants.\n\n\nÉtape 5 - Commands : On va ajouter des commandes sur des post-its bleus, juste avant l’event qui résulte de la commande.\nLes commandes sont formulées à l’impératif.\nExemple : “Publish campaign”, “Submit order”.\nSi l’actor (la persona du business domain, ex : client, administrateur etc.) qui émet la commande est évident, on l’ajoute sur le post-it de la commande avec un post-it jaune.\n\n\nÉtape 6 - Policies : Presque à chaque fois il y a des commandes qui n’ont pas d’actor associé. On peut alors leur ajouter une automation policy qui consiste à les déclencher lors d’un domain event, sans intervention manuelle.\nOn ajoute ces policies avec des post-its violets, et on va les placer entre la commande et l’event concernés.\nOn peut ajouter d’éventuels critères de déclenchement. Par exemple, si un event “complaint received” doit déclencher la commande “escalate” seulement si ça vient d’un client VIP, on peut mettre sur le post-it de policy que c’est seulement si client VIP.\nDans le cas où l’event et la commande sont loin, ne pas hésiter à tracer une flèche pour les joindre.\n\n\nÉtape 7 - Read Models : On ajoute les “read models”, c'est-à-dire les interfaces utilisateur (écran, rapport, notification etc.) utilisées par les actors pour prendre leurs décisions d’exécuter les commandes.\nIls seront ajoutés sur des post-its verts.\nExemple de read model : “Shopping cart”, pour un actor “customer”, qui va actionner la commande “submit order”.\n\n\nÉtape 8 - External Systems : On va représenter les intéractions avec des systèmes externes, c’est-à-dire qui ne font pas partie du domaine qu’on est en train d’explorer.\nIls seront ajoutés sur des post-its roses.\nÇa peut être un composant qui déclenche une commande, par exemple un CRM qui déclenche l’ordre d’expédier une commande?\nCa peut aussi être un composant qu’on va notifier, par exemple notifier le CRM suite à un event qui dit que l’expédition est approuvée.\nA la fin de cette étape, toutes les commandes devront avoir leur origine associée : soit un actor, soit une policy, soit un système externe.\n\n\nÉtape 9 - Aggregates : On va pouvoir organiser les commandes et events représentés en aggregates, sachant qu’un aggregate reçoit des commandes, et émet des events.\nOn va ajouter de longs post-its jaunes verticaux pour représenter l’aggregate. On va les placer entre les commandes et les events.\n\n\nÉtape 10 - Bounded Contexts : On va finalement essayer de regrouper les aggregates entre eux, soit parce qu’ils représentent des fonctionnalités liées entre elles, soit par leur couplage via des policies.\nCes groupes d’aggregates seront de bons candidats pour des bounded contexts.\nOn peut les matérialiser avec des pointillés entourant les groupes d’aggregates liés.\n\n\n\n\nLe créateur de l’event storming lui-même (Alberto Brandolini) dit qu’il s’agit simplement de conseil sur la manière de mener l’atelier. On peut très bien l’adapter comme ça nous convient.\nL’auteur du livre applique d’abord les étapes 1 à 4 sur le domaine tout entier pour obtenir une vision large du domaine et identifier les process business.\nEnsuite il organise un atelier pour chaque process business pertinent, en suivant cette fois toutes les étapes.\n\n\nLa vraie valeur de l’event storming c’est surtout le processus en lui-même, et le fait qu’il permette la communication entre les différentes parties prenantes, en leur permettant d’aligner leur modèle mental, de découvrir d’éventuels modèles en conflit, et de formuler un ubiquitous language.\nLes objets obtenus sur le tableau sont un bonus. Tous les ingrédients sont là pour implémenter un event-sourced domain model pattern si on est bien face à un core subdomain et qu’on veut faire ce choix.\n\n\nL’utilisation de l’event storming permet non seulement de construire l’ubiquitous language, et construire le domain model, mais il est aussi utile pour explorer de nouvelles fonctionnalités, retrouver de la connaissance du domaine perdue, onboarder de nouvelles recrues en leur donnant de la connaissance du domaine.\nEn revanche, l'event storming sera peu utile si le process business examiné est simple et évident.\n\n\nTips pour les facilitateurs :\nAu début de l’atelier, donner une vision d’ensemble du processus, et présenter les éléments de modélisation qui vont être utilisés tout au long (en construisant une légende avec les post-its de chaque couleur par exemple).\nSi on sent un ralentissement du dynamisme du groupe, on peut relancer avec une question par exemple, ou peut être que c’est le moment de passer à l’étape suivante.\nTout le monde doit participer, si certain(e)s sont en retrait, essayer de les inclure en leur posant une question sur l’état actuel du modèle.\nL’activité étant intense, il y aura au moins un break.\nIl ne faut pas reprendre l’activité tant que tout le monde n’est pas de retour.\nOn peut reprendre en récapitulant le modèle tel qu’il a été défini pour le moment.\n\n\nLes ateliers en remote sont plus difficiles à mener, l’auteur conseille de se limiter à 5 personnes.\nParmi les outils existants, miro.com est celui qui est le plus connu au moment de l’écriture du livre.\n\n\nPas besoin d’être ceinture noire pour faciliter une session : on apprend en faisant.","13---domain-driven-design-in-the-real-world#13 - Domain-Driven Design in the Real World":"Les techniques du domain driven design apporteront le plus de bénéfices aux brownfield projects : les projets ayant déjà un business et s’étant éventuellement embourbés sous forme de big ball of mud.\nPas besoin que tous les devs soient des ceintures noires du DDD, ni d’appliquer toutes les techniques que le DDD propose.\nPar exemple, si on préfère d’autres patterns tactiques que ceux du DDD, c’est tout à fait OK.\n\n\nIl faut d’abord commencer par l’analyse stratégique :\nD’abord comprendre le domaine avec une vue haut niveau (qui sont les clients, que fait l’entreprise, qui sont les compétiteurs etc.).\nPuis identifier les subdomains. Pour ça on peut partir de l’organigramme de l’entreprise.\nPour les core subdomains, on peut se demander ce qui différencie l’entreprise de ses compétiteurs :\nPeut-être un algorithme maison que les autres n’ont pas ?\nPeut être un avantage non-technique comme la capacité à embaucher du personnel top niveau, ou de produire un design artistique unique ?\nUne heuristique qui fonctionne bien est de trouver les composants qui sont dans le pire état : ceux qui sont devenus des big balls of mud, que les devs détestent mais que les dirigeants refusent de faire réécrire à cause du risque business.\n\n\nPour les generic subdomains il s’agit simplement de trouver les solutions prêtes à l’emploi, soit open source, soit payantes.\nLes supporting subdomains sont les composants restants.\nIls peuvent être dans un mauvais état mais suscitent moins de plainte de la part des devs parce qu’ils sont moins souvent modifiés.\n\n\nOn n’est pas obligés d’identifier tous les subdomains. On peut commencer par les plus importants.\n\n\nOn peut ensuite identifier et analyser les différents composants logiciels.\nLe critère ici c’est le cycle de vie : les différents composants sont ceux qu’on peut faire évoluer et déployer indépendamment des autres.\nOn peut alors regarder les patterns d’architecture utilisés pour chaque composant, et vérifier si un pattern complexe serait plus adapté, ou à l’inverse si on pourrait utiliser un plus simple ou même une solution existante.\nOn peut ensuite faire comme si ces composants étaient des bounded contexts, et tracer le context map avec la relation entre chacun d’entre eux.\nOn peut là aussi vérifier si les patterns d’intégration de bounded context sont améliorables : plusieurs équipes travaillant sur le même composant, implémentations de core subdomain dupliquées, implémentation de core subdomain sous-traitée, frictions à cause d’une mauvaise communication entre équipes etc.\n\n\n\n\nOn pourra par la suite utiliser l’event storming pour construire un ubiquitous language, et éventuellement retrouver de la connaissance du domaine perdue.\n\n\nEnsuite on peut mettre en place une stratégie de modernisation :\nIl ne s’agira pas de tout réécrire parce que ça marche rarement, et que c’est supporté par le management encore plus rarement.\nIl faut accepter que dans un grand système, tout ne sera pas bien designé, et se concentrer sur les composants qu’on estime stratégiques.\nMais pour ça il faut déjà s’assurer qu’on a bien une délimitation, au moins logique, entre les subdomains (code séparé sous forme de modules, namespaces, packages etc.).\nNe pas oublier aussi les bouts de code du subdomain qui seraient ailleurs, sous forme de stored procedures d’une BDD, ou sous forme de serverless functions.\n\n\nOn peut alors commencer à extraire des composants logiques en bounded contexts physiques, en commençant par ceux qui apportent le plus de valeur.\nIl faut ensuite bien examiner les composants extraits et réfléchir à comment les moderniser :\nIl faut examiner leur intégration vis-à-vis de la relation entre les équipes qui en ont la charge et de leur niveau de communication (partnership vers customer/supplier, shared kernel vers separate ways etc.).\nCôté tactique, il faut se concentrer sur les composants qui apportent beaucoup de valeur (core) mais dont l’implémentation ne serait pas adaptée et rendrait difficile la maintenance, en utilisant plutôt un domain model pattern.\nIl faut aussi ne pas oublier de construire un ubiquitous language avec les domain experts, notamment au travers d’ateliers d’event storming.\n\n\nConcernant la modernisation de chaque bounded context extrait, on peut procéder de deux manières :\nLe strangler pattern : on va créer un nouveau bounded context à côté de l’ancien, et toutes les nouvelles fonctionnalités iront dedans. On migrera aussi les anciennes progressivement jusqu’à ce qu’il ne reste plus rien du premier.\nOn peut mettre en place une façade devant les deux bounded contexts pour rediriger les appels vers l’un ou l’autre. Elle disparaît quand le composant legacy meurt.\nLes deux bounded contexts peuvent temporairement partager une même base de données (chose qu’on ne fait pas d’habitude pour deux bounded contexts).\n\n\nLe refactoring progressif : on va changer le modèle et les patterns progressivement au sein du bounded context comme expliqué au chapitre 11.\nIl faut procéder par étapes, et ne pas sauter d’un transaction script à un event sourced domain model par exemple.\nIl faut passer du temps à trouver les bonnes limites pour les aggregates, en particulier si on va jusqu’à l’event sourced domain model, les changer ensuite est plus difficile.\nL’introduction du domain model pattern elle-même peut se faire en plusieurs étapes : par exemple commencer par trouver les objets immutables pour les extraire en value objects.\n\n\n\n\n\n\nComment introduire le DDD au sein de mon organisation ?\nÉtant donné les changements importants, y compris organisationnels et d’implication des effectifs hors ingénierie, avoir un appui du top management peut beaucoup aider. Mais c’est plutôt rare.\nCeci dit, l'essentiel du DDD reste des pratiques d’ingénierie logicielle, donc on peut commencer à l’utiliser dans ses activités quotidiennes même sans mise en place à l’échelle de l’organisation.\nOn peut déjà commencer à construire un ubiquitous language et l’utiliser.\nÉcouter les domain experts parler, leur demander des clarifications, repérer les doublons et demander à ce qu’on n'utilise qu’un des termes.\nParler avec les domain experts plus souvent, pas forcément au cours de meetings formels. En général ils sont ravis de parler aux devs qui sont sincèrement intéressés par comprendre le domaine.\nUtiliser la terminologie qu’on a constitué dans le code, et dans tous les autres supports ou échanges.\n\n\nPour les bounded contexts, l’important est de comprendre les principes sous-jacents pour pouvoir les utiliser :\nPourquoi créer plusieurs modèles utiles à chaque problème ? Parce qu’utiliser un très gros modèle est rarement efficace.\nPourquoi un bounded context ne doit pas avoir des modèles en conflit en son sein : à cause de la complexité que ça engendre.\nPourquoi plusieurs équipes ne devraient pas travailler sur un même bounded context ? A cause de la friction et de la mauvaise collaboration que ça engendre.\n\n\nC’est la même chose pour les patterns tactiques : il faut comprendre la logique de chaque pattern et utiliser cette logique pour améliorer son design, plutôt que faire appel à l’argument d’autorité “DDD” qui ne mènera nulle part.\nPourquoi faire des limites transactionnelles explicites ? Pour protéger la consistance de la donnée.\nPourquoi une transaction DB ne peut pas modifier plus d’une instance d’un aggregate à la fois ? Pour être sûr que les limites de consistance sont correctes.\nPourquoi l’état d’un aggregate ne peut pas être modifié directement par un autre composant ? Pour s’assurer que toute la logique business est située au même endroit et pas dupliquée.\nPourquoi ne pourrait-on pas déplacer une partie de la logique d’un aggregate dans une stored procedure ? Pour être sûr de ne pas dupliquer la logique, parce que la logique dupliquée dans un autre composant a tendance à se désynchroniser et mener à de la corruption de données.\nPourquoi essayer d’avoir des limites d’aggregates petites ? Parce que des limites transactionnelles larges augmentent la complexité de l’aggregate et impactent négativement la performance.\nPourquoi, à la place de l’event sourcing, ne pourrait-on pas écrire la donnée dans un fichier de log ? Parce qu’on n’aura pas de garantie de consistance de longue durée pour la donnée.\n\n\nA propos de l’event sourcing en particulier, expliquer le principe aux domain experts, et en particulier le niveau d’insight qu’on acquiert sur la donnée, va en général les convaincre que c’est une bonne idée.","part-iv--relationships-to-other-methodologies-and-patterns#Part IV : Relationships to Other Methodologies and Patterns":"","14---microservices#14 - Microservices":"Un service est une entité qui reçoit des données en entrée et envoie des données en sortie. Ca peut être de manière synchrone comme avec le modèle request/response, ou asynchrone comme avec le modèles basé sur les events.\nIl a une interface publique qui décrit comment on peut communiquer avec. Elle est en général suffisante pour comprendre ce que le service fait.\n\n\nUn microservice est un service avec une petite interface publique.\nEn limitant son interface on le rend plus facilement compréhensible, et on réduit les raisons qu’il a de changer.\nC’est aussi pour ça qu’un microservice possède sa propre base de données et ne l’expose pas.\nNDLR : cette courte définition n’est pas partagée par tout le monde.\nDave Farley définit le microservice comme étant d’abord une unité déployable indépendamment, et donc conçue de telle manière qu’elle n’ait pas besoin d’être testée avec d’autres unités. cf. vidéo 1, vidéo 2\n\n\n\n\nQuand on se pose la question d’à quel point notre service devrait avoir une petite interface, il faut prendre en compte la complexité locale (la complexité interne de chaque microservice) et la complexité globale. (la complexité de l’ensemble résultant de l’interaction entre microservices).\nPour réduire au maximum la complexité globale, il suffit d’implémenter l’ensemble sous forme d’un unique service monolithique. La complexité locale est alors maximale, et le risque est de finir avec un big ball of mud.\nPour réduire au maximum la complexité locale, on pourrait mettre chaque fonction dans un microservice, avec sa base de données. La complexité globale est alors maximale, et on risque alors de se retrouver avec un distributed big ball of mud.\nIl s’agit donc de trouver un juste milieu entre complexité locale et globale.\nL’auteur évoque le concept de depth proposé par John Ousterhout dans son livre The philosophy of Software Design : un deep module a une petite interface mais une plus grande implémentation, alors qu’un shallow module a une grande interface comparé à son implémentation (donc beaucoup de choses sont exposées).\nC'est la même chose pour les microservices, il faut les concevoir avec l’interface la plus petite possible, tout en ayant une implémentation importante en comparaison. Il faut que le microservice en tant qu’entité d’encapsulation encapsule des choses, sinon il ne fera qu’ajouter de la complexité accidentelle.\n\n\n\n\nLes microservices sont souvent confondus avec les bounded contexts.\nIl est vrai qu’un microservice est forcément un bounded context : il ne peut pas être géré par deux équipes, et il ne peut pas y avoir plusieurs modèles en conflit en son sein.\nEn revanche, un bounded context peut être plus large que ce qui serait raisonnable pour un microservice, il peut contenir plusieurs subdomains tant qu’un même modèle (et un même ubiquitous language) permet d’en rendre compte.\nDu coup :\nDes ensembles trop vastes contenant des modèles en conflit mènent à du big ball of mud.\nDes ensembles moins vastes avec un modèle unique sont des bounded contexts.\nDes ensemble suffisamment petits pour avoir une petite interface publique sont des microservices et aussi des bounded contexts.\nSi on découpe au-delà, on tombe sur du distributed big ball of mud.\n\n\n\n\nParfois on a envie d’utiliser des aggregates comme microservices.\nIl s’agit de regarder le couplage entre l’aggregate et les autres composants du subdomain : s'il communique souvent avec d’autres composants, si le changer mènerait souvent à changer d’autres composants etc. Plus ce couplage est fort, plus le microservice résultant sera “shallow”.\nParfois l’aggregate est suffisamment indépendant et ça marche bien, mais la plupart du temps c’est une mauvaise idée : l’aggregate se trouve être un découpage trop petit et mène au distributed big ball of mud.\n\n\nEnfin, une autre possibilité est d’utiliser les subdomains pour les microservices.\nLes subdomains sont de bons candidats pour des deep modules.\nIls sont concentrés sur les use-cases plutôt que sur la manière dont ceux-ci seront implémentés.\nLes use-cases sont dépendants les uns des autres.\nLe subdomain agit sur un même jeu de données.\n\n\nChoisir les subdomains comme heuristique pour ses microservices est donc une bonne idée.\nParfois il sera préférable de choisir un ensemble plus vaste qui sera un bounded context, parfois un ensemble plus restreint qui sera un aggregate. Mais la plupart du temps les subdomains feront l’affaire.\n\n\n\n\nLes Open-Host Services, et Anticorruption Layers peuvent contribuer à réduire davantage l’interface publique des microservices, en n’exposant qu’une version réduite du modèle interne.\nDans le cas de l’ACL, il peut être mis dans un service à part, réduisant donc l’interface publique du service consommateur qui se protège.","15---event-driven-architecture#15 - Event-Driven Architecture":"L’event driven architecture est une architecture dans laquelle les composants souscrivent et réagissent à des événements de manière asynchrone, plutôt que sous forme de requête/réponse de manière synchrone.\nLe pattern saga en est un exemple.\n\n\nOn parle bien ici de communication entre composants (ie. bounded contexts). Alors que l’event sourcing porte sur l’utilisation d’events à l’intérieur du BC, l’event driven architecture s’intéresse aux events utilisés pour communiquer entre BCs.\nL’event et la commande sont tous deux des messages.\nL’event décrit quelque chose qui s’est déjà passé et ne peut pas être annulé ou refusé.\nLa commande décrit quelque chose qui doit être fait, et qui pourrait être refusé, auquel cas on peut déclencher des commandes de compensation.\n\n\nOn peut classer les events en 3 catégories :\n1- L’event notification est un event qui sert à notifier un composant de quelque chose, pour le pousser à faire une query dès qu’il est disponible.\nOn ne va pas mettre toute l’info dans l’event, étant donné qu’on s’attend à ce que le composant refasse une query quand il est prêt.\nAvantages d’obliger à faire une query :\nCa peut être bien niveau sécurité : notifier avec des infos non sensibles dans l’event, et vérifier avec une autorisation plus forte quand la query explicite est faite.\nÇa peut permettre au composant d’être sûr d’avoir des infos à jour au moment du processing vu que la query sera synchrone, contrairement à l’event.\nÇa peut permettre de mettre en place une opération bloquante du point de vue concurrence pour que ce soit fait par une seule instance.\n\n\nExemple : ici on n’a quasi aucune info à part l’ID et un lien pour en savoir plus.\n{\n\"type\": \"mariage-recorded\",\n\"person-id\": \"01b9a761\",\n\"payload\": {\n\"person-id\": \"01b9a761\",\n\"details\": \"/01b9a761/mariage-data\"\n}\n}\n\n\n\n2- L’event-carried state transfer (ECST) est un event qui va donner l’information complète permettant à un composant externe de maintenir un cache de l’état interne de nos objets.\nOn peut soit envoyer à chaque fois un snapshot complet de l’état d’un objet, ou alors ne renvoyer que les modifications de cet état et le receveur se débrouille pour maintenir la cohérence de l’état au fur et à mesure.\nAvantages de maintenir un cache à distance :\nOn est plus tolérant aux fautes : le composant qui nous consomme peut continuer à fonctionner même si on est down.\nOn économise des requêtes : le composant consommateur n’a pas à faire de requête à chaque fois pour obtenir la donnée, et si elle ne change pas il n’y aura pas non plus d’event vu que le cache sera déjà à jour.\n\n\nExemple : ici on a la donnée qui a changé dans le state de la personne concernée, pour qu’on mette à jour notre cache.\n{\n\"type\": \"personal-details-changed\",\n\"person-id\": \"01b9a761\",\n\"payload\": {\n\"new-last-name\": \"Williams\"\n}\n}\n\n\n\n3- Le domain event décrit un événement lié au business domain, et qui s’est produit dans le passé. Il est là dans un but de modélisation du business domain et pas spécialement pour des considérations techniques vis-à-vis des autres composants.\nPar rapport à l’event notification :\nLe domain event inclut toute l’info nécessaire pour décrire l’event, alors que l’event notification non.\nLe domain event est utile en interne même si aucun composant externe s’y intéresse, alors que le but de l’event notification est uniquement l’intégration avec les composants externes.\n\n\nPar rapport à l’ECST :\nLe but est différent : le domain event décrit le fonctionnement du domaine, alors que l’ECST est là pour exposer l’état interne des objets pour des raisons techniques.\nEn s’abonnant à un type précis de domain event, on n’a pas du tout la garantie d’obtenir tous les changements d’un aggregate par exemple, contrairement à ce que permet l’ECST.\n\n\nExemple : on modélise l’événement qui s’est produit au plus près possible du domaine.\n{\n\"type\": \"married\",\n\"person-id\": \"01b9a761\",\n\"payload\": {\n\"person-id\": \"01b9a761\",\n\"assumed-partner-last-name\": \"true\"\n}\n}\n\n\n\n\n\nPour montrer qu’il ne suffit pas de saupoudrer un système d’events pour le transformer en event driven architecture, voici un exemple réel d’architecture produisant un distributed big ball of mud :\nImaginons un composant (bounded context) CRM utilisant l’event sourced domain model. Trois autres composants choisissent de consommer l’ensemble de ses domain events :\nLe BC Marketing les transforme en état, puis les utilise pour se mettre à jour.\nLe BC AdsOptimization fait quelque chose de similaire à Marketing pour d’autres besoins.\nLe BC Reporting les consomme, puis attend 5 mn avant de faire une query vers AdsOptimization, pour espérer qu’AdsOptimization aura déjà traité les données liées à cet event.\n\n\nOn se retrouve avec 3 problèmes :\nUn couplage temporel : Reporting attend 5 mn avant de faire sa requête, mais rien de garantit que ce sera suffisant. Si AdsOptimization est surchargé, qu’on a des problèmes réseau ou autres, Reporting risque de faire sa requête avant qu’AdsOptimization n’ait fini de processer les données liées à cet event.\nSolution : plutôt que de faire consommer les events de CRM à Reporting, on peut faire en sorte que ce soit AdsOptimization qui envoie un notification event à Reporting quand il a processé quelque chose de nouveau, pour que Reporting fasse sa requête.\n\n\nUn couplage fonctionnel : Marketing et AdsOptimization consomment tous deux les mêmes events, qu’ils transforment tous deux en objets avec état exactement de la même manière. Ça crée une duplication de logique dans ces deux bounded contexts.\nSolution : CRM peut implémenter l’open-host service pour y implémenter la logique qui était dupliquée dans Marketing et AdsOptimization. Comme ça les consommateurs ne s'encombrent pas d’implémenter ça.\n\n\nUn couplage à l’implémentation : vu que CRM est event-sourced, les autres BCs sont couplés à l’implémentation de CRM. A chaque fois qu’il change quelque chose, il faut qu’ils mettent à jour leur code aussi.\nSolution : CRM, bien qu’event-sourced, n’a pas besoin d’exposer tous ses domain events. Il peut choisir publiquement d’exposer seulement certains d’entre eux, ou choisir d’exposer un type d’event différent, par exemple ici des ECST vu que Marketing et AdsOptimization sont intéressés par le fait d’avoir un cache de l’état de certains des objets de CRM.\n\n\n\n\n\n\nQuelques bonnes pratiques pour l’event driven architecture :\nIl faut toujours s’attendre au pire : éviter le mindset “things will be ok” et partir du principe que tout peut échouer.\nOn parle d’architecture distribuée : le réseau peut avoir des problèmes, les serveurs peuvent crasher, les events peuvent arriver deux fois ou pas du tout etc.\nIl faut donc s’assurer à tout prix que les events arrivent bien correctement :\nUtiliser l’outbox pattern pour publier les messages.\nS’assurer que les messages pourront être dédupliqués ou réordonnés grâce à leur numéro s' ils arrivent dans le mauvais ordre.\nUtiliser les patterns saga et process manager pour orchestrer des process cross-bounded contexts qui nécessitent des actions de compensation.\n\n\n\n\nIl faut bien distinguer les events publics des privés : ne pas tout exposer tel quel mais traiter les events qu’on expose comme une interface publique classique.\nNe jamais exposer l’ensemble des domain events d’un event sourced bounded context.\nQuand on implémente un open-host service, bien s’assurer que le modèle qu’on veut exposer est bien différent du modèle interne à notre bounded context, ce qui peut impliquer de transformer certains events, et pas seulement les filtrer.\nExposer plutôt des ECST et des notification events, et des domain events avec parcimonie, en distinguant bien ceux qu’on expose.\n\n\nOn peut utiliser le besoin de consistance comme critère supplémentaire pour le choix du type d’event :\nSi l’eventual consistency est OK, on peut utiliser l’ECST.\nSi le BC consommateur a besoin de lire la dernière écriture dans le state du producteur, alors le notification event appelant à faire une query est sans doute plus adapté.","16---data-mesh#16 - Data Mesh":"Les transactions OLTP (T pour transaction) et OLAP (A pour analytics) ont des buts, des consommateurs et des temporalités différents.\nLes OLTP permet de servir les clients sur des opérations plutôt en temps réel, et dont les fonctionnalités sont connues et optimisées.\nLes OLAP sont là pour obtenir des insights à partir des données, et permettre à l’entreprise d’optimiser le business.\nIls prennent en général plus de temps parce qu’ils portent sur de grandes quantités de données, et utilisent des données moins à jour.\nIls portent sur des données normalisées qui permettent une grande flexibilité de requêtes de la part des data analysts.\nNDLR : les OLAP utilisent souvent des BDD orientées colonne plutôt que lignes, c’est-à-dire que les données de toutes les entrées d’une même colonne sont stockées physiquement les unes à la suite des autres, pour faciliter les requêtes qui portent sur peu de colonnes et beaucoup d’entrées. (cf. Designing Data-Intensive Applications).\n\n\n\n\nLes OLTP vont typiquement utiliser des données organisées sous forme relationnelle avec des entités individuelles reliées entre elles, pour faciliter le fonctionnement des systèmes opérationnels.\nLes OLAP en revanche s’intéressent plus aux activités business et vont utiliser un modèle de données basé sur les fact tables et dimension tables.\nLes fact tables sont des tables qu’on va remplir avec des événements liés au business qui se sont produits dans le passé.\nExemple : Fact_Sales peut être un fact table contenant une entrée pour chaque vente qui a eu lieu.\nIls sont choisis pour répondre aux besoins des data analysts qui vont utiliser la BDD.\nSelon les besoins, on peut choisir d’y mettre seulement certaines données, par exemple des changements de statut dont on garde seulement une entrée toutes les 30 minutes, parce que plus serait inutile ou inefficace pour ce qu’on veut.\nLes données y sont ajoutées pour être lues, on n’y fait pas de modification.\n\n\nLes dimension tables sont référencées par des foreign keys à partir de la fact table, et décrivent des propriétés du fact en question.\nExemple : Dim_Agents, Dim_Customers etc. qui vont chacun avoir leurs propres champs qui les décrivent.\nOn remarque bien la forte normalisation avec la fact table au centre, et les dimension tables autour, qui permettent de maximiser la flexibilité des requêtes possibles sur les données autour de ce fact.\n\n\nOn appelle ce modèle fact tables / dimension tables le star schema.\nIl existe une variante appelée le snowflake schema, où les dimensions sont sur plusieurs niveaux : les dimension tables ont elles-mêmes des foreign keys vers d’autres dimensions qui les décrivent.\nL’avantage du snowflake c’est qu’il prend moins de place pour les mêmes données, par contre il faudra faire plus de jointures et donc les requêtes seront plus lentes.\n\n\n\n\nLe data warehouse consiste à extraire les données des systèmes opérationnels, et les mettre dans une grande BDD avec un modèle orienté analytics (star, snowflakes etc.). Les data analysts et ingénieurs BI vont alors consommer cette BDD avec du SQL.\nOn a des flows de type ETL qui vont consommer les BDD mais aussi éventuellement des events, des logs etc. pour construire le data warehouse.\nIl peut y avoir de la déduplication, de l’élimination d’informations sensibles, de l'agrégation etc.\nLe data warehouse pose plusieurs problèmes :\nOn retombe sur la problématique d’un modèle unique pour régler tous les problèmes (dont on s’était sorti par la création de bounded contexts), qui est inefficace.\nUne des solutions qui a été trouvée c’est de créer des data marts qui vont constituer des BDD spécifiques, soit extraites à partir de la data warehouse, soit extraite directement à partir d’un système opérationnel.\nLe problème c’est que si le data mart est extrait de la data warehouse on ne règle pas le problème du modèle unique dont on dépend. Et si on extrait à partir d’un système opérationnel, on a du mal ensuite à faire des requêtes cross-database entre plusieurs data marts, à cause de problèmes de performance.\n\n\nOn a aussi un couplage à l’implémentation des systèmes opérationnels. Or être couplé à l’implémentation de gens à qui on parle peu c’est catastrophique. Dès qu’ils modifient leur schéma de données, ça casse les scripts d’ETL qui nourrit le data warehouse.\n\n\n\n\nLe data lake est censé résoudre certains de ces problèmes, en se plaçant entre les systèmes opérationnels et la data warehouse.\nElle centralise au même endroit les données opérationnelles sans changer leur schéma. Le data warehouse va alors pouvoir être rempli avec un ou plusieurs flows ETL à partir du data lake.\nQuand le modèle du warehouse n’est plus satisfaisant, les data analysts vont pouvoir piocher dans le data lake avec d’autres scripts ETL.\nIl y a cependant des problèmes :\nLe processus est plus complexe, et les data engineers se retrouvent souvent à maintenir plusieurs versions d’un même script ETL pour gérer plusieurs versions d’un système opérationnel.\n=> on n’a pas vraiment réglé le souci de couplage à une implémentation gérée par une autre équipe.\n\n\nLe data lake étant schema-less, il n’apporte pas de garantie vis-à-vis de la consistance des données venant des systèmes opérationnels.\nQuand les données deviennent grandes, le data lake se transforme en data swamp (marécage de données), rendant le travail des data scientists très difficile.\n\n\n\n\n\n\nLe data mesh tente de répondre à son tour à ces problématiques, en adoptant d’une certaine manière une approche DDD appliquée à la data.\nLe data mesh a 4 grands principes :\nDecompose data around domains :\nOn va prendre les bounded contexts qu’on avait créés, et y intégrer une partie data correspondant aux données issues des BDD opérationnelles de ce bounded context.\nL’équipe en charge du bounded context aura alors sous sa responsabilité à la fois la partie OLTP et la partie OLAP de son bounded context.\nOn élimine donc la friction qu’il y avait entre équipe feature et équipe data en intégrant une personne avec les compétences data dans l’équipe feature.\n\n\nData as a product :\nFini les scripts ETL douteux pour construire la data, chaque bounded context expose sa data proprement avec une API publique, à laquelle il accorde le même soin qu’une API publique destinée à être consommée par un composant OLTP du système.\nOn va mettre en place des SLA/SLO, on versionne le modèle de données exposé, les endpoints doivent être faciles à trouver et le schéma clairement défini etc.\n\n\nLa data étant un produit plutôt qu’un élément de seconde classe, chaque équipe gérant le bounded context a la responsabilité d’assurer la qualité et l’intégrité de la data exposée. Mais aussi servir la donnée sous les formats qui pourront intéresser les consommateurs (SQL, fichier etc.).\nL’idée c’est que les data analysts/BI puissent aller chercher facilement des données de plusieurs bounded context, appliquer éventuellement des transformations en local, et faire leur analyse.\n\n\n\n\nEnable autonomy :\nCréer une infrastructure pour gérer la data étant difficile, il faut une équipe centrale dédiée à l'infrastructure de la plateforme data.\nElle sera en charge de maintenir la plateforme qui permet aux feature teams de créer facilement leur produit data sous les différents formats possibles.\nPar contre elle reste la plus agnostique possible par rapport au modèle de données, qui est de la responsabilité des équipes qui produisent la donnée.\n\n\nBuild ecosystem :\nIl faut un corps de “gouvernance fédérale” pour penser l’écosystème data au sein de l’entreprise, et notamment la question de l’interopérabilité.\nCe corps est composé de représentants data et produit des équipes feature, et de représentants de l’équipe plateforme centrale.\n\n\n\n\nCôté relation avec le DDD :\nLe DDD aide à structurer la donnée analytique en amenant l’ubiquitous language et la connaissance du domaine.\nExposer une donnée différente de la donnée utilisée est l’open-host service pattern du DDD.\nGrâce au CQRS, on peut facilement mettre en place une ou plusieurs formes dénormalisées de plus qui auront un modèle analytique.\nLes patterns de relation entre bounded context s’appliquent aussi aux données analytiques (partnership, ACL, separate ways etc.).","appendix-a---applying-ddd-a-case-study#Appendix A - Applying DDD: A Case Study":"Vlad a passé quelques années dans une startup qui appliquait le DDD. Il s’agit ici d’une description de ce qu’ils ont fait, et des erreurs commises.\nL’entreprise portait sur le marketing en ligne, de la stratégie marketing aux éléments graphiques, campagnes marketing et appels des prospects récoltés.\nIl y avait aussi une importante partie data analytics sur l’optimisation des campagnes de marketing, le fait de faire travailler les agents sur les prospects les plus intéressants etc.\n\n\nLes 5 bounded contexts qu’il nous présente pour en tirer des leçons :\n1- Marketing : il s’agit d’une solution de gestion des campagnes.\nQuasiment chaque nom présent dans les requirements était un aggregate. Pour autant la plupart n’avaient que peu de logique, celle-ci étant essentiellement dans un énorme service layer.\nQuand on essaye d’implémenter un domain model mais qu’on finit avec un active record pattern, on appelle ça un anemic domain model antipattern.\n\n\nMalgré l’architecture mal adaptée, le projet a été un succès, grâce à l’ubiquitous language mis en place dès le début, et les conversations très fréquentes avec les domain experts.\n\n\n2- CRM : les sales l’utilisaient pour se répartir les prospects de manière optimisée.\nIls ont d’abord commencé à développer le CRM à l’intérieur du même monolithe que Marketing. Puis voyant que le modèle avait des incohérences, ils ont extrait CRM dans son propre bounded context (au niveau du code seulement).\nIls ont cette fois tenté d’utiliser le domain model pattern en mettant beaucoup de logique dans les aggregates, et chaque transaction n’affectant qu’un aggregate.\nLe tout prenant beaucoup de temps, le management a décidé de donner certaines fonctionnalités à l’équipe database, qui l’a fait sous forme de stored procedures.\nDeux équipes qui ne se parlent que peu, et qui travaillent sur le même bounded context : de la duplication de logique, des corruptions de données etc. la cata.\n\n\n3- Event Crunchers : ils ont remarqué que les événements venant des clients amenaient à modifier les deux bounded contexts, alors ils ont extrait la logique dans un 3ème.\nInitialement pensé comme un supporting subdomain, et développé avec du transaction script, la logique s’est vite complexifiée, et la qualité dégradée.\nFinalement au fil du temps la logique était devenue tellement complexe et bordélique, qu’ils ont dû le refaire sous forme d’event-sourced domain model, avec les autres bounded contexts souscrivant à ses events.\n\n\n4- Bonuses : il s’agit de calculer les bonus des sales.\nLà encore ça partait d’une logique simple, donc un supporting subdomain. Ils ont choisi d’utiliser l’active record pattern.\nLà encore la logique s’est complexifiée assez vite, grâce à l’ubiquitous language qui était en place avec les domain experts, ils ont pu se rendre compte que le modèle ne convenait plus à la complexité plus tôt que pour Event Crunchers : ils l’ont recodé sous forme d’event-sourced domain model.\n\n\n5- Marketing Hub : une nouvelle idée du management : se servir des nombreux prospects acquis pour les vendre à des clients.\nIls ont dès le début catalogué le subdomain comme core, et utilisé l’event-sourcing et CQRS.\nIls ont aussi utilisé les microservices, un concept qui devenait populaire à ce moment. Mais ils ont fait un microservice par aggregate, avec un aggregate event-sourced, et les autres state-based.\nAu fil du temps chaque micro service avait besoin de quasiment tous les autres, et ils se sont retrouvés avec un distributed monolith.\nFinalement, même si le subdomain était une source de profit et donc core, la partie logicielle en elle-même était très simple, et le pattern utilisé s’est révélé être largement overkill, amenant de la complexité accidentelle.\n\n\n\n\nL’ubiquitous language est selon Vlad le “core subdomain” du DDD : à chaque fois qu’ils l’ont bien mis en place, le projet a plutôt marché, et à chaque fois qu’ils ne l’ont pas fait, le projet a plutôt échoué.\nPlus on le met en place tôt, plus on évite des problèmes.\n\n\nLes subdomains sont aussi très importants. Mal les identifier amène à utiliser les mauvais patterns.\nVlad propose d’inverser la relation entre le subdomain et les patterns tactiques : d’abord choisir le pattern tactique qui convient le mieux au requirement, puis qualifier le type de subdomain, et enfin vérifier ce type avec les gens du business.\nSi le business pense que le subdomain est core, mais qu’on peut le réaliser facilement, on l’a peut être mal découpé et analysé, ou il faut se poser des questions sur la viabilité de l’idée business.\nSi le business pense que c’est supporting, mais qu’on ne peut le réaliser qu’avec des patterns complexes, alors :\nSoit le business s’enflamme sur les requirements et ajoute de l’accidental business complexity (mettre beaucoup de ressources sur une activité non rentable).\nSoit les gens du business ne se rendent pas compte qu’ils obtiennent un gain compétitif qu’ils n’avaient pas envisagé avec le subdomain en question.\n\n\n\n\nIl ne faut pas ignorer la douleur : si elle se manifeste, c’est sans doute que les patterns utilisés ne sont pas en phase avec la problématique business. Il ne faut pas hésiter à requalifier le subdomain.\n\n\nA propos des bounded contexts, comme évoqué précédemment, le mieux est de les choisir larges, puis de les découper quand le besoin se fait sentir, et que la connaissance du domaine augmente.\nFinalement la startup a été profitable assez vite, et a fini par être rachetée par un de leur gros client. Pendant ces années, ils ont été en mode startup : changer les priorités et requirements rapidement, des timeframes agressives, et une petite équipe R&D. Pour Vlad le DDD a rempli ses promesses."}},"/notes/unit-testing":{"title":"Unit Testing: Principles, Practices, and Patterns","data":{"":"","i---the-bigger-picture#I - The bigger picture":"","1---the-goal-of-unit-testing#1 - The goal of unit testing":"De nos jours, la plupart des entreprises créent des tests pour leurs logiciels.\nEn moyenne le ratio code de test / code de prod est entre 1/1 et 3/1 (en faveur du code de test), et parfois plus.\n\n\nLe but principal des unit tests c’est de permettre une croissance durable du projet. Sans eux, le temps de développement explose au bout d’un moment.\nOn appelle cette explosion la software entropy : la désorganisation progressive du code.\n\n\nLe fait qu’un code soit difficilement testable est un signe de mauvaise conception à cause d’un couplage inapproprié. C’est un bon indicateur négatif. Par contre, si le code est testable, ça ne veut pas dire qu’il est bon, on ne peut pas en faire un indicateur positif.\nLes tests sont un code comme un autre, ils ont un coût de maintenance, et peuvent avoir une valeur nulle ou même négative. Il vaut mieux ne garder que les bons tests.\nLe code coverage est un bon indicateur négatif : si le code coverage est faible c’est que le code est peu testé. Par contre, si le coverage est élevé, on ne peut rien conclure : c’est un mauvais indicateur positif.\nLe problème du coverage c’est :\nQu’on ne peut pas s’assurer qu’on vérifie tout ce qui est fait. Par exemple, si un code renvoie un résultat et assigne ce résultat dans une variable globale, et que le test vérifie seulement l’une de ces choses, on ne pourra pas savoir que l’autre n’est pas testée malgré le coverage de 100%.\nQu’on ne teste pas les chemins permis par nos dépendances. On délègue souvent des responsabilités à des dépendances qui permettent beaucoup de flexibilité, mais sans tester chaque possibilité offerte. Et on ne peut pas vérifier qu’on le fait.\nLe simple fait de faire un parseInt(variable) fera que variable marchera dans des cas précis supportés par la fonction standard parseInt(). Pour autant, on ne peut pas s’assurer de tester chacun de ces chemins et leurs conséquences avec notre code.\n\n\n\n\nSe fixer le coverage comme target crée un incentive pervers qui va à l’encontre de l’objectif du unit testing. Le coverage doit rester un indicateur (négatif).\n\n\nLe branch coverage est une autre forme de coverage qui compte le nombre d’embranchements (if, switch etc.) testés sur le nombre total d’embranchements. C’est un peu mieux que le code coverage, mais ça reste pour autant seulement un indicateur négatif.\nUn bon test c’est un test qui :\nest intégré au cycle de développement, exécuté le plus souvent possible.\nteste les parties les plus importantes de la codebase. En général c’est la logique business.\noffre une grande valeur comparé aux coûts de sa maintenance.","2---what-is-a-unit-test#2 - What is a unit test?":"Il existe deux écoles de unit testing : la classical school (qu’on pratiquait à l’origine), et la London school, qui est née à Londres.\nUn livre canonique pour le style classique est Test-Driven Development: By Example de Kent Beck, et un livre pour le style London est Growing Object -Oriented Software, Guided by Tests de Steve Freeman et Nat Pryce.\n\n\nUn unit test est un test qui vérifie une unité de code, de manière rapide, et isolée.\nLe code testé peut avoir des dépendances.\nLes shared dependencies sont celles qui affectent les tests entre eux parce qu’ils sont changés par le code et ne sont pas réinitialisés entre les tests. Par exemple, une base de données est shared. Elle pourrait ne pas l’être si elle était instanciée à chaque test.\nLes private dependencies sont celles qui ne sont pas partagées.\nLes out-of-process dependencies sont celles qui sont exécutées dans un autre processus. Elles impliquent un temps d’exécution plus important que de rester dans le même processus que le code exécuté. La base de données est out-of-process, même si on l’instancie à chaque fois.\nLes volatile dependencies sont soit non installées sur un environnement par défaut (c’est le cas d’une base de données, mais pas d’un filesystem par exemple), soit on un comportement non déterministe (par exemple new Date()).\nA propos de la gestion de dépendances, l’auteur conseille Dependency Injection: Principles, Practices, Patterns de Steven Deursen et Mark Seemann.\n\n\nOn appellera par la suite\ncollaborators les dépendances qui sont soit shared, soit mutables (un objet utilisé par l’objet qu’on teste, la base de données etc.)\nvalues les dépendances qui sont immutables (par exemple un Value Object, le nombre 5 etc.).\n\n\nLa controverse entre les deux écoles porte sur l’isolation :\nPour la London school l’isolation porte sur le code testé.\nTout collaborator qui n’est pas directement testé doit être remplacé dans les tests par un test double (c’est le terme générique, le terme mock est une forme particulière de test double). On tolère seulement les dépendances immutables (les values).\nLe “unit” c’est l’unité de code (la classe), donc on a un fichier de test par classe.\nAvantages :\nÇa permet de tester du code même très couplé, en remplaçant simplement les dépendances dans les tests par des doubles.\nÇa permet d’être sûr que seul un test ne marchera plus si une fonctionnalité ne marche plus.\n\n\nInconvénients :\nÇa ne force pas à faire du code découplé.\nLes tests cassent facilement au moindre refactoring.\n\n\n\n\nPour la classical school l’isolation porte sur les tests entre eux : il faut pouvoir les jouer en parallèle sans qu'ils s'affectent mutuellement.\nOn n’utilise les doubles que très peu, seulement pour éliminer les shared dependencies.\nLe “unit” c’est le comportement (la fonctionnalité), et celle-ci peut contenir plusieurs classes qui seront toutes instanciées indirectement dans les tests.\nAvantages :\nCa force à faire du code découplé.\nCela permet d’avoir des tests qui collent mieux au cas d’usage business, et qui sont moins fragiles aux refactorings.\n\n\nInconvénients :\nSi une fonctionnalité ne marche pas, plusieurs tests peuvent casser.\nMais si on rejoue tous les tests à chaque changement de code, on peut savoir que c’est lui qui vient de faire passer les tests au rouge.\nEt en plus si un changement casse beaucoup de tests, ça permet de savoir que cette partie du code est très importante.\n\n\n\n\n\n\n\n\nLes deux écoles ont aussi une différence dans leur rapport au TDD :\nLa London school va avoir tendance à faire du outside-in TDD, en construisant d’abord les classes de plus haut niveau utilisant des collaborators sous forme de test doubles. Puis les implémenter petit à petit en allant vers le détail.\nLa classical school va plutôt mener à du inside-out TDD, en partant des classes les plus bas niveau dans le modèle, pour construire par-dessus jusqu’aux couches supérieures.\n\n\nUn test d’intégration est un test qui ne répond pas à une des 3 caractéristiques du test unitaire (tester une unité, de manière rapide et isolée).\nPour la London school, les caractéristiques sont :\nVérifier le comportement d’une seule classe.\nLe faire vite.\nLe faire en isolation vis-à-vis des dépendances de cette classe (grâce aux doubles).\n\n\nPour la classical school :\nVérifier le comportement d’une unité de comportement.\nLe faire vite.\nLe faire avec des tests isolés les uns par rapport aux autres.\n\n\nDu coup pour savoir ce qui est test d’intégration :\nLa plupart des tests unitaires selon la classical school sont des tests d’intégration pour la London school puisqu’ils font intervenir plusieurs classes.\nUn test qui teste plusieurs unités de comportement sera un test d’intégration pour la classical school.\nDans le cas où on a une out-of-process dependency (comme une DB) impliquée, les tests sont lents donc on sera sur des tests d’intégration pour les deux écoles.\nSi on a une shared dependency (comme une DB) impliquée, là encore on aura un test d’intégration pour les deux écoles.\n\n\n\n\nUn test end-to-end est un test d’intégration qui teste toutes les dépendances out-of-process (ou la plupart d’entre elles), là où les autres tests d’intégration n’en testent qu’une ou deux (genre juste la DB, mais pas RabbitMQ ou le provider d’emails).\nDans la suite du livre l’auteur va plutôt adopter l’approche classique, parce que c’est celle qu’il préfère et celle qui est la plus courante.","3---the-anatomy-of-a-unit-test#3 - The anatomy of a unit test":"Les tests unitaires doivent être structurés avec les 3 blocs Arrange, Act, Assert (qu’on appelle aussi Given, When , Then) :\nArrange : Il peut être aussi gros que les deux autres sections réunies. S’il est plus gros, il est conseillé de l’extraire dans une fonction pour augmenter la lisibilité.\nAvoir une méthode unique (constructeur de la classe de tests, ou beforeAll / BeforeEach global) est une moins bonne idée puisqu’on couple les tests ensemble, et que ce que possède chaque test est moins clair.\nL’idéal c’est avoir des fonctions de type factory configurables, qu’on peut réutiliser dans les tests en sachant depuis le test à peu près ce qu’on crée.\n\n\nAct : Il ne devrait faire qu’une ligne vu qu’on est censé vérifier une unité de comportement. S’il fait plus, c’est qu’on a la possibilité de faire une partie de la chose et pas l’autre, ce qui peut vouloir dire qu’on est en état de casser un invariant, et donc qu’on a une mauvaise encapsulation de notre code.\nExemple : si notre act c’est deux lignes qui font :\ncustomer.purchase(item);\nstore.removeFromInventory(item);\nC’est que l’un peut être fait sans l’autre au niveau de l’interface publique (celle-là même qui est testée). C’est un danger qu’on s’impose pour rien. Une seule méthode publique devrait faire les deux.\n\n\nAssert : Vu qu’on vérifie une unité de comportement, il peut y avoir plusieurs outcomes, donc plusieurs asserts.\nAttention quand même, si cette section grossit trop, c’est peut être le signe d’une mauvaise abstraction du code. Par exemple, si on doit comparer toutes les propriétés d’un objet un par un, et qu’on aurait pu implémenter l’opérateur d’égalité sur l’objet en question, et ne faire qu’un assert.\n\n\n\n\nOn écrit en général d’abord la partie arrange si on a déjà écrit le code, et d’abord la partie assert si on fait du TDD.\nQuand on teste plusieurs unités de comportement, on se retrouve par définition avec un test d’intégration. Il vaut mieux revenir sur de l’unitaire si possible.\nIl faut éviter les if dans les tests. Ça complique la compréhension et la maintenance.\nPour repérer facilement l’objet qu’on teste, et ne pas le confondre avec des dépendances, l’auteur conseille d’appeler l’objet testé sut (pour System Under Test) :\n// Arrange\nconst sut = new Calculator();\n// Act\nconst result = sut.sum(3, 2);\n// Assert\nexpect(result).toBe(5);\n\nPour bien séparer les 3 sections AAA l’une de l’autre, l’auteur conseille :\nSoit de laisser une ligne entre chaque section.\nSoit, si certaines sections doivent déjà sauter des lignes parce qu’elles sont longues, de laisser en commentaire //Arrange, //Act et //Assert\n\n\nA propos de la manière de nommer les tests :\nVu que les tests testent un comportement, le nom des tests doit être une phrase, qui a du sens pour les experts métier.\nSauf dans le cas où on teste des fonctions utilitaires, qui n’ont donc pas de sens pour les experts métier.\n\n\nIl faut éviter de mettre le nom du SUT (la fonction testée) dans le nom du test. Ça oblige à changer le nom du test si le nom de la fonction change, et ça n’apporte pas grand chose puisque c’est le comportement qui nous intéresse.\nIl vaut mieux être spécifique dans le nom du test. Par exemple, si on teste qu’une date est invalide si elle est au passé, préciser ça plutôt que de rester vague en parlant simplement de vérifier si la date est valide.\n\n\nOn peut utiliser les tests paramétrisés pour grouper des tests dont seule une valeur d’entrée et la valeur attendue change. Par exemple tester avec la date d'aujourd'hui, avec la date de demain, etc.\nAttention quand même, faire ce genre de regroupement a un coût en lisibilité. Donc à faire que si les tests sont simples.\nIl faut éviter de mettre dans le même test les cas positifs et les cas négatifs.\nEn général, le framework de test fournit la possibilité de paramétriser les tests, en acceptant une liste de paramètres à faire varier en entrée du test.","ii---making-your-tests-work-for-you#II - Making your tests work for you":"","4---the-four-pillars-of-a-good-unit-test#4 - The four pillars of a good unit test":"Dans ce chapitre on pose des critères pour évaluer la qualité d’un test.\nUn bon test a 4 caractéristiques fondamentales :\n1- Protéger des régressions : éviter les bugs.\nPour évaluer ce point on peut prendre en compte :\nLa quantité de code exécutée : plus il y en a plus c’est fiable.\nLa complexité du code exécuté.\nL’importance du code : tester du code du domaine est plus utile que de tester du code utilitaire.\n\n\n\n\n2- Résister aux refactorings : à quel point on peut refactorer sans casser le test.\nPour évaluer ce critère, on peut regarder si le test produit souvent des faux positifs pendant les refactorings.\nL’intérêt de ce critère c’est que si on a trop de faux positifs :\nOn porte de moins en moins attention au résultat des tests puisqu’ils disent souvent n’importe quoi : et on laisse passer de vrais bugs.\nOn n’ose plus refactorer le code, puisqu’on n’a pas confiance dans les tests. Et le code pourrit.\n\n\nCe qui fait casser les tests pendant les refactorings c’est souvent le couplage aux détails d’implémentation, au lieu de porter sur un comportement attendu du point de vue métier.\nPour avoir une idée de ce que veut dire “tester les détails d’implémentation”, l’exemple le plus extrême de ce genre serait un test qui vérifierait simplement que le code source de la fonction testée est bien le code source attendu dans le test. Ce test casserait littéralement à chaque changement.\nSans aller jusqu’à cet extrême, on retrouve souvent des tests qui vérifient la structure interne d’un objet, ou qu'une fonction appelle telle ou telle autre fonction etc. sans que ça n’ait aucun intérêt d’un point de vue métier.\n\n\n\n\n3- Donner un feedback rapide : à quel point on peut exécuter le test vite.\nPlus le test est lent, moins souvent on l’exécutera.\n\n\n4- Être maintenable : il y a deux composantes :\nA quel point c’est difficile de comprendre le test. Ça dépend de la taille du test, de la lisibilité de son code.\nA quel point c’est difficile de lancer le test. Par exemple à cause de la database qui doit être en train de tourner etc.\n\n\n\n\nLes deux premiers piliers caractérisent la précision (accuracy) du test.\nLa protection contre les régressions dépend de la capacité à ne pas avoir de faux négatifs (bugs présents mais ratés par les tests). C’est le fait d’avoir le bon signal.\nLa résistance aux refactorings dépend de la capacité à ne pas avoir de faux positifs (fausses alarmes). C’est l’absence de bruit.\nAu début du projet, les faux positifs (les bugs pas couverts) ont la plus grande importance. Mais à mesure que le projet avance, les faux négatifs deviennent de plus en plus gênants et empêchent de garder le code sain en le refactorant.\nDonc si on est sur un projet moyen ou gros, il faut porter une attention égale aux faux positifs et aux faux négatifs.\n\n\n\n\nOn peut noter un test sur chacun des 4 critères, et lui donner une note finale qui nous aidera à décider si on le garde ou non (pour rappel : garder un test n’est pas gratuit, ça implique de la maintenance).\nOn peut évaluer (subjectivement) la valeur du test à chacun des 4 critères, entre 0 et 1, puis multiplier ces quatre valeurs pour avoir le résultat final.\nCa implique donc qu’un test qui vaut zéro à l’un des critères aura une valeur finale de zéro. On ne peut pas négliger un des critères.\n\n\nOn ne peut malheureusement pas obtenir la note maximale partout, parce que les 3 premiers critères ont un caractère exclusif entre eux : on ne peut en avoir que deux parfaits.\nLes tests end to end, par exemple, maximisent la protection vis-à-vis des régressions parce qu’ils exécutent beaucoup de code, et sont résistants aux refactorings vu qu’ils testent depuis ce que voit l’utilisateur final. Par contre ils sont très lents.\nEt si on a des tests très rapides, en général on n’obtiendra pas à la fois un découplage et donc une résistance aux refactorings, et en même temps une capacité à arrêter tous les bugs.\n\n\nLa règle à retenir c’est que la résistance aux refactorings est non-négociable, pour la raison que ce critère est assez binaire : soit on est bien découplé, soit non. Et si on ne l’est pas, la valeur du test passe à zéro.\nLe choix qui reste c’est donc la possibilité de faire varier le curseur entre la rapidité du test, et sa capacité à empêcher les régressions.\n\n\n\n\nSi on examine notre pyramide de tests (unit, integration, e2e), on maximisera d’abord le critère non-négociable de résistance aux refactorings pour tous, puis :\nLes tests unitaires sont les plus rapides et protègent le moins, puis on a les tests d’intégration qui sont au milieu, et les tests e2e sont très lents et protègent le plus.\nEn général on a peu de tests e2e parce que leur extrême lenteur diminue beaucoup leur valeur. Et ils sont aussi difficiles à maintenir.\nPour les projets classiques on aura une pyramide, et pour les projets très simples (CRUD etc.), on pourra se retrouver avec un rectangle.\n\n\nLe black-box testing consiste à tester sans prendre en compte la structure interne, seulement avec les considérations business. Le white-box testing consiste à faire l’inverse.\nLe white-box testing menant à du code couplé aux détails d’implémentations, il n’est pas résistant aux refactorings, donc il ne faut pas l’utiliser (sauf pour analyser).","5---mocks-and-test-fragility#5 - Mocks and test fragility":"Il y a principalement deux types de test doubles :\n1- Les mocks qui aident à émuler et examiner les interactions sortantes, c’est-à-dire le cas où le SUT interagit pour changer l’état d’une de ses dépendances.\nOn pourrait voir le mock comme la commande du pattern CQS.\nIl existe une petite distinction avec les spies qui sont des mocks écrits à la main, alors que les mocks sont en général générés par une librairie de mock.\n\n\n2- Les stubs qui aident à émuler les interactions entrantes, c’est-à-dire le cas où une des dépendances fournit une valeur utilisée par le SUT.\nOn pourrait voir le stub comme la query du pattern CQS.\nIl existe des sous ensemble de stubs :\nle dummy qui est très simple\nle stub qui est plus sophistiqué, et retourne la bonne valeur en fonction du cas\net le fake qui est un stub utilisé pour remplacer un composant qui n’existe pas encore (typique de l’école de Londres).\n\n\n\n\n\n\nLe mot mock peut vouloir dire plusieurs choses, ici on l’utilise pour sa définition principale de sous ensemble de test double, mais parfois il est utilisé pour désigner tous les tests doubles, et parfois il désigne l’outil (la librairie qui permet de créer des mocks et des stubs).\nVérifier les interactions sur des stubs est un antipattern : les stubs émulent des données entrantes, et donc vérifier que le stub a bien été appelé relève du couplage à des détails d’implémentation.\nLes interactions ne doivent être vérifiées que sur des mocks, c’est-à-dire des interactions sortantes, dans le cas où l’appel qu’on vérifie a du sens d’un point de vue business.\n\n\nLa distinction entre comportement observable et détail d’implémentation :\nIl faut d’abord choisir le client qu’on considère, puis vérifier si notre code lui permet :\nSoit d’exécuter une opération pour faire un calcul ou un side effect pour atteindre ses objectifs.\nSoit d’utiliser un état pour atteindre ses objectifs.\n\n\nSi oui, alors on a un comportement observable, si non alors notre code est un détail d’implémentation.\nLe choix du client considéré est important, on reviendra sur cet aspect dans la suite.\n\n\nSi l’API publique coïncide avec le comportement observable, alors on dira que notre système est bien conçu.\nSinon, on dira qu’il fait fuiter des détails d’implémentation. Parce que des détails d’implémentation pourront alors être accédés de manière publique sans protection (sans encapsulation).\nExemple : Le cas où le renommage de l’utilisateur se faisait en deux temps : renommer, puis appeler la fonction de normalisation qui coupe le nom à 50 caractères max. Ici la fonction de normalisation ne permet d’atteindre aucun objectif du client qui l’appelle (il voulait juste renommer), pourtant elle est publique. On a donc un problème de fuite.\nUn bon moyen de savoir si on fait fuiter des détails d’implémentation, c’est de voir les cas où on a besoin de plus d’une opération pour atteindre un objectif du client (le “act” du test).\n\n\n\n\nL’architecture hexagonale consiste en plusieurs hexagones communiquant entre eux.\nChaque hexagone est constitué de deux couches :\nLe domain layer qui n’a accès qu’à lui-même et qui contient les règles et invariants business de l’application.\nIl est une collection de domain knowledge (how-to).\n\n\nL’application service layer qui orchestre la communication entre le domain layer et le monde externe. Elle instancie des classes importées du domain layer, leur donne les données qu’elle va chercher en base, les sauve à nouveau en base, répond au client etc.\nElle est est une collection de use-cases business (what-to).\n\n\n\n\nLe terme hexagone est une image, chaque face représente une connexion à un autre hexagone, mais le nombre n’a pas besoin d’être 6.\nAu sein de chaque couche, le client est la couche d'au-dessus, et donc ce sont ses objectifs qui sont pris en compte pour savoir si on lui expose des détails d’implémentation ou non.\nLes objectifs du client final sont transcrits en objectifs secondaires dans la couche du dessous, et donc on a une relation fractale qui permet à tous les tests d’avoir toujours un rapport avec un requirement business (Les objectifs de l’application service layer sont des sous-objectifs du client final).\nNDLR : un peu comme les OKR.\n\n\n\n\nExemple :\n// domain layer\nclass User {\nsetName(newName: string) {\n// On normalise et on set la valeur.\n}\n}\n// application service layer\nclass UserController {\nrenameUser(userId: number, newName: string) {\nconst user: User = getUserFromDatabase(userId);\nuser.setName(newName);\nsaveUserToDatabase(user);\n}\n}\n\n\n\nPour savoir quand utiliser les mocks sans abimer la résistance au refactoring, il faut se demander si l’interaction sortante qu’on veut vérifier est interne à notre application (notre hexagone par exemple), ou porte vers des systèmes externes.\nSi l’interaction est interne, alors il ne faut pas mocker, même s'il s’agit d’une dépendance out-of-process comme une base de données. Tant qu’elle n’est visible que depuis notre application, elle est un détail d’implémentation pour nos clients.\nSi l’interaction est externe, et donc visible par nos clients externes, alors il faut vérifier qu’elle se fait correctement par un mock. Par exemple, l'envoi d’un email répond à un besoin client, donc il faut vérifier que l’appel vers le système externe se fait correctement.\nPour parler un peu des écoles : l’école de Londres préconise de mocker toutes les dépendances mutables, ça fait beaucoup trop de mocks. Mais l’école classique préconise de mocker aussi des choses en trop : typiquement la base de données qui est une shared dependency. On peut au lieu de mocker nos interactions avec elle, la remplacer intelligemment par autre choses dans nos tests (cf. les deux prochains chapitres).","6---styles-of-unit-testing#6 - Styles of unit testing":"Il y a 3 “styles” de tests :\nOutput-based : c’est quand il n’y a pas de side effect, et qu’on teste une fonction qui prend des paramètres, et renvoie quelque chose. Il s’agit de fonction pure, donc de programmation fonctionnelle.\nState-based : on fait une opération, et on vérifie l’état d’un objet.\nCommunication-based : on utilise des mocks pour vérifier qu’un appel à une fonction a été fait avec les bons paramètres.\n\n\nA propos des écoles de test :\nL’école classique préfère le state-based plutôt que la communication based.\nL’école de Londres fait le choix inverse.\nEt toutes les deux utilisent l’output-based testing quand c’est possible.\n\n\nOn peut comparer les 3 styles de test vis-à-vis des 4 critères d’un bon test :\nPour la protection contre les régressions et la rapidité de feedback les 3 styles se valent à peu près.\nConcernant la résistance au refactoring :\nL’output-based testing offre la meilleure résistance parce que la fonction se suffit à elle-même.\nLe state-based testing est un peu moins résistant parce que l’API publique exposée est plus importante, et donc les chances de faire fuiter des détails d’implémentation dans la partie publique sont plus grandes.\nLe communication-based testing est le plus fragile, et nécessite une grande rigueur pour ne pas coupler à des détails d’implémentation.\n\n\nConcernant la maintenabilité : c’est à nouveau l’output-based qui est le plus maintenable parce que prenant le moins de place, suivi du state-based, et enfin du communication-based qui prend beaucoup de place avec ses mocks et stubs.\nGlobalement l’output-based testing est le meilleur, mais il nécessite d’avoir du code écrit de manière fonctionnelle.\n\n\nA propos de la programmation fonctionnelle, l’auteur conseille les livres de Scott Wlaschin.\nPour pouvoir faire de l’output-based testing, il faut écrire du code avec des fonctions pures, c’est-à-dire qui renvoient le même résultat à chaque fois qu’on donne les mêmes paramètres, sans qu’il n’y ait d’inputs ou d’outputs cachés.\nParmi ces choses cachées, on a :\nLes side-effects : des outputs cachés, par exemple la modification d’un état d’une classe, l’écriture dans un fichier etc.\nLes exceptions : elles créent un chemin alternatif à celui de la fonction, et peuvent être traitées n’importe où dans la stack d’appel.\nLa référence à un état interne ou externe : un input caché qui va permettre de récupérer une valeur qui n’est pas indiquée dans la signature de la fonction.\n\n\nPour savoir si on a une fonction pure, on peut essayer de remplacer son appel par la valeur qu’elle devrait renvoyer, et vérifier que le programme ne change pas de comportement. Si oui on a une referential transparency.\n\n\nL’architecture fonctionnelle consiste à maximiser la quantité de code écrite de manière fonctionnelle (fonctions pures, avec valeurs immutables), et confiner le code qui fait les side-effects à un endroit bien précis.\n1- Il y a le code qui prend les décisions, qui est sous forme de fonctions pures. C’est le functional core.\n2- Et le code qui agit suite aux décisions, qui prend les inputs et crée les side-effects (UI, DB, message bus etc.). C’est le mutable shell.\nOn va couvrir le functional core par de nombreux tests unitaires output-based, et couvrir le mutable shell qui est la couche d’au-dessus par des tests d’intégration moins nombreux.\nL’architecture fonctionnelle est en fait un cas particulier de l’architecture hexagonale :\nLes deux ont bien deux couches organisées par inversion de dépendance.\nLa différence principale c’est que l’architecture fonctionnelle exclut tout side-effect du functional core vers le mutable shell, alors que l’architecture hexagonale permet les side-effects dans la couche domaine tant que ça n’agit pas au-delà de cette couche (DB par exemple).\n\n\n\n\nExemple d’application peu testable, refactorée vers la functional architecture :\nDescription :\nOn a un système d’audit qui enregistre tous les visiteurs d’une organisation.\nLe nom de chaque visiteur et la date sont ajoutés à un fichier de log.\nQuand le nombre de lignes max du fichier est atteint, on écrit dans un autre fichier.\n\n\nInitialement la classe AuditManager a une méthode addRecord() qui va lire les fichiers existants, les classer pour trouver le dernier. Puis vérifier s’il est plein pour soit écrire dedans, soit écrire dans dans un nouveau.\nLa logique et la lecture/écriture sont dans la même fonction. Donc les tests vont être à la fois lents, et difficiles à paralléliser à cause de la dépendance out-of-process partagée qu’est le filesystem.\npublic class AuditManager {\nconstructor(\npublic maxEntriesPerFile: number,\npublic directoryName: string\n) {}\n\naddRecord(visitorName: string, timeOfVisit: Date) {\n// Get all files in the given directory\nconst fs = require('fs');\nconst files = fs.readdirSync(directoryName);\n// Build the record content\n\n// If no file, create one with our record\nfs.writeFile(...\n\n// Sort by file name to get the last one\n// If file's lines do no exceed max, write inside\n// Otherwise create a new file and write inside\n}\n}\n\n\n\nUne 1ère étape est d’utiliser des mocks pour découpler le filesystem de la logique :\nUne des manières de faire ça c’est d’injecter un objet qui respecte une interface IFileSystem, qui sera soit le vrai filesystem, soit un mock dans les tests.\nLe mock va à la fois servir de stub pour renvoyer le contenu des fichiers, et aussi de mock pour vérifier qu’on appelle bien la bonne fonction avec les bons paramètres pour écrire dans le filesystem. L’usage du mock ici est légitime parce que ces fichiers sont user-facing.\npublic class AuditManager {\nconstructor(\npublic maxEntriesPerFile: number,\npublic directoryName: string,\npublic fileSystem: IFileSystem,\n) {}\n\naddRecord(visitorName: string, timeOfVisit: Date) {\nconst files = fileSystem.readdirSync(directoryName);\n// Build the record content\n\n// If no file, create one with our record\nfileSystem.writeFile(...\n\n// Sort by file name to get the last one\n// If file's lines do no exceed max, write inside\n// Otherwise create a new file and write inside\n}\n}\n\nit(\"creates a new file when the current file overflows\", () => {\nconst fileSystemMock: IFileSystem = {\nreaddirSync: () => [\"audits/audit_1.txt\", \"audits/audit_2.txt\"],\nwriteFile: jest.fn(),\n// ...\n};\nconst sut = AuditManager(3, \"audits\", fileSystemMock);\n\nsut.addRecord(\"Alice\", new Date(\"2019-04-06\"));\n\nexpect(fileSystemMock.writeFile).toHaveBeenCalledTimes(1);\nexpect(fileSystemMock.writeFile).toHaveBeenCalledWith(\n\"audits/audit_3.txt\",\n\"Alice; 2019-04-06\"\n);\n});\n\nOn n’a rien changé à la protection contre les régressions et à la résistance aux refactorings. Par contre on a rendu les tests plus rapides, et on a un peu amélioré la maintenabilité parce qu’on n’a plus à se préoccuper du filesystem. Mais le setup des mocks est verbeux, on peut faire mieux sur la maintenabilité.\n\n\nLa 2ème étape est de refactorer vers la functional architecture :\nAuditManager ne connaît plus du tout l'existence du filesystem : il reçoit des valeurs en entrée (une liste de FileContent à partir duquel il lira le contenu des fichiers), et renvoie des valeurs en sortie : une liste de FileUpdate qui contiendront les contenus à changer).\nclass AuditManager {\nconstructor(public maxEntriesPerFile: number) {}\n\naddRecord(\nfiles: FileContent[],\nvisitorName: string,\ntimeOfVisit: Date\n) {\n// Build the record content\n// If no file, create one with our record\nif (files.length === 0) {\nreturn new FileUpdate(\"audit_1.txt\", newRecord);\n}\n\n// Sort by file name to get the last one\n// If file's lines do no exceed max, write inside\n// Otherwise create a new file and write inside\n}\n}\n\nclass FileContent {\nconstructor(public fileName: string, public lines: string[]) {}\n}\nclass FileUpdate {\nconstructor(public fileName: string, public newContent: string) {}\n}\n\nOn a une classe Persister qui va permettre de lire tous les fichiers, et de renvoyer leurs informations sous forme de FileContent, et une autre méthode pour prendre une liste de FileUpdate, et les appliquer sur le filesystem. Il doit être le plus simple possible pour que le max de logique soit dans AuditManager.\nconst fs = require(\"fs\");\n\nclass Persister {\nreadDirectory(directoryName: string): FileContent[] {\nreturn fs.readdirSync(directoryName).map((file) => {\nreturn new FileContent(file.name, file.lines);\n});\n}\n\napplyUpdate(filePath: string, update: FileUpdate) {\nfs.writeFile(filePath, update);\n}\n}\n\nPour faire fonctionner ensemble le functional core (AuditManager), et le mutable shell (Persister), on a besoin d’une autre classe de type “application service” (pour utiliser la terminologie de l’hexagonal architecture).\nIl va manipuler manipuler Persister pour obtenir les données des fichiers, les donner à une instance d’AuditManager, puis appeler la méthode de calcul sur AuditManager, récupérer les commandes d’écriture en sortie, et les donner à Persister pour mettre à jour le filesystem.\nclass ApplicationService {\nconstructor(public directoryName: string, maxEntriesPerFile: number) {\nthis.auditManager = new AuditManager(maxEntriesPerFIle);\nthis.persister = new Persister();\n}\n\naddRecord(visitorName: string, timeOfVisit: Date) {\nconst files: FileContent[] = this.persister.readDirectory(\nthis.directoryName\n);\nconst update: FileUpdate = this.auditManager.addRecord(\nfiles,\nvisitorName,\ntimeOfVisit\n);\nthis.persister.applyUpdate(this.directoryName, update);\n}\n}\n\n\n\nOn a gardé les précédents avantages, et on a amélioré la maintenabilité en éliminant le setup de mocks verbeux, remplacés par la simple instanciation de valeurs mis dans les objets FileContent et FileUpdate.\nit(\"creates a new file when the current file overflows\", () => {\nconst sut = new AuditManager(3);\nconst files = [\nnew FileContent(\"audits/audit_1.txt\", []),\nnew FileContent(\"audits/audit_1.txt\", [\n\"Peter; 2019-04-06\",\n\"Jane; 2019-04-06\",\n\"Jack; 2019-04-06\",\n]),\n];\n\nconst update = sut.addRecord(files, \"Alice\", new Date(\"2019-04-06\"));\n\nexpect(update.fileName).toBe(\"audit_3.txt\");\nexpect(update.newContent).toBe(\"Alice; 2019-04-06\");\n});\n\nPour rester sur du fonctionnel, on peut renvoyer les erreurs par valeur de retour, et décider de quoi en faire dans l’application service.\n\n\n\n\nLa functional architecture n’est pas toujours applicable.\nElle permet d’avoir des avantages en termes de maintenabilité du code et des tests, mais elle a des désavantages :\nLe code pourra être un peu plus gros pour permettre la séparation entre logique et side effects.\nLe code pourra souffrir de problèmes de performance.\nDans notre cas, ça a marché parce qu’on lisait tous les fichiers avant d’appeler la logique en donnant tous ces contenus et la laissant décider. Si on avait voulu n’en lire que certains en fonction de paramètres décidés par la logique, on n’aurait pas pu la garder comme fonction pure.\nUne autre solution aurait pu être de concéder un peu de centralisation de la logique dans le core en faveur de la performance, en laissant la décision de charger les données ou non à l’application service.\n\n\n\n\nIl faut donc appliquer la functional architecture stratégiquement.\nNe pas sacrifier la performance si elle est importante dans le projet.\nL’appliquer si le projet est censé durer dans le long terme, et que l’investissement initial de séparer en vaut la peine.\n\n\n\n\nEn général (surtout si on fait de l’OOP), on aura une combinaison de tests state-based et output-based, et quelques tests communication-based.\nLe conseil ici c’est de privilégier les tests output-based quand c’est raisonnablement possible.","7---refactoring-toward-valuable-unit-tests#7 - Refactoring toward valuable unit tests":"Les tests et le code sont profondément liés, il est impossible d’obtenir de bons tests avec du mauvais code.\nOn va catégoriser le code en 4 catégories, en fonction de 2 axes :\nL’axe de complexité ou d’importance vis-à-vis du domaine.\nLa complexité cyclomatique est définie par le nombre de branches possibles dans le code : 1 + le nombre de branches.\nLe calcul tient compte du nombre de prédicats dans les conditions : si notre if vérifie 2 prédicats, ça ajoute 2 points.\n\n\nL’importance vis-à-vis du domaine c’est la connexion du code avec le besoin de l’utilisateur final. Du code utilitaire ne rentrera pas là-dedans.\nC’est la complexité ou l’importance domaine. Un code signifiant du point de vue du domaine mais simple rentre dans la description.\n\n\nL’axe du nombre de collaborators impliqués.\nPour rappel un collaborator est une dépendance, qui est soit mutable (autre chose que des valeurs primitives et des value objects), soit out-of-process.\n\n\n\n\nLes 4 catégories de code sont :\nDomain models and algorithms : grande valeur sur l’axe de complexité, faible valeur sur l’axe des collaborators.\nC’est eux qu’il faut le plus tester, à la fois parce qu’ils sont faciles à tester et parce qu’on obtiendra une grande résistance aux régressions. C’est d’eux qu’on obtient le meilleur “retour sur investissement” de nos tests.\n\n\nTrivial code : faible valeur sur les deux axes.\nC’est du code simple, qui ne mérite pas de tests.\n\n\nControllers : faible valeur sur l’axe de complexité, grande valeur sur l’axe des collaborators.\nIl s’agit de code pas complexe mais qui coordonne le code complexe ou important.\nOn peut les tester avec des tests d’intégration qui seront beaucoup moins nombreux que les unit tests des domain models and algorithms.\n\n\nOvercomplicated code : grande valeur sur les deux axes.\nLà on est embêté : c’est à la fois du code qu’on ne peut pas se permettre de ne pas tester, et du code difficile à tester. Par exemple des fat controllers qui font tout eux-mêmes.\nOn va chercher à se débarrasser de ce code en le découpant, pour obtenir du code qui score beaucoup sur l’un des axes mais jamais les deux.\n\n\n\n\nLe Humble Object Pattern va nous permettre de découpler la logique de la partie difficile à tester (par difficile on entend code asynchrone/multi-thread, UI, dépendances out-of-process etc.).\nLe test va tester la partie logique complexe/métier directement.\nLe humble object est une fine couche avec très peu de logique, qui va lier la logique et la dépendance qui pose problème dans les tests.\nIl s’agit de dire qu’un code doit soit avoir une grande complexité (domain layer and algorithms), soit travailler avec beaucoup de dépendances (controllers), mais jamais les deux.\nExemples :\nLa functional et l’hexagonal architecture utilisent le humble object pattern.\nOn peut aussi mettre dans cette catégorie les patterns MVC et MVP qui séparent la logique (le modèle) de la UI (view), avec le humble object (le presenter ou le controller).\nL’aggregate du DDD est aussi un exemple : on groupe les classes dans des clusters (les aggregates) où elles auront une forte connectivité, et les clusters auront une faible connectivité entre eux. Ca permet de faciliter la testabilité en ayant besoin d’instancier essentiellement les collaborators du cluster concerné.\nNDLR : que l’aggregate permette d’améliorer la testabilité ou la maintenabilité OK, mais j’arrive pas à voir le rapport avec le humble object pattern ici. On n’a pas de hard-to-test dependency.\n\n\n\n\n\n\nExemple d’application avec du code overcomplicated, refactorée vers du humble object pattern :\nDescription :\nOn a un CRM qui gère les utilisateurs, et les stocke en DB.\nLa seule fonctionnalité dispo c’est le changement d’email : si le nom de domaine du nouvel email appartient à l’entreprise le user est un employé, sinon il devient un customer.\nEn fonction des emails des users, et donc de leur statut, le nombre d’employés est calculé et mis en base.\nQuand le changement d’email est fait, on doit envoyer un message dans un message bus.\n\n\nLa 1ère implémentation contient une classe User, avec une méthode changeEmail() qui calcule le nouveau statut du user, et sauve son email en base, mais aussi recalcule et sauve le nouveau nombre d’employés dans la table de l’entreprise. Elle envoie aussi le message dans le message bus.\nclass User {\nconstructor(\nprivate userId: number,\nprivate email: Email,\nprivate type: UserType\n) {}\n\nchangeEmail(userId: number, newEmail: Email) {\n// Get user data from database\n// If new email is same as before, return\n// Get company data from database\n// Check whether the email is corporate\n// Set the user type accordingly\n// If the type is different, update company number\n// of employees.\n// Save user info in database\n// Save company info in database\n// Send message to message bus\n}\n}\n\nNotre méthode changeEmail() fait des choses importantes du point de vue domaine, mais en même temps elle a deux collaborators out-of-process (la DB et le message bus), ce qui est un no-go pour du code compliqué ou avec importance domaine.\nOn est en présence du pattern Active Record : la classe domaine se query et se persiste en DB directement. C’est OK pour du code simple, mais pas pour du code qui va croître sur le long terme.\n\n\nPossibilité 1 : rendre explicites les dépendances implicites, en donnant l’objet de DB et message bus en paramètre (ce qui permettra de les mocker dans les tests).\nQue les dépendances soient directes ou via une interface, ça ne change rien au statut du code : il reste overcomplicated.\nOn va devoir mettre en place une mécanique de mocks complexe pour les tests. On peut trouver plus clean que ça.\n\n\nPossibilité 2, étape 1 : introduire un application service (humble object) qu’on appelle UserController pour prendre la responsabilité de la communication avec les dépendances out-of-process.\nLa nouvelle classe va chercher les informations du user et de l’entreprise en DB, crée un objet User avec ces infos. Puis elle appelle user.changeEmail(), et enfin sauve les données du user et de l’entreprise en DB, et envoie l’event d’email changé dans le message bus.\nclass UserController {\nconstructor(\nprivate database: Database,\nprivate messageBus: MessageBus\n) {}\n\nchangeEmail(userId: number, newEmail: Email) {\n// Get user data from database\n// Get company data from database\n\nconst user = new User(userId, email, type);\nconst numberOfEmployees = user.changeEmail(\nnewEmail,\ncompanyDomainName,\nnumberOfEmployees\n);\n\n// Save user info in database\n// Save company info in database\n// Send message to message bus\n}\n}\n\npublic class User {\n// ...\nchangeEmail(\nnewEmail: Email,\ncompanyDomainName: string,\nnumberOfEmployees: number\n) {\n// If new email is same as before, return\n\n// Check whether the email is corporate\n// Set the user type accordingly\nconst newType = ...\n\n// If the type is different, update company number\n// of employees.\nnumberOfEmployees = ...\n\nthis.email = newEmail;\nthis.type = newType;\n\nreturn numberOfEmployees;\n}\n}\n\nProblèmes :\nOn a une logique complexe dans le fait de reconstruire les données à partir de la base de données (le mapping), c’est le travail d’un ORM.\nL’event de changement d’email est envoyé systématiquement, même si l’email n’a pas été changé.\nOn a un petit code smell : la méthode user.changeEmail() prend le nombre d’employés en paramètre, et renvoie le nouveau nombre d’employés. Ça n'a rien à voir avec un user donné.\n\n\nMais au moins la classe User a perdu ses collaborators, elle est donc en l’état purement fonctionnelle. On va pouvoir la tester à fond facilement.\n\n\nÉtape 2 : enlever de la complexité de l’application service.\nPour faire le mapping entre les données de la DB et un objet en mémoire, on va soit utiliser un ORM, soit créer nous-mêmes un objet de type factory qui va renvoyer notre User.\nCette logique a l’air simple avec peu de branches apparentes, mais il faut prendre en compte les branches cachés liés aux dépendances : on fait des conversions de type, on va chercher des objets inconnus dans un tableau un à un etc. beaucoup de choses peuvent mal aller dans ce processus.\n\n\npublic class UserFactory {\ncreate(data: Record&lt;any, any>) {\nPrecondition.requires(data.length >= 3);\n\nconst id = data[0];\nconst email = data[1];\nconst type = data[2];\n\nreturn new User(id, email, type);\n}\n}\n\nOn a ici du code utilitaire complexe.\n\n\nA la fin de l’étape on a bien User qui est dans la case “domain models and algorithms”, et UserController qui est dans la case “Controllers”. Il n’y a plus d’overcomplicated code.\n\n\nÉtape 3 : on introduit une nouvelle classe domaine Company.\nNotre nouvelle classe domaine Company peut récupérer la logique de calcul du nombre d’employés qu’on sort de User.\nOn a donc UserController qui crée les deux objets de domaine à partir des données de la DB, et qui appelle user.changeEmail() en donnant l’instance company en paramètre.\nOn a un principe important d’encapsulation OO ici : tell, don’t ask. Le user va dire (tell) à l’instance de company de mettre à jour elle-même son nombre d’employés, plutôt que lui demander (ask) ses données brutes et faisant l’opération à sa place.\n\n\nuser.changeEmail() n’est plus une fonction pure puisqu’elle a un collaborator (company), mais vu qu’il n’y en a qu’un et qu’il n’est pas out-of-process, c’est raisonnable.\nOn va donc devoir faire du state-based testing, l’output-based étant possible qu’avec des fonctions pures.\n\n\nclass Company {\nconstructor(\nprivate domainName: string,\nprivate numberOfEmployees: number\n) {}\n\nchangeNumberOfEmployees(delta: number) {\nPrecondition.requires(this.nomberOfEmployees + delta >= 0);\nthis.numberOfEmployees += delta;\n}\n\nisEmailCorporate(email: Email) {\n// Get the domain part from the email\n// and return whether it is equal to this.domainName\n}\n}\n\nclass User {\n// ...\nchangeEmail(newEmail: Email, company: Company) {\nif (newEmail === this.email) return;\n\nconst newType = company.isEmailCorporate(newEmail)\n? Usertype.Employee\n: Usertype.Customer;\n\n// If the type is different, update company number\n// of employees.\ncompany.changeNumberOfEmployees(delta);\n\nthis.email = newEmail;\nthis.type = newType;\n}\n}\n\nclass UserController {\nconstructor(\nprivate database: Database,\nprivate messageBus: MessageBus\n) {}\n\nchangeEmail(userId: number, newEmail: Email) {\nconst userData = this.database.getUserById(userId);\nconst user = UserFactory.create(userData);\nconst companyData = this.database.getCompany();\nconst company = CompanyFactory.create(companyData);\n\nuser.changeEmail(newEmail, company);\n\nthis.database.saveCompany(company);\nthis.database.saveUser(user);\nthis.messageBus.sendEmailChangedMessage(userId, newEmail);\n}\n}\n\nA la fin, user et company sont sauvés en DB, et l’event est envoyé dans le message bus par UserController.\n\n\n\n\nComment tester notre exemple refactoré ?\nLe code des classes domaine (User et Company), et le code utilitaire complexe (factory si on n’a pas utilisé d’ORM) peuvent être unit testés à fond.\nExemple : \"changement d'email de corporate à non corporate\", \"changement d'email de non corporate à corporate\", \"changement d'email au même email\" etc.\nit(\"changes email from non corporate to corporate\", () => {\nconst company = new Company(\"mycorp.com\", 1);\nconst sut = new User(\"user@gmail.com\", UserType.Customer);\n\nsut.changeEmail(\"new@mycorp.com\", company);\n\nexpect(company.numberOfEmployees).toBe(2);\nexpect(sut.email).toBe(\"new@mycorp.com\");\nexpect(sut.userType).toEqual(UserType.Employee);\n});\n\n\n\nLes méthodes ultra simples comme le constructeur de User n’ont pas à être testées.\nLe controller doit être testé avec des tests d’intégration moins nombreux. Ce sera l’objet des prochains chapitres.\nLes pré-conditions sont des checks qui permettent de throw une exception tôt si une incohérence est détectée, pour éviter des problèmes plus importants.\nCes pré-conditions doivent être testées seulement si elles ont un lien avec le domaine, sinon c’est pas la peine.\nExemple de pré-condition qu’on teste : la méthode qui permet de mettre à jour le nombre d’employés sur Company throw si le nombre souhaité est inférieur à 0.\nExemple de pré-condition à ne pas tester : notre user factory vérifie que les données venant de la base ont bien 3 éléments avant de reconstruire le user. Cette vérification n’a pas de sens d’un point de vue domaine.\n\n\n\n\nNotre découpage domaine/controller marche bien parce qu’on récupère l’ensemble des données upfront, et les sauve à la fin en base inconditionnellement dans le controller. Mais que faire si on a besoin d’accès à des données seulement dans certains cas dictés par la logique ?\nIl y a des trade-offs à faire en fonction de :\nla testabilité du code du domaine\nla simplicité du code du controller\nla performance\n\n\nOn a 3 possibilités :\nGarder toute la logique dans le domaine, et toute l’interaction avec les deps out-of-process dans le controller.\nDans ce cas on va avoir une moins bonne performance, puisqu’on fera la lecture de la donnée dont on n’aura peut-être pas besoin à l’avance systématiquement. Le controller n’ayant pas la connaissance de si on a besoin ou non, il prend la donnée et la donne tout le temps.\nPar contre on a un code de domaine testable, et un controller simple.\n\n\nInjecter les dépendances out-of-process dans le domaine, et laisser le code business décider quand récupérer ou non les données.\nLe souci ici c’est la maintenabilité des tests du domaine, avec soit des tests lents à travers la DB, soit des mocks compliqués à maintenir.\nPar contre on a un controller simple, et de la performance.\n\n\nDécouper le processus de décision en plusieurs parties.\nLe controller va appeler la 1ère partie, récupérer les données, puis décider lui-même s’il faut faire la deuxième partie. Si oui il récupère les données additionnelles depuis la DB, et exécute la 2ème partie. Et à la fin comme d’habitude sauve le tout en DB.\nUne partie de la logique risque de fuiter du domaine vers le controller et rendre le controller plus compliqué.\nPar contre on a le code du domaine testable, et on garde la performance.\n\n\n\n\nLa plupart du temps, céder sur la performance n’est pas possible.\nIl nous reste donc les 2 dernières possibilités.\nL’auteur conseille de privilégier la séparation du processus de décision plutôt que l’injection des dépendances out-of-process dans le domaine. On peut gérer la fuite de la logique vers le controller et la complexification du code du domaine avec certaines techniques.\n\n\n\n\nUne de ces techniques est le pattern CanExecute / Execute.\nImaginons qu’on veuille mettre à jour l’email du user seulement si son compte n’est pas encore confirmé.\n1ère possibilité : on query les infos upfront, on donne tout à user, et le user décide de changer ou non l’email. Mais on a peut être récupéré les infos de la company pour rien, si le user était déjà confirmé => problème de performance.\n2ème possibilité : le controller vérifie lui-même si le compte du user est confirmé avant de faire éventuellement la query des infos de la company. Ici le controller a récupéré une partie de la logique chez lui.\n3ème possibilité (CanExecute / Execute) : le user expose une méthode canChangeEmail() qui encapsule la logique de prise de décision. Le controller n’a plus qu’à l’appeler pour décider si on passe à l’étape suivante ou non. La décision ne se fait plus vraiment au niveau du controller.\n// controller\nconst canChangeEmail = user.canChangeEmail();\nif (!canChangeEmail) {\nreturn;\n}\nuser.changeEmail(newEmail, company);\n\nPour s’assurer que le controller n’a d’autre choix que d’appeler cette méthode avant d’aller plus loin (et donc lui retirer de la responsabilité), on va mettre une pré-condition dans la méthode user.changeEmail(), où on appelle explicitement canChangeEmail() en vérifiant que la réponse est oui. Et cette pré-condition métier sera testée (contrairement à l’appel à canChangeEmail() dans le controller).\n// user\npublic canChangeEmail() {\nreturn this.isEmailConfirmed ? false: true;\n}\npublic changeEmail(newEmail: Email, company: Company) {\nPrecondition.requires(this.canChangeEmail());\n// [...]\n}\n\n\n\n\n\n\n\nVoici une autre de ces techniques concerne l’envoi de domain events :\nOn parle bien ici des domain events au sens DDD, ces events permettent d’informer les autres composants du système des étapes importantes qui ont lieu dans nos objets domaine.\nSi on revient à notre exemple de CRM, au moment du changement d’email du user, le controller envoie un message dans un message bus. Mais cet envoi est fait dans tous les cas, même si le changement n’a pas eu lieu. On veut l’envoyer seulement si le changement est fait.\nPour enlever la décision d’envoyer ou non l’event du controller, et la mettre dans le domaine, on va créer une liste d‘events qu’on met à l’intérieur de la classe domaine.\nOn a un event :\nclass EmailChanged {\npublic userId: number;\npublic newEmail: Email;\n}\n\nLe User crée l’event si l’envoi est confirmé :\npublic changeEmail(newEmail: Email, company: Company) {\n// [...]\nthis.emailChangedEvents.push(new EmailChanged(userId, newEmail);\n}\n\nEt le Controller va itérer sur les domain events de User pour envoyer les bons messages dans le message bus :\npublic changeEmail(userId: int, newEmail: Email) {\n// [...]\nuser.changeEmail(newEmail, company);\n// [...]\nuser.emailChangedEvents.forEach((event) =>\nthis.messageBus.sendEmailChangedMessage(\nevent.userId,\nevent.newEmail\n);\n);\n}\n\nOn va donc pouvoir unit tester la création de chaque domain event dans chaque cas dans le user, et on fera beaucoup moins d’integration tests pour vérifier que le controller lit bien les events du user et envoie ce qu’il faut.\n\n\nDans des projets plus gros, on pourrait vouloir fusionner les events avant de les dispatcher, cf. Merging domain events before dispatching.\n\n\nPour ce qui est de l’envoi de l’email, c’est un comportement observable de l’extérieur donc il doit être fait que si l’email est changé. Par contre, l'écriture en DB peut être faite inconditionnellement parce qu’elle est privée et que le résultat ne changera pas.\nOn a un petit souci de performance à écrire en DB si l’email n’a pas changé, mais c’est un cas plutôt rare.\nOn peut aussi le mitiger par le fait que la plupart des ORM n’iront pas écrire en DB si l’objet n’a pas changé. Donc on peut faire l’appel sans crainte.\n\n\nLe conseil général de Vladimir est de ne jamais introduire de dépendances out-of-process (même mockées dans les tests) dans le code du domaine. Il conseille plutôt de fragmenter les appels au domaine, et au pire mettre ce code dans le controller et le tester par des tests d’intégration.\nLes cas dans lesquels on va devoir mettre la logique dans le controller peuvent être par exemple :\nVérifier qu’un email est unique (il faut faire un appel out-of-process pour ça).\nGérer les cas d’erreur liés aux appels out-of-process.\n\n\n\n\nA propos de qui est le client de qui et de la notion de détail d‘implémentation :\nAu niveau du controller, le client c’est l’utilisateur final, donc il faut tester ou mocker ce qui lui est visible ou sert directement son but. Les appels qui sont faits vers le domaine sont un détail d'implémentation.\nAu niveau du domaine, le client c’est le contrôler, donc il faut unit tester ce qui sert directement son but. Les appels éventuels vers d’autres classes du domaine sont des détails d’implémentation qu’on n’a pas à mocker.","iii---integration-testing#III - Integration testing":"","8---why-integration-testing#8 - Why integration testing?":"Pour rappel, un test d’intégration est un test qui ne répond pas à au moins un des 3 critères des tests unitaires : vérifier une unité de comportement, le faire vite, le faire en isolation par rapport aux autres tests.\nEn pratique les tests d’intégration vont être ceux qui gèrent la relation avec les dépendances out-of-process.\nOn est donc dans la partie “controllers” en termes de type de code.\n\n\nLes règles de la pyramide des tests sont de :\nCouvrir le maximum de cas par des tests unitaires.\nTester un happy path, ainsi que les edge cases qui ne peuvent pas être couverts par les tests unitaires avec des tests d’intégration.\nQuand la logique est simple, on a moins de tests unitaires, mais les tests d’intégration gardent leur valeur.\n\n\nQuand un edge case amène à un crash immédiat, il n’y a pas besoin de le tester avec un test d’intégration.\nExemple du pattern CanExecute/Execute.\nOn appelle ce principe le Fail Fast principle.\nOn reste dans l’esprit coût/bénéfice pour la maintenance d’un test, dans ce cas le bénéfice n’est pas suffisant parce que ce genre de cas ne mène pas à de la corruption de données, et est rapide à remarquer et à fixer.\n\n\nIl y a 2 manières de tester les dépendances out-of-process : les tester directement ou les remplacer par des mocks.\nOn peut classer ces dépendances en deux catégories :\nLes managed dependencies sont celles que seuls nous utilisons, et que le monde externe ne connaît pas. Exemple typique : la base de données.\nCes dépendances sont considérées comme des détails d’implémentation.\nOn n’a donc pas à se préoccuper de nos interactions avec elles, ce qui compte c’est leur état final, et l’impact que ça aura sur le résultat observable. Donc pas besoin de mock.\n\n\nLes unmanaged dependencies sont celles qui sont observables de l’extérieur. Exemple typique : un serveur SMTP dont les mails seront visibles par les clients finaux, ou encore un message bus dont les messages vont affecter des composants externes à notre système.\nCes dépendances sont considérées comme faisant partie du comportement observable.\nPuisque les unmanaged dependencies sont observables, ils font partie de l’API publique, et donc il faut nous assurer que nos interactions avec elles restent les mêmes : les mocks sont parfaits pour ça.\n\n\nIl peut arriver qu’une dépendance soit à la fois managed et unmanaged : par exemple une base de données dont on choisit de partager certaines tables publiquement avec un composant externe.\nPartager une DB est en général une mauvaise idée parce que ça va nous coupler fortement, il vaut mieux passer par une API synchrone ou un message bus.\nCeci dit, si ça arrive, il faudra différencier les tables partagées des tables privées, et traiter chacune comme ce qu’elle est (managed/unmanaged) : des mocks pour assurer l’ensemble de nos interactions avec les tables partagées, et la vérification de l’état final seulement pour les tables privées.\n\n\nDans le cas où on n’aurait pas la possibilité de tester en intégration une DB privée (base legacy trop grosse, trop coûteuse, raisons de sécurité etc.), l’auteur conseille de ne pas écrire de tests d’intégration du tout pour celles-ci, et de se concentrer sur les unit tests.\nLa raison est que ça compromet la résistance aux refactorings en traitant une dépendance privée comme publique, et ça n’ajoute que très peu de protection contre des régressions en plus des unit tests. Le rapport coût/bénéfice n’est pas suffisant.\n\n\n\n\n\n\nSi on reprend l’exemple du CRM, pour écrire des tests d’intégration pour le UserController :\nOn va d’abord écrire un test pour couvrir le happy path le plus long. Ici ce serait le cas où on change l’email d’un user, qui passe de non corporate à corporate. On va mettre à jour en DB le user, les infos de company, et aussi envoyer le message dans le message bus pour l’email.\nIl n’y a qu’un edge case non couvert par des unit tests : le cas où l’email ne peut pas être changé. Mais dans ce cas on est sur du fail fast : une exception sera lancée et le programme s’arrêtera. Donc pas besoin de test d’intégration pour ça.\nA propos des tests end to end, on peut en faire quelques-uns pour notre projet, et leur faire traverser les scénarios les plus longs pour s’assurer que tout est bien branché. On vérifiera le résultat pour le client final au lieu de regarder dans la DB, et on vérifiera le message envoyé dans le message bus pour la dépendance externe à laquelle on n’a pas accès. Ici pour cette feature on choisit de ne pas en faire.\nConcernant notre test d’intégration de happy path donc, il faut d’abord décider de la manière dont on traite nos dépendances out-of-process : la DB est managed donc doit être testée au niveau de son état pour le user et la company, alors que le message bus est unmanaged donc doit être mocké pour tester les interactions avec lui.\nNotre test va contenir 3 sections :\nD’abord mettre le user et la company en DB et initialiser le mock pour le message bus. (Arrange)\nEnsuite appeler la méthode de notre controller. (Act)\nEt enfin tester le résultat en DB et l’interaction avec notre mock (Assert).\n\n\nit(\"changes email from non corporate to corporate\", () => {\n// Arrange\ndb.createCompany(\"mycorp.com\", 0);\ndb.createUser(1, \"user@gmail.com\", \"customer\");\nconst busMock = { send: jest.fn() };\nconst sut = new UserController(new MessageBus(busMock));\n\n// Act\nsut.changeEmail(1, \"user@mycorp.com\");\n\n// Assert\nconst user = db.getUserById(1);\nexpect(user.email).toBe(\"user@mycorp.com\");\nexpect(user.type).toBe(\"employee\");\n\nconst company = db.getCompany();\nexpect(company.numberOfEmployees).toBe(1);\n\nexpect(busMock.send).toHaveBeenCalledTimes(1);\nexpect(busMock.send).toHaveBeenCalledWith(expect.toInclude(\"1\"));\nexpect(busMock.send).toHaveBeenCalledWith(\nexpect.toInclude(\"user@mycorp.com\")\n);\n});\n\nIl ne faut pas utiliser les mêmes objets entre les sections, de manière à être sûr à chaque fois de lire et écrire depuis la DB.\n\n\n\n\nL’introduction d’interfaces prématurées est une mauvaise idée. Il faut en introduire une quand elle existe déjà mais est implicite, c'est-à-dire quand il y a au moins deux implémentations de celle-ci.\nLe principe fondamental ici c’est YAGNI (you ain’t gonna need it) qui dit que le code supposément utile pour plus tard ne le sera sans doute pas, ou pas sous cette forme.\nPour plus d’info sur le trade off YAGNI vs OCP, l’auteur a fait un article.\nPar conséquent, étant donné qu’un mock est une implémentation de plus, il nous faudra la plupart du temps faire une interface seulement pour les unmanaged dependencies.\n\n\nQuelques bonnes pratiques pour les tests d’intégration :\nCréer une séparation explicite entre domain model et controllers permet de savoir quoi tester unitairement, et quoi tester en intégration.\nLimiter le nombre de couches à seulement 3 : infrastructure layer, domain layer et application service layer.\nDavid J. Wheeler a dit à ce propos : “All problems in computer science can be solved by another layer of abstraction, except for the problem of too many layers of abstraction.”\nOn se retrouve souvent avec 4, 5, 6 layers, ce qui rend l’ajout d’une feature, et même la compréhension d’une feature complexe parce qu’on doit toucher à de nombreux fichiers.\nOn a souvent tendance à tester le layer du dessous depuis le layer du dessus. Et avec de nombreux layers on aboutit à de nombreux tests avec mocks qui apportent chacun peu de valeur.\n\n\nÉliminer les dépendances circulaires : quand deux classes dépendent l’une de l’autre pour fonctionner.\nLes dépendances circulaires créent aussi une difficulté à appréhender le code parce qu’on ne sait pas par où commencer.\nPar exemple, quand une classe en instancie une autre et lui passe une instance d’elle-même. On se retrouve à introduire des interfaces et utiliser des mocks pour les tests.\n\n\nNe pas mettre plusieurs Act dans le même test : parfois on est tenté de mettre en place plusieurs Arrange/Act/Assert à la suite dans le même test. C’est une mauvaise idée parce que le test devient difficile à lire et à modifier, et a tendance à grossir encore.\n\n\nA propos de la question des logs :\nSelon l’auteur, les logs doivent être testé uniquement s’ils sont destinés à être observés par des personnes autres que les développeurs eux-mêmes.\nPar exemple des personnes du business qui en ont besoin pour des insights.\nSteve Freeman et Nat Pryce distinguent deux types de logs dans Growing Object-Oriented Software, Guided by Tests : le support logging qui est destiné au personnel de support et sysadmins, et le diagnostic logging qui est destiné aux développeurs eux-mêmes pour du débug.\n\n\nIl faut bien distinguer le diagnostic logging et le support logging, en n’y appliquant pas la même technique de code.\nLe support logging étant plus important, on pourra utiliser une classe à part inspirée du structured logging : une manière de logger qui sépare les paramètres et le texte principal, de manière à pouvoir reformater ces logs comme on veut.\nExemple de code :\ndomainLogger.userTypeHasChanged(45, \"customer\", \"corporate\");\n\nclass DomainLogger {\npublic userTypeHasChanged(\nuserId: number,\noldType: UserType,\nnewType: UserType\n) {\nthis.logger.info(\n`User ${userId} changed type ``from ${oldType} to ${newType}`\n);\n}\n}\n\nPour le tester il va falloir le traiter comme une dépendance out-of-process unmanaged (puisqu’elle ne nous est pas privée). Et donc on peut faire comme avec le message envoyé dans le message bus :\nSi c’est le controller qui doit faire le log, il peut le faire directement et ce sera testé dans un test d’intégration sous forme de mock.\nSi c’est le code de domaine qui le fait, il faut séparer la logique d’envoi du log de l’envoi du log lui-même : créer un domain event pour l’envoi de ce log dans le domaine, et itérer sur les events de log dans le controller pour logger les logs dans la dépendance out-of-process. Le test pourra être fait sous forme unitaire pour vérifier la création du domain event.\n\n\n\n\nConcernant la quantité de logs :\nPour le support logging la question ne se pose pas : il en faut autant qu’il y a de requirement business.\nPour le diagnostic logging il faut faire attention à ne pas en abuser :\nTrop de logs noient l’information importante.\nMême si on log avec des niveaux différents, on pollue quand même le code avec des lignes de log un peu partout, ce qui rend plus difficile la lecture.\nL’auteur conseille de ne pas utiliser de logs dans le domaine, et dans ne le controller les utiliser que temporairement pour trouver un bug, puis les enlever.\nIdéalement il faudrait que les logs ne servent que pour les exceptions non gérées.\n\n\n\n\nConcernant la manière de passer le logger à nos objets, l’auteur conseille de le passer explicitement dans le constructeur ou dans l’appel à une méthode.","9---mocking-best-practices#9 - Mocking best practices":"Il faut mocker les unmanaged dependencies à l’edge (au bord) de notre système.\nLa raison est d’augmenter la protection contre les régressions en mettant en jeu le plus possible de code. On va donc mocker au plus près de l’appel à la dépendance externe.\nOn améliore aussi la résistance aux refactorings parce que ce qui est mocké est une API publique, et donc peu susceptible de changer contrairement à notre code interne.\n\n\nExemple : Si on a une classe MessageBus qui encapsule et ajoute des fonctionnalités à une classe Bus qui elle-même est un simple wrapper autour de la dépendance externe, il faut mocker Bus et non pas MessageBus.\nclass MessageBus {\nprivate _bus: Bus;\n// [...]\n}\n\nDans les tests on va peut être instancier un peu plus de choses pour que le mock soit en bout de chaîne, mais c’est pas grave :\n// Arrange\nconst busMock = new Mock&lt;IBus>();\nconst messageBus = new MessageBus(busMock);\nconst sut = new UserController(messageBus)\n// [...]\n// Assert\nexpect(busMock).toHaveBeenCalledWith(/* ... */);\n\nPour le mock de notre DomainLogger, on n’est pas obligés d’aller jusqu’à l’edge parce que contrairement à MessageBus où la structure exacte des message est cruciale pour maintenir la compatibilité avec la lib, la structure exacte des messages de log nous importe peu.\n\n\nQuand on veut mocker du code réutilisé dans de nombreux endroits (ce qui est en général le cas du code qui est à l’edge du système), il peut être plus lisible d’implémenter son propre objet de mock, qui est par définition un spy.\nExemple de spy :\nclass busSpy {\npublic send(message: string) {\nthis.sentMessages.push(message);\n}\n\npublic shouldSendNumberOfMessages(num: number) {\nexpect(this.sentMessages.length).toBe(num);\nreturn this;\n}\n\npublic withEmailChangedMessage(userId: number, newEmail: Email) {\nconst message = `Type: user email changed id: ${userId}``email: ${newEMail}`;\nexpect(this.sentMessages).toContain(message);\nreturn this;\n}\n}\n\nUtilisation dans le code :\nbusSpy\n.shouldSendNumberOfMessages(1)\n.withEmailChangedMessage(user.userId, \"new@gmail.com\");\n\n\n\nBonnes pratiques pour les mocks :\nVu que les mocks doivent êtres réservés aux dépendances out-of-process unmanaged, ils doivent être seulement dans les tests d’intégration.\nOn peut utiliser autant de mocks que nécessaire pour gérer toutes les dépendances out-of-process unmanaged qui sont utilisées dans notre controller.\nPour bien s’assurer de la stabilité de l’utilisation de l’API publique constituée par notre dépendance unmanaged, il faut aussi vérifier le nombre d’appels vers la dépendance.\nIl ne faut mocker que les classes qu’on possède. Ca veut dire qu’il faut wrapper toute dépendance unmanaged out-of-process par un adapter qui représente notre utilisation de cette dépendance. C’est ce wrapper qu’on va mocker.\nUn des avantages c’est que si la dépendance change de manière importante dans son interface, elle ne pourra pas impacter le reste de notre code sans qu’on change notre wrapper. Il s‘agit d’une protection.\nA l’inverse, selon l’auteur, créer des wrappers autour de dépendances qui ne sont pas unmanaged ne vaut pas le coup en terme de maintenance. Un exemple en est l’ORM.","10---testing-the-database#10 - Testing the database":"Il est préférable d’avoir tout ce qui concerne la structure de la base de données dans l’outil de versionning, tout comme le code.\nEn plus de la structure, certaines données sont en fait des reference data, et doivent être aussi versionnées avec le code.\nIl s’agit de données qu’il faut générer pour que l’application puisse fonctionner.\nOn peut différencier les reference data du reste en se demandant si l’application peut modifier ces données : si non alors ce sont des reference data.\nExemple : imaginons qu’on reprenne notre exemple CRM, et qu’on veuille mettre le type d’utilisateur en base. Si on veut garantir par la DB elle-même que le type ne sera pas autre chose que les types autorisés, on peut créer une table UserTypes, y mettre les types autorisés, et faire une foreign key depuis la table User vers cette table.\nLes données dans cette table sont là juste pour des raisons techniques, pour faire ce qui est fait ou pourrait l’être dans le code mais avec plus de sécurité. Elles ne sont pas accessibles aux utilisateurs de l’application. Ce sont des reference data.\n\n\n\n\nIl est préférable de permettre à tous les développeurs d’avoir leur base de données (idéalement sur leur machine locale).\nUne DB partagée peut devenir inutilisable, au moins momentanément, et ne permet pas de garantir l’exécution des tests vu que des modifications peuvent être faites par les autres développeurs.\n\n\nIl y a deux types d’approche pour le développement vis-à-vis de la base de données : la delivery state-based et migration-based.\nLa state-based consiste à avoir l’état actuel de la structure de la DB versionnée. On va alors créer une DB modèle à partir de cette structure, puis utiliser un outil de comparaison qui va la comparer avec la DB de production, pour ensuite appliquer les modifications sur la production.\nLa migration-based consiste à écrire des scripts de migration qui vont être versionnées. On ne connaît pas l’état actuel de la DB depuis ces scripts, mais les jouer tous dans l’ordre permet d’en obtenir un exemplaire.\nL’outil de comparaison de DB ne sera pas utilisé ici, sauf pour éventuellement permettre de détecter des anomalies dans la DB de prod.\n\n\nLa state-based est plus utile pour gérer les conflits de merge en ayant l’état explicite, alors que la migration-based est plus utile pour gérer la data motion (le fait de changer la structure de la DB avec des données dedans).\nLa raison est que gérer la transformation de données existantes est difficile à faire automatiquement, il faut y appliquer des règles métier.\nDans la plupart des cas, gérer la data motion est plus important que la gestion de conflits de merge. Donc il vaut mieux préférer l’approche migration-based.\n\n\n\n\nIl ne faut jamais faire de changements directement dans la DB sans passer par l’app, autrement que par des scripts de migration versionnés.\nSi une migration est incorrecte, il vaut mieux faire une migration pour la corriger (sauf si elle n’a pas encore été jouée et que la jouer amènera à de la perte de données).\n\n\nA propos de la gestion des transactions dans nos DB :\nLes transactions sont importantes à la fois dans le code pour garantir la consistance des données, et aussi dans les tests pour s’assurer qu’ils sont fiables.\nDans le code, on a deux notions liées à la DB :\nLes transactions qui décident si les modifications faites doivent être gardées ou non. Elles durent le temps de l’opération entière.\nLes repositories qui prennent une transaction, et agissent sur les données (en lecture ou écriture) dans le cadre de cette transaction.\nExemple dans notre controller du CRM :\npublic UserController {\n\npublic UserController(\nprivate transaction: Transaction\n) {\nthis.userRepository =\nnew UserRepository(transaction);\n}\n\n// [...]\nconst user = this.userRepository.getById(userId);\n// [...]\nthis.userRepository.save(user);\nthis.transaction.commit();\n// [...]\n}\n\n\n\nIl existe aussi un pattern appelé unit of work, qui consiste à retenir les modifications sur les objets qui doivent avoir lieu au cours d’une transaction, et à les soumettre en une seule fois à la DB au moment où la transaction est validée.\nCa permet notamment d’économiser le nombre de connexions à la DB. La plupart des ORM l’implémentent.\n\n\nDans le cas où on travaille avec les document databases comme MongoDB, les transactions sont souvent garanties au sein d’un même document seulement.\nDans ce cas, il faut se débrouiller pour que nos opérations n’affectent qu’un document. Si on utilise le domain model pattern du DDD, on pourra affecter un aggregate par document et suivre la guideline de ne mettre à jour qu’un document à la fois.\n\n\nConcernant les tests d’intégration, il faut que chacun des 3 blocs (Arrange, Act, Assert) ait sa transaction à lui.\nLa raison est de chercher à reproduire au mieux l’environnement de production. Dans le cas contraire on peut par exemple se retrouver à avoir certaines de nos libs (ORM notamment) qui vont mettre en cache certaines données au lieu d’aller lire/écrire en DB explicitement.\nÇa compromet donc l’idée de wrapper chaque test dans une transaction qu’on annule à la fin du test (l’idée est évoquée et balayée pour cette raison).\n\n\n\n\nSelon l’auteur, la parallélisation des tests d’intégration n’en vaut pas le coup, parce que ça nécessite trop d’efforts. Il vaut mieux les jouer séquentiellement, et cleaner les données entre les tests.\nIl suggère de cleaner au début de chaque test. Le faire à la fin peut poser problème à cause de potentiel crash avant la fin.\nConcernant la manière d’effacer les données, il suggère une simple commande SQL de type DELETE FROM dbo.User;\n\n\nIl vaut mieux éviter d’utiliser une DB “in memory” à la place de la vraie DB dans les tests d’intégration. Ça permet de transformer les tests d’intégration en unit tests, mais ça leur enlève aussi de la fiabilité vis-à-vis de l’intégration à cause des différences entre les deux bases de données.\nSelon l’auteur on va finir de toute façon par faire des tests d’intégration à la main si on va sur une BD différente.\n\n\nOn peut utiliser certaines techniques de refactoring pour rendre le code des tests d’intégration plus lisible :\nPour la section Arrange : on peut par exemple utiliser des méthodes de type factory pour que la création des objets en base avec transaction prenne moins de place.\nLe pattern Object mother consiste à avoir une méthode qui crée l’objet, et le renvoie.\nIl conseille de commencer par mettre ces méthodes dans la classe de test, et de ne les déplacer que si besoin de réutilisation.\nOn peut mettre des valeurs par défaut aux arguments, pour n’avoir à spécifier que ceux qui sont nécessaires.\n\n\nPour la section Act : on peut aussi utiliser une fonction helper pour réduire ça à un appel qui créera la transaction et la passera à la méthode testée, comme en production.\nEx :\nconst result = execute(() => user.changeEmail(userId, \"new@gmail.com\"));\n\n\n\nPour la section Assert : là aussi on peut utiliser des fonctions helper :\nOn peut mettre des fonctions qui abstraient le fait d’aller chercher des données en base.\nconst user = queryUser(user.id);\n\nOn peut créer une classe exposant une fluent interface par dessus des instructions assert.\nclass UserExtensions {\npublic shouldExist(user: User) {\nexpect(user).toBeTruthy();\nreturn user;\n}\n\npublic withEmail(user: User, email: Email) {\nexpect(email).toEqual(user.email);\nreturn user;\n}\n}\n\n// In test\nuser.shouldExist().withEmail(\"new@gmail.com\");\n\nTODO : ce code ne fonctionne pas en l’état, il faudrait trouver le moyen de faire de l’extension de méthode en Typescript.\n\n\nAvec les helpers qui créent des objets dans la section Arrange, ou qui lisent des objets dans la section Assert, on crée plus que 3 transactions en tout. Pour autant, ça reste un bon trade off selon l’auteur : on sacrifie un peu de performance du test, contre une amélioration substantielle de maintenabilité du test.\n\n\nFaut-il tester les opérations de lecture ? (comme renvoyer une information au client)\nLe plus important est de tester les opérations d’écriture qui peuvent corrompre les données. Pour celles de lecture il n’y a pas ce genre d’enjeu, donc la barre pour ajouter des tests est plus haute : il ne faut tester que les opérations les plus complexes.\nEn fait, l’intérêt principal du domain model c’est de protéger la consistance des données à travers l’encapsulation. Dans les opérations de lecture il n’y en a pas besoin.\nL’auteur conseille donc de ne tester les opérations qu’avec des tests d’intégration, et seulement pour celles qu’on veut tester.\nIl conseille aussi d’écrire les requêtes pour la lecture directement en SQL, l’ORM n’étant pas utile dans ce cas, et ajoutant des couches d’abstractions inutiles et peu performantes.\n\n\n\n\nFaut-il tester les repositories ?\nNon. Malgré l’intérêt apparent, le rapport bénéfice/coût est défavorable :\nD’un côté les repositories manipulent la DB qui est une dépendance out-of-process, donc si on les testait, ce serait avec des tests d’intégration (et ceux-ci coûtent cher).\nDe l’autre, ils ne fournissent pas tant de protection contre les régressions que ça, et surtout ils sont pour l’essentiel déjà testés par les tests d’intégration des controllers.\nSi on arrive à isoler les factories à part, ça pourrait valoir le coup de les tester à part unitairement, mais quand on utilise un ORM, on ne peut en général pas tester le mapping à part de la DB.\n\n\nIl en est de même pour les event dispatchers par exemple, dont le rapport bénéfice/coût des tests sera défavorable.","iv---unit-testing-anti-patterns#IV - Unit testing anti-patterns":"","11---unit-testing-anti-patterns#11 - Unit testing anti-patterns":"Il ne faut pas rendre publique une méthode privée, juste pour la tester.\nLa 1ère règle est de tester la fonctionnalité privée par l’effet qu’elle a sur l’API publique.\nSi la fonctionnalité privée est trop compliquée pour être testée à travers ce qui est public, c’est le signe d’une abstraction manquante. Il faut alors la matérialiser.\nExemple de code dont on a envie de tester la méthode privée getPrice() sans passer par la méthode publique :\nclass Order {\npublic generateDescription() {\nreturn `Name: ${this.name}, ``total price: ${this.getPrice()}`;\n}\n\nprivate getPrice() {\n// de la logique compliquée ici\n}\n}\n\nOn matérialise l’abstraction manquante et on la teste avec de l’output-based testing :\nclass Order {\npublic generateDescription() {\nconst calculator = new PriceCalculator();\nreturn `Name: ${this.name}, ``total price: ``${calculator.calculate(\nthis.products\n)}`;\n}\n}\n\nclass PriceCalculator {\npublic calculate(products: Products[]) {\n// de la logique compliquée ici\n}\n}\n\n\n\n\n\nIl en est de même avec un attribut privé : le rendre public juste pour le tester est un antipattern.\nIl ne faut pas faire fuiter du domain knowledge du code vers les tests : réutiliser le même algorithme dans le test ne permettra pas de remarquer qu’on s’est trompé.\n_ Un exemple simple peut être un code qui fait une addition :\nreturn a + b, et un test qui teste avec l’addition aussi : expect(result).toBe(3 + 2);\nIl vaut mieux vérifier des valeurs pré-calculées sans réimplémenter l’algo : expect(result).toBe(5);\n_ Si on copie l’algo dans le test, alors on aura tendance à mettre à jour en même temps le code et le test en cas de changement, sans pouvoir se rendre compte que l’algo est faux. * Idéalement il faut pré-calculer le résultat à expect dans le test avec l’aide d’un expert métier (quand on n’est pas expert nous-mêmes comme pour l’addition), et en tout cas il ne faut pas obtenir le calcul à partir du code qui est censé être testé.\nLa code pollution consiste à introduire des choses dans le code, qui ne sont utiles que pour le test. C’est un antipattern.\nPar exemple avoir un if(testEnvironment) ... else ... introduit de la pollution qui posera des problèmes de maintenance plus tard.\nOn peut en général régler le problème avec des interfaces : par exemple s’il s’agit d’éviter certaines opérations de log dans les tests en ne loggant pas si on est en env de test, on peut injecter le logger dans le code avec une interface. Dans le test on donnera une version fake du logger qui ne log pas.\nL’interface est une petite pollution aussi, mais elle crée beaucoup moins de danger que des bouts de code dans des if.\n\n\n\n\nOn est parfois tenté de vouloir stubber/mocker une seule méthode d’une classe qui fait quelque chose de complexe, pour tester ce qui est complexe et éviter qu’elle ne communique avec une dépendance out-of-process. Ceci est un antipattern.\nLa bonne façon de faire est de séparer la logique complexe de la partie qui communique la chose à la dépendance out-of-process (typiquement avec un humble object pattern qui fait le lien entre les deux), et unit tester la logique.\n\n\nConcernant la notion de temps utilisée dans le code (new Date()), l’introduire en tant qu’élément statique est, comme dans le cas du logger, un antipattern qui introduit une dépendance partagée dans les tests, et pollue le code.\nLa bonne manière est d’introduire la dépendance temporelle explicitement dans le constructeur ou la méthode appelée.\nOn peut le faire soit sous forme de service appelable pour obtenir la date, soit en passant la valeur pré-générée. Passer la valeur directement est ce qui présente le moins d’inconvénients, à la fois pour la clarté du code, et pour la testabilité."}},"/notes/the-design-of-web-apis":{"title":"The Design of Web APIs","data":{"":"","1---what-is-api-design#1 - What is API design":"Ici nous parlons des API web, c'est-à-dire utilisant le protocole HTTP.\nUne API est dite publique quand elle est fournie comme service à une autre entreprise, et privée quand elle est fournie à des services internes.\nLe design d’une API web doit être fait pour rendre la vie simple aux développeurs qui vont la consommer.\nL’API doit cacher l’implémentation, et n’exposer que ce dont l’utilisateur a besoin.\nL’API doit être intuitive, ressembler à ce à quoi on s’attendrait.","2---designing-an-api-for-users#2 - Designing an API for users":"Il faut comprendre les besoins de l’utilisateur de l’API pour designer une API qui y réponde, et non pas utiliser le point de vue du backend.\nÉviter d’adopter le point de vue du créateur de l’API passe par :\nÉviter l’influence des structures de données côté backend. Si on expose exactement ce qui est en BDD c’est un signe qu’on n’adopte probablement pas le point de vue du consommateur.\nIl faut correctement nommer les choses, mais aussi introduire de l’abstraction pour masquer les détails qui n’intéressent pas le consommateur.\n\n\nÉviter l’influence du code business côté backend. Si le consommateur n’a pas besoin de savoir ce qui se passe finement, on peut lui exposer une API grossière et faire ce qu’il faut dans l’implémentation, ce sera plus simple et plus sûr.\nÉviter l’influence de l’architecture du backend. Par exemple, si le backend utilise une architecture séparée en services, le fait de faire apparaître cette même séparation interne sur l’API exposée n’est probablement pas une bonne idée. Il faut se concentrer sur le besoin du consommateur.\nÉviter l’influence de l’organisation humaine du provider de l’API. Le fait que le provider soit structuré avec tels ou tels départements par exemple ne doit pas influencer l’API.\n\n\n\n\nEn fait, il faut traiter les consommateurs de l’API vis-à-vis de celle-ci comme des utilisateurs finaux vis-à-vis du produit : on doit se demander ce que le consommateur veut, et de quelle manière il le veut. Ça revient à créer des User Stories du consommateur d’API.\nLes deux autres choses dont on a besoin sur les utilisateurs c’est les inputs et outputs : ce que l’utilisateur doit apporter, et ce dont il a besoin en retour lors de l’action.\nEnfin, pour trouver des éléments manquants et compléter l’API, on va analyser d’où vont venir les inputs requis, et ce qui sera fait des outputs.\nAutre question importante qui nous permettra d’être plus efficace : trouver qui sont nos utilisateurs.\nL’API goals canvas est donc un tableau avec les cases suivantes :\nWhos : les utilisateurs de l’API\nWhats : Ce que les utilisateurs font\nHows : les étapes de ce qu’ils font\nInputs : ce dont ils ont besoin à chaque étape, et d’où ils l’ont\nOutputs : ce qu’ils récupèrent à chaque étape, et ce qu’ils en feront\nGoals : Reformulation concise du besoin\n\n\nCe tableau ne peut pas être rempli à l’avance pour toute l'application. Il s’agit d’un processus itératif où on traite les fonctionnalités par petits bouts.","3---designing-a-programming-interface#3 - Designing a programming interface":"REST est une méthode de design d’API qui se calque sur HTTP. Il s’agit d’utiliser à chaque fois une action HTTP (par exemple GET), et un chemin identifiant une ressource (par exemple /product/123), et un contenu optionnel.\nLa réponse est un statut qui indique si la transaction s’est bien passée, et un contenu.\n\n\nPour faire une API REST, il va s’agir de trouver les ressources à partir des API goals qu’on a pu analyser par exemple avec le canvas, et ensuite de leur attribuer des actions.\nQuand on a plusieurs ressources citées dans un API goal, il faut identifier la ressource principale. C’est sur elle que porte l’action. L’autre n’étant en général qu’une information donnée dans le chemin ou le contenu additionnel de la requête.\nExemple : Rechercher des produits dans un catalogue avec une requête textuelle : c’est bien le catalogue qui est la ressource principale.\n\n\nLes chemins doivent refléter cette organisation, en ayant un nom compréhensible pour la ressource au début (/catalogue), suivi des paramètres.\nLa manière la plus standard de faire des chemins c’est d’avoir un nom pluriel pour les collections, suivi d’un identifiant d’une des ressources de cette collection :\n/ressources/{ressourceId}\n/ressources/{ressourceId}/sub-ressources/{subRessourceId}\n\n\n\n\nSi on a un paramètre à donner dans une requête qui doit simplement récupérer un résultat (par exemple faire une recherche), alors on utilise GET et on donne la chaîne de recherche dans un paramètre de l’URL.\nEx : GET /products?free-query=blabla\n\n\nDe même que le path et l’action de l’API doit être conçue selon les besoins du client et non pas selon la structure du backend, le contenu des données doit aussi être structuré dans ce but :\nSi certaines propriétés ne sont pas nécessaires au client pour son usage, on les supprime.\nSi certaines propriétés seraient plus claires dans ce contexte sous un autre nom : on les renomme.\n\n\nQuand on ne sait pas quelle méthode REST choisir pour une action, par défaut on choisit POST.\nIl est facile de faire correspondre une action métier avec une API REST, tant qu’on est dans du CRUD d’une ressource. Mais quand on veut faire des actions plus complexes, alors il faut faire des compromis entre la user-friendlyness, et la conformité avec le standard REST.\nEx : Valider son panier peut être fait avec :\nPOST /cart/check-out, ce qui est très user friendly, mais pas très conforme à la norme de manipulation de ressources de REST.\nPOST /orders, ce qui est plus conforme à REST mais on perd la notion métier de panier dans l’histoire.\nEn réalité il n’y a pas de “bonne solution”, il s’agit de faire un compromis. Il n’y a pas toujours de bonne API.\n\n\n\n\nL’architecture REST :\na été élaborée par un thésard en 2000, et a depuis envahi l’univers des API réseau.\na été conçue spécifiquement pour les architectures distribuées, dont d’ailleurs l’architecture navigateur / serveur en est un rudiment. Il s’agit de favoriser l’efficacité, la stabilité, la scalabilité.\nPour être RESTful, il faut respecter ces 6 principes :\nSéparation Client / Serveur\nStatelessness : toute l’information d’une requête est contenue dans la requête. Le serveur ne conserve pas de session.\nCacheability : une réponse doit indiquer si elle peut être stockée par le client, et si oui pour combien de temps avant qu’il faille qu’il refasse la requête.\nLayered system : quand le client interagit avec le serveur, il n’en voit qu’une couche, le reste est caché derrière l’API.\nCode on demand (optionnel) : le serveur peut transférer du code exécutable au client, par exemple du JavaScript.\nUniform Interface :\nToutes les interactions se font au travers d’actions standardisées sur des ressources, et en fonction de l’état de ces ressources (qu’on obtient par GET, et qu’on modifie par PATCH etc.).\nLes interactions doivent aussi fournir les metadata suffisantes pour comprendre la structure de ces états et ce qu’il est possible de faire aux ressources.\n\n\n\n\n\n\nSi on se rend compte qu’un des paramètres d’une méthode de notre API ne peut pas être fournie par le client, alors c’est qu’on a sans doute oublié quelque chose… Il faut revoir l’API, ajouter une méthode etc.","4---describing-an-api-with-an-api-description-format#4 - Describing an API with an API description format":"Décrire une API selon un format standardisé permet de la partager avec des êtres humains qui vont savoir la lire, et avec des outils automatisés qui pourront l’exploiter pour générer une documentation HTML, ou dans n’importe quel autre but.\nL’OAS (Open API Specification) est un format ouvert, communautaire, et très utilisé pour REST.\nInitialement la spécification s’appelait Swagger, mais elle a été renommée OpenAPI en 2016.\nOn peut utiliser JSON ou YAML pour écrire le document, mais Arnaud recommande YAML pour la lisibilité accrue pour les humains.\nOn édite le fichier à la main et on le versionne.\nVSCode a une extension Swagger viewer qui aide à visualiser ce qu’on a écrit.\n\n\nTips YAML :\nPour indiquer qu’il n’y a rien, il faut mettre des , par exemple quand il n’y a pas encore de paths : paths: {}\nPour écrire une string multiligne, on met un | d’abord :\ndescription: |\nblabla\nblabla\n\nLes noms de propriété doivent être des strings en YAML, donc il faut entourer les chiffres par des guillemets quand ils sont en propriété.\n\n\nLe format OpenAPI :\nIl ne faut pas hésiter à écrire des descriptions.\nex\npaths:\n/objects:\ndescription: description du path\nget:\nsummary: résumé court de la méthode\ndescription: résumé long de la méthode\nresponses:\n\"200\":\ndescription: description de la réponse\ncontent:\napplication/json:\nschema:\n# et on décrit le schéma de la réponse\n\nDans le cas où une méthode GET prend un paramètre, ça se passe dans parameter :\nget:\nparameters:\n# il s'agit du nom qui va apparaître dans l'URL\n- name: nom-du-parametre\ndescription: description du paramètre\n# où se trouve le paramètre\nin: query\n# caractère obligatoire ou non\nrequired: false\nschema:\n# type de données du paramètre\ntype: string\n\nMême chose pour les path parameters quand un morceau du path est paramétrisé. Dans ce cas il faut l’indiquer entre accolades :\npaths:\n/objects/{objectId}\ndelete:\nparameters:\n- name: objectId\nin: path\n# obligatoire, sinon ça ne compilera pas\nrequired: true\nschema:\ntype: string\n\nPour décrire les formats JSON attendus en entrée, ou en réponse, OpenAPI utilise la spécification JSON-schema :\nExemple pour {prop1: \"\", prop2: { prop2-1: \"\"}}\ntype: object\ndescription: une description de l'objet JSON\nrequired:\n- prop1\n- prop2\nproperties:\nprop1:\ntype: string\ndescription: description de prop1\nexample: Exemple de valeur\nprop2:\ntype: object\nproperties:\nprop2-1\ntype: string\nexample: Exemple de valeur\n\nPour les méthodes comme POST qui ont besoin de données en entrée, ça se fait dans requestBody, à côté de responses.\n\n\nOn peut réutiliser des schémas JSON déjà définis pour éviter de les répéter dans notre fichier OpenAPI :\nIl faut renseigner le schéma à la racine du document, dans :\ncomponents:\nschemas:\nmon_objet:\ntype: object\nproperties:\n# [...]\n\nEt ensuite on peut le référencer avec :\nschema:\n$ref: #/components/schemas/mon_objet\n\n\n\nOn peut placer le mot-clé parameters non seulement au niveau des actions (get, post etc.), mais aussi un niveau au-dessus, au niveau des ressources (c’est-à-dire au niveau du path). Et alors à ce moment là ces paramètres s’appliquent à toutes les méthodes du path. Ça évite de les répéter, fût-ce avec un $ref.","5---designing-a-straightforward-api#5 - Designing a straightforward API":"Une des clés d’un design “straitforward” est de reproduire l’habitude des gens, et ça dans tous les domaines.\nIl faut des noms de propriété clairs et évocateurs.\nÉviter les abréviations (min ou max c’est OK parce que c’est entré dans la langue commune)\nÉviter les détails techniques qu’on peut retrouver autrement (ex. type de variable booléenne)\nÉviter ce qui n’intéresse pas le consommateur de manière générale.\nEssayer de ne pas dépasser 3 mots dans un nom quel qu’il soit.\n\n\nIl faut des formats de données bien choisis.\nPar exemple le format ISO 8601 pour une date.\nAdopter le point de vue du consommateur :\n\"type\": \"checking\" plutôt que \"type\": 1\n\n\nIl faut aussi que les données qu’on donne soient pertinentes pour le consommateur.\nNe pas hésiter à ajouter des données pour que le consommateur ait ce qu’il lui faut déjà préparé.\n\n\nA propos des erreurs :\nIl faut bien distinguer 3 types d’erreurs par leur type :\nerreur de format parmi les paramètres fournis\nerreur fonctionnelle\nerreur interne du serveur\n\n\nPour une erreur interne, en général la seule chose qu’a besoin de savoir l’utilisateur c’est qu’il y a eu une erreur et que ce n’est pas de sa faute.\nEn HTTP, les codes 4XX concernent les erreurs causés par l’utilisateur, alors que les codes 5XX concernent les erreurs sur lesquelles l’utilisateur n’a pas d’influence.\nLe code de statut ne suffit pas. Il faut aussi donner d’autres informations, à la fois du texte pour les êtres humains, et des informations pour les machines.\nPour les machines ça peut être un string représentant un type d’erreur précis, une valeur d’ID etc.\n\n\nDans le cas où il y a plusieurs erreurs, il vaut mieux les signaler toutes dans la même réponse.\nSi les erreurs sont de plusieurs types (par ex données mal formées et erreur fonctionnelle), on peut utiliser le code de retour 400 qui est générique.\n\n\n\n\nPour les messages de succès c’est comme pour les erreurs :\nBien renseigner le bon code 2XX\nDonner des informations supplémentaires, comme par exemple retourner l’objet qu’on vient de créer avec son ID générée.\n\n\nIl faut regarder le flow dans son ensemble, et repérer ce dont l’utilisateur d’API a besoin pour lui éviter des erreurs qui seraient de son fait.\nUne fois qu’on a repéré les choses dont il aurait besoin, on les lui fournit avec une API pour lui faciliter le travail.\nNe pas hésiter à agréger des données ensemble dans une API, si ça peut aider le consommateur à consulter une autre API en évitant des erreurs\nPar exemple lui fournir les éléments qui ont une particularité plutôt que bruts, pour qu’il puisse les utiliser dans une autre API qui a besoin en paramètre d’un élément qui a la particularité.","6---designing-a-predictable-api#6 - Designing a predictable API":"Il faut rester consistant au sein de l’API :\nDans le nommage et le format des différents paramètres dans notre API.\nEx : si on a choisi accountNumber quelque part, on ne le change pas en juste number dans un des autres endpoints.\nEt si c’était un nombre, ça reste un nombre.\n\n\nEntre les différents paths de nos API endpoints.\nDans la structure des données prises en paramètre ou renvoyées.\nDans les flows permis par les différents API endpoints.\n\n\nEn plus de la consistance au sein de l’API, il faut respecter la consistance au sein de l’organisation, au sein du domaine métier, et enfin avec ce qui est partagé par le reste du monde.\nNe pas hésiter à utiliser des standards ISO et autres.\nTIP : chercher “<some data> standard” ou “<some data> format” sur google.\n\n\nNe pas hésiter à copier ce que fait une autre API connue, les utilisateurs se sentiront chez eux et tout le monde sera gagnant.\n\n\nIl faut formaliser les règles choisies pour l’API dans un document (cf. chapitre 13), au risque de les oublier et de faire perdre à l’API peu à peu sa consistance.\nIl peut être intéressant de rendre l’API adaptable aux besoins de l’utilisateur :\nUn même endpoint peut proposer d’envoyer les données sous plusieurs formats, par ex CSV et JSON.\nOn peut utiliser un paramètre dans l’URL : /ressource?format=csv\nEt on peut aussi utiliser un header HTTP fait pour ça : Accept: text/csv\nSi le serveur ne peut pas répondre dans ce format, il peut renvoyer le code 406 Not Acceptable.\n\n\nA contrario, quand c’est le client qui envoie par exemple du contenu XML, il peut le spécifier avec le header HTTP Content-type: application/xml\nSi le serveur ne comprend pas ce format, il peut répondre 415 Unsupported Media.\n\n\n\n\nDe la même manière que pour les formats, un endpoint peut supporter plusieurs langues (traduction des phrases, système de mesure etc.).\nOn peut là aussi tirer avantage du protocole HTTP en utilisant ses headers :\nAccept-Language: en-US pour dire qu’on accepte l’anglais US.\nContent-Language: en-US pour dire que le body sera en anglais US.\n\n\n\n\nEnfin on peut aussi permettre à l’utilisateur des fonctionnalités de pagination, de filtrage ou d’ordre des éléments.\nLe plus courant est d’utiliser des paramètres dans l’URL :\n/ressources?page=3&sort=-amount&category=car\n\n\nPour la pagination on pourrait penser au header HTTP Range: items=10-19 qui dit qu’on veut les items du 10ème au 19ème.\n\n\n\n\nIl peut être très pratique le notre API soit discoverable.\nOn peut faire ça à l’aide de metadata qu’on place dans le body. Par exemple, si on renvoie du contenu paginé, avoir un attribut “pagination” qui va donner la page actuelle et le nombre total de pages, pour aider l’utilisateur.\nOn peut aussi ajouter des URLs pour aider l’utilisateur à faire ses prochaines requêtes d’API : ça fait des hypermedia API. Par exemple pour la pagination, ajouter en plus un attribut next et un last avec en valeur les URLs permettant d’obtenir ces pages.\nIl s’agit d’une certaine manière de faire la même chose qu’avec les pages web et leurs liens, mais avec les APIs.\nUsuellement, les URLs vers le reste de l’API sont mis dans des attributs nommés href, links, ou _links.\nPlusieurs spécifications ont été créées sur la manière d’organiser ces URLs d’API, parmi eux HAL, Collection+JSON, JSON API, JSON-LD, Hydra et Siren.\n\n\nUn des moyens d’obtenir des informations sur un endpoint est d’utiliser la méthode HTTP OPTIONS, qui devrait renvoyer les autres méthodes disponibles sur cet endpoint, mais aussi des informations sur le format, ou encore des URLs vers d’autres endpoints utiles liés.\nAttention cependant à bien documenter ce qu’on implémente dans notre API, les utilisateurs sont rarement des experts du protocole HTTP…","7---designing-a-concise-and-well-organized-api#7 - Designing a concise and well-organized API":"Il faut bien organiser son API :\nOrganiser les données :\nPar exemple en groupant les propriétés liées entre elles ensemble, côte à côte ou même au sein d’un objet.\nDans le cas où une propriété est optionnelle, et qu’une autre propriété n’a de sens que si la première est utilisée, les grouper dans un objet serait une bonne idée.\n\n\nEn classant les propriétés des plus importantes aux moins importantes à mesure qu’on descend\n\n\nOrganiser le feedback :\nQuand des erreurs se produisent, on peut les catégoriser avec un type précis avec un système d’enums (en plus du statut HTTP).\nEt on peut les classer de la plus importante à la moins importante.\n\n\nOrganiser les API endpoints :\nOn peut les grouper par catégorie fonctionnelle dans notre document OpenAPI, pour plus de clarté.\nEt dans chaque catégorie, classer du plus important au moins important.\n\n\n\n\n\n\nIl faut bien dimensionner ses endpoints :\nBien réfléchir au nombre de propriétés d’une structure de données, et à la profondeur d’imbrication en fonction de son cas d’usage.\nPour la profondeur, il est recommandé de ne pas dépasser 3 niveaux.\n\n\nBien réfléchir aux endpoints eux-mêmes et à la complexité qu’ils peuvent embarquer pour l’utilisateur :\nPar exemple, un endpoint qui permet d’avoir des infos sur un compte en banque, et qui donne en même temps la liste de toutes les transactions du compte sera probablement surchargé inutilement si l’utilisateur veut juste les infos sur le compte.\nIl vaut mieux s’assurer que notre endpoint ne fait pas 2 choses très différentes (ou plus) en même temps.\n\n\nLe dimensionnement dépend en fait du contexte et doit être optimisé pour l’expérience utilisateur des consommateurs. En général, les petites unités fonctionnent mieux qu’un énorme couteau suisse.","8---designing-a-secure-api#8 - Designing a secure API":"Une des techniques utilisées couramment pour sécuriser des API web est le framework OAuth 2.0.\n\nil faut enregistrer des consumers.\n\n\nCa peut se faire par exemple sur un portail de développement où on va se connecter, payer ce qu’il faut etc.\nQuand on enregistre le consumer, on va lui attribuer des scopes d’autorisation, qui vont correspondre à la possibilité d’accéder à un certain nombre d’API endpoints et de méthodes.\n\n\n\nle client consumer va pouvoir demander un token au serveur d’authentification.\n\n\nL’utilisateur final propriétaire entre ses identifiants sur la page du serveur d’authentification, et après vérification le serveur d’authentification envoie le token au client.\nIl existe d’autres flows possibles dans OAuth.\n\n\n\nLe client va pouvoir consommer l’API en fournissant à chaque fois le token.\n\n\nA chaque requête, le serveur qui a les ressources va valider auprès du serveur d'authentification que le token est valide et possède les bons scopes d’autorisation.\nLe serveur de ressources a connaissance du user ID et du scope d'autorisation attaché au token, et donc va pouvoir éventuellement filtrer le contenu avant de le renvoyer ce que spécifie l’API.\n\n\n\n\nL’ID de l’utilisateur et les scopes attachés à son compte ne sont en général pas dans le body de la requête ou les paramètres d’URL, mais sont pourtant là de par l’authentification initiale. Il est donc intéressant de remarquer qu’un même API endpoint peut renvoyer différentes données en fonction de l’utilisateur qui y fait appel.\nSelon l’organisation internationale OWASP, il faut réduire la surface d’attaque pour réduire le risque. Et donc il faut éviter de donner la possibilité de faire certaines actions à des utilisateurs qui n’en ont pas besoin.\nComment bien définir les scopes d’autorisation ?\nIl faut choisir la bonne granularité : un scope par endpoint et méthode c’est très flexible mais très complexe à configurer pour les utilisateurs, et si on a trop peu de scopes on risque d’être obligé de donner trop de pouvoir à certains utilisateurs.\nOn peut par exemple partir de l’API goals canvas qui nous a permis de construire notre API en fonction des besoins de l’utilisateur qu’on a définis, et regrouper sous un même scope les actions qui concernent un même flow.\nOn peut aussi avoir des scopes arbitraires, comme un scope “admin” qui a accès à tout.\nIl existe plusieurs manières de définir des scopes et pas de solution magique, et la bonne manière dépend en fait du contexte.\nOn peut aussi définir plusieurs granularités en même temps : des scopes grossiers mais faciles à configurer, et des scopes plus fins. L’utilisateur pourra alors utiliser un mélange et configurer plus finement les parties qu’il veut.\n\n\nOpenAPI permet de spécifier des scopes pour chaque endpoint/méthode défini :\nCa se fait dans components -> securitySchemes\ncomponents:\nsecuritySchemes:\nBankingAPIScopes:\ntype: oauth2\nflows:\nimplicit:\nauthorizationURL: \"https://...\"\nscopes:\n\"beneficiary:read\": List beneficiaries\n\"beneficiary:manage\": Create, list beneficiaries\n\nEt ensuite on les indique dans nos paths existants :\npaths:\n/beneficiaries:\nget:\ndescription: Get beneficiaries list\nsecurity:\n- BankingAPIScopes:\n- \"beneficiary:read\"\n- \"beneficiary:manage\"\nresponses:\n…\n\n\n\nToujours dans l’optique de réduire la surface d’attaque, il faut identifier les données sensibles et ne pas les fournir quand ce n’est pas nécessaire.\nPar exemple, quand on obtient les informations d’un compte, on peut donner par défaut les autres infos mais pas le numéro du compte, ou seulement les 4 derniers chiffres.\nQuand il faut les fournir quand même, ou quand il faut fournir une action avec des conséquences jugées sensibles, alors on peut y faire attention et utiliser des techniques pour s’assurer qu’elles sont protégées :\nIsoler l’action sensible dans un endpoint dédié qu’on va pouvoir mieux protéger, dans le cas où il y a une vraie différence de nature entre la version sensible et la version non sensible d’un point de vue utilisateur.\nUtiliser un scope dédié, pour que par défaut la donnée sensible ne soit pas retournée ou actionnée, et le soit si le scope d’autorisation est activé.\nSe baser sur les permissions de l’utilisateur qui fait l’appel pour ne dévoiler la donnée sensible qu’à certaines personnes.\nEt enfin on peut combiner l’access control de l’utilisateur et du consommateur (scope) pour être sûr qu’il s’agit de la bonne personne (et non pas d’une personne qui a une délégation par exemple) et qu’elle le fait dans un cadre où elle a la permission de le faire.\n\n\n\n\nConcernant le feedback lié à la sécurité, quand on demande quelque chose auquel on n’a pas accès, on peut recevoir une réponse 401 Unauthorized si on n’est pas correctement authentifié, ou 403 Forbidden si les scopes qui nous sont associés ne sont pas suffisants pour qu’on fasse l’action.\nAttention à ne pas dévoiler inutilement des informations : il vaut parfois mieux renvoyer systématiquement 404 Not Found plutôt qu’un 403 même si la donnée existe, pour éviter qu’un attaquant essaye de nombreuses données et puisse savoir lesquelles existent même s’il n’a pas les autorisations d’y accéder.","9---evolving-an-api-design#9 - Evolving an API design":"Quand on change notre API, il faut surtout éviter les breaking changes, c’est-à-dire ce qui obligerait les consommateurs à changer leur code pour ne pas que leur application casse.\nSur les données en sortie de l’API :\nIl ne faut pas :\nEnlever, modifier ou déplacer des propriétés.\nModifier le format / type d’une donnée.\nAugmenter la taille d’une chaîne qui était limitée.\nRendre optionnelle une propriété qui était obligatoire.\nAjouter des valeurs à un enum.\n\n\nEn revanche on peut :\nAjouter des propriétés, optionnelles ou obligatoires.\nRendre obligatoire une propriété qui ne l’était pas.\nDiminuer la taille maximale de chaînes limitées.\n\n\nUne des possibilités c’est d’ajouter des données déjà existantes dans de nouvelles propriétés, mais sous une autre forme. Ça peut par contre avoir des limites dans la mesure où ça complique aussi l’API.\n\n\nSur les données en entrée et les paramètres :\nC’est pareil que pour les données en entrée, sauf :\nOn peut rendre optionnelle une propriété qui était obligatoire, mais pas l’inverse.\nOn peut augmenter la taille maximale d’une chaîne et pas la diminuer.\nOn peut ajouter des valeurs à un enum.\nSi on ajoute une propriété, il faut qu’elle soit optionnelle.\n\n\n\n\nSur le feedback :\nLe feedback peut être traité comme des données de sortie.\nOn ne peut donc pas ajouter un élément à un enum d’erreurs existant.\n\n\nPour les codes de statut HTTP, la spécification dit que les clients sont censés au moins comprendre les classes d’erreur (2XX, 4XX, 5XX etc.), et donc que s’ils rencontrent une erreur qu’ils ne savent pas gérer (par ex 429), ils doivent se rabattre sur l’erreur par défaut de la classe (par ex 400).\nDans la réalité, les clients ne vont pas forcément suivre la spec, et n’importe quel changement de feedback pourrait les impacter (sauf peut être d’enlever un code d’erreur qui ne peut plus arriver). Donc si on peut éviter d’introduire de nouvelles erreurs avec des clients qu’on connait pas, c’est mieux.\n\n\n\n\nSur les endpoints et les flows d’appels eux-mêmes :\nRenommer le path d’un endpoint est en théorie possible en renvoyant 301 Moved Permanently sur l’ancien path, mais dans les faits les clients ne vont pas forcément le prendre en compte et se retrouver en état d’erreur.\nOn ne peut pas non plus enlever des endpoints sans casser les clients.\nOn peut en ajouter, mais à condition que ça ne change pas les flows existants.\nPar exemple, si on ajoute un endpoint de validation qui devra être appelé juste après un ancien endpoint pour améliorer la sécurité, les clients non mis à jour ne l’appelleront pas, et auront des échecs \"silencieux\", parce que le flow aura changé.\n\n\n\n\n\n\nMême sans changer le contrat d’API, on peut quand même casser les clients par des changements d’apparence anodins du contrat invisible.\nLa loi de Hyrum dit : “With a sufficient number of users of an API, it does not matter what you promise in the contract: all observable behaviors of your system will be depended on by somebody”.\n\n\nLes breaking changes sont bien plus graves sur les API publiques. Sur les API privées on peut en général se débrouiller pour mettre à jour les clients. Mais c’est quand même un poids non négligeable, qui doit nous amener à nous poser la question pour chaque breaking change, de savoir si on le veut vraiment.\nQuand les breaking changes deviennent inévitables, on peut versionner son API.\nLe semantic versioning classique des implémentations (Major.Minor.patch) pourrait se transcrire dans l’univers des API par une version à 2 chiffres : Breaking.Non-breaking.\nPour créer les nouvelles versions, on peut :\nUtiliser l’URL (blabla.com/apiv2/) ou les sous-domaines (v2.blabla.com/api/). C’est ce qui est le plus courant.\nAjouter des paramètres aux requêtes, que ce soit dans l’URL, dans le header HTTP ou autre. Mais globalement cette option-là est moins claire, et moins appréciée des consommateurs.\nUtiliser le fait que l’utilisateur de l’API est identifié, et donc stocker côté provider une association UserID / Version. C’est plus simple pour le consommateur qui n’a rien à faire de son côté à part demander à passer à la V2, mais moins flexible aussi.\n\n\nIl est possible de versionner avec une plus grande granularité que l’API entière, par exemple ajouter une V2 seulement pour certains endpoints, seulement certaines méthodes, ou encore donner la possibilité de visionner le contenu du body avec des headers HTTP.\nMais c’est peu connu des consommateurs, surtout dans le monde de REST, et ça peut porter à confusion. Donc bien réfléchir avant d’aller par là.\n\n\n\n\nPour éviter d’avoir à faire des breaking changes, on peut essayer dès le début d’avoir des structures extensibles.\nPar exemple, prendre l’habitude d’encapsuler dans un objet toute propriété qui nous paraît importante va nous permettre d’y ajouter des propriétés par la suite.\nSi on a des propriétés similaires, il peut être judicieux de les mettre dans une liste ou un objet, pour pouvoir facilement les étendre plus tard.\nPar ex si on a une date de début, de fin, on peut les mettre dans une liste d’événements, pour peut être plus tard ajouter une date d’exécution comme élément de liste supplémentaire.\n\n\nUtiliser des formats de données standard permet aussi de baisser le risque de vouloir les changer plus tard.\nConcernant les erreurs, les mettre dans une liste, avec chacune dans son objet permet de les rendre extensibles, et de pouvoir en ajouter aussi.\nLa propriété type correspondant à un enum indiquant le type d’erreur doit être le plus générique possible parce qu’on ne peut pas changer les enums sans breaking change.\nEx : MISSING_MANDATORY_ATTRIBUTE, plutôt que MISSING_AMOUNT si la propriété manquante était un amount.\n\n\nAutre exemple : l’auteur conseille de ne pas retourner d’erreur si l’utilisateur demande 150 résultats par page et que le maximum est de 100, mais d’en retourner simplement 100. Et plus tard, si on veut abaisser ce maximum à 50, on ne provoquera pas non plus d’erreurs chez les clients.\nBien sûr, si l’utilisateur veut transférer 1500€ et qu’il a 1000€, il ne faut pas faire le transfert de 1000€ silencieusement, quand ça concerne le domaine métier ce genre de choix doit être réfléchi avec soin :)\n\n\n\n\nPlus l’API va grossir, et plus l’étendre va devenir difficile sans casser soit les données, soit les flows. Une bonne pratique est de garder les API petites, et d’en faire plusieurs autonomes.","10---designing-a-network-efficient-api#10 - Designing a network-efficient API":"Le “design idéal” d’API est contrebalancé par des facteurs supplémentaires à prendre en compte. L’un d’entre eux est la performance réseau. Il faut trouver un compromis entre les deux.\nLes problèmes de performance réseau dépendent :\nde la vitesse du réseau\ndu volume de données échangée\ndu nombre d’appels API\n\n\nIl y a d’abord des optimisations au niveau du protocole :\nLa compression et les connexions persistantes sont disponibles par défaut dans HTTP, et peuvent être activées dès le début.\nLa compression permet de réduire les données échangées\nLes connexions persistantes permettent de réutiliser les mêmes connexions pour plusieurs requêtes, pour gagner de la latence.\n\n\nChaque endpoint/méthode peut renvoyer des métadonnées indiquant combien de temps la réponse doit être mise en cache (donc conservée sans refaire d’appel API) par le client.\nPour le protocole HTTP ça se fait avec le header Cache-Control.\nPour choisir la valeur on se base sur le contexte au cas par cas : est-ce qu’il est probable statistiquement que telle donnée soit changée dans l’heure ? Dans les 10 mn ? Est-ce que c’est important d’avoir des données très à jour sur telle ou telle donnée ?\n\n\nEn plus du cache on peut aussi utiliser les requêtes conditionnelles : cette fois il s’agira pour le backend de soit retourner la donnée si elle a été modifiée depuis la dernière fois, soit une simple réponse 304 Not Modified sans rien dans le body.\nOn gagne en bande passante.\nLe mécanisme consiste à ce que le backend envoie un Etag dans le header HTTP la première fois, et que ce tag soit renvoyé par le client pour les prochaines fois. Le backend peut alors savoir si depuis cet Etag la donnée a changé ou non. Si elle a changé, il renverra un nouvel Etag pour la prochaine fois.\n\n\n\n\nMais on peut aussi optimiser l'utilisation réseau grâce à des techniques de design d’API :\nPermettre la pagination et le filtrage est une des panières d’envoyer moins de données à la fois.\nDans le cas d’une pagination, si des éléments sont ajoutés au fur et à mesure et qu’on veut garder une fiabilité et une exhaustivité de ce qui est affiché, on peut demander les X prochains éléments à partir de l’élément qui a tel ID, plutôt que juste la page 2.\n\n\nBien choisir les propriétés pour les éléments d’une liste.\nIl faut trouver une balance entre trop de propriétés (voir toutes) quand on fait GET sur une ressource sans préciser laquelle, et pas assez de propriétés. Dans un cas on a beaucoup de données quelle que soit l’utilisation, dans l’autre on peut se retrouver à devoir refaire un GET mais cette fois avec l’ID de chaque objet, pour obtenir le détail avec les propriétés qui nous intéressent.\nLa solution est de trouver les propriétés qui sont souvent utiles quand on traite une liste d’objets, et les ajouter à la réponse de l’API de liste.\nUne autre solution serait d’accepter un paramètre ou un header HTTP (par ex Accept:application/notre.content.type.custom+json), et de renvoyer la version courte, complète, ou même enrichie en propriétés en fonction de ce qui est demandé par le client.\n\n\nAgréger des données venant de ressources différentes dans une même réponse d’API. Il s’agit de dénormalisation de données dans l’API.\nÇa peut permettre d’économiser des appels d’API.\nPar exemple en obtenant dans un même appel le profil d’une personne, et la liste de ses adresses qui sont d'habitude dans une ressource séparée.\n\n\nMais c’est à manipuler avec précaution :\nD’abord c’est pas sûr que ça nous fasse économiser du temps dans tous les cas : parfois une énorme requête peut prendre plus de temps que plusieurs en parallèle, ou même être plus fragile sur des réseaux lents (type 3G), et donc plus souvent sujette à être retentée de zéro.\nEnsuite ça peut être plus difficile à mettre en cache, sachant que le temps de cache sera alors celui de la ressource contenue dans l’agrégation qui a le temps de vie le plus court.\nDonc mauvaise idée de grouper une ressource qui change peu souvent et une ressource qui change très souvent dans un même endpoint.\n\n\nOn peut aussi le faire de l’expansion à la demande avec un paramètre : dénormaliser les données des propriétés indiquées par le client, si c’est possible.\nMême exemple que le profil avec ses adresses, sauf que le client ajoute un paramètre /profile?expand=addresses\n\n\n\n\nPermettre le querying côté client avec des API du type GraphQL. Les clients indiquent alors chaque propriété qu’ils veulent parmi celles disponibles pour une même ressource.\nC’est utile quand le réseau est vraiment important pour nous, et que chaque milliseconde compte.\nPar contre GraphQL utilise uniquement la méthode POST sur un unique endpoint HTTP, donc les mécanismes de cache usuels qu’on peut mettre en place avec les API REST et les headers HTTP (Cache-Control etc.) ne marchent pas.\nL’éventuelle mise en cache doit être gérée par le client plutôt qu’un niveau du protocole, mais c’est de toute façon assez compliqué étant donné qu’on fait en fait de l’agrégation dans tous les sens.\n\n\n\n\nDe manière générale, revenir aux bases et envoyer aux utilisateurs ce dont ils ont besoin peut probablement permettre de réduire les appels réseau. Que ce soit avec de l’agrégation, l’ajout d’une propriété ici ou là, l’ajout de nouveaux endpoints spécifiques pour pour des besoins précis etc.\nMais comme toujours il faut trouver le bon compromis. En créant trop d’endpoints personnalisés on risque aussi d’obtenir une API compliquée et pas très réutilisable. Une solution peut être alors de créer des couches d’API intermédiaires consommant l’API initiale, et fournissant des endpoints spécifiques à un contexte donné.\nPar exemple, les BFF (Backend For Frontend) sont des API qui vont consommer des API de données plus génériques, et fournir des endpoints personnalisés pour répondre exactement aux besoins du frontend qu’ils ont en charge.","11---designing-an-api-in-context#11 - Designing an API in context":"La nature des informations et de la relation provider / consumer peut lourdement influencer le design d’API. On peut avoir parfois besoin d’une communication initiée par le provider.\nExemple : une transaction initiée par le client est acceptée tout de suite (donc réponse 202 Accepted), mais doit ensuite être validée puis exécutée, et ça peut prendre des minutes, heures voire jours.\nLe client peut faire des appels réguliers pour voir où ça en est, et le header Cache-Control peut donner au client une idée de la fréquence à laquelle faire ces appels.\nPour autant, il est hors de question que le client patiente pendant des heures ou des jours en faisant des appels réguliers, c’est trop inefficace.\n\n\nLa solution peut être la mise en place d’un webhook côté client.\nIl s’agit d’une API HTTP, standardisée par le provider (pour que ce ne soit pas l’anarchie) et mise en place par le client, et dont le client a donné l’URL au provider au moment de souscrire pour avoir un token et pouvoir consommer l’API du provider.\nUne fois que la transaction a été exécutée, le provider va alors faire un appel POST sur ce webhook, en donnant les infos de l’événement qui s’est produit. Ce backend maintenu par le client peut alors utiliser les moyens nécessaires pour envoyer l’info au téléphone, ou au navigateur (par des mécanismes push, des websockets, du simple polling HTTP de la part du navigateur etc.).\nAttention à penser à la sécurité du webhook. Une des possibilités c’est d’en faire un event assez générique qui va pousser le client à faire une autre requête pour obtenir les infos détaillées de l’event.\nCe système de webhooks est standardisé par le W3C sous le nom WebSub.\n\n\nDans le cas où c’est le client qui veut obtenir des updates très fréquents pendant un certain temps, et pour éviter qu’il fasse des calls permanents, il peut faire un appel qui initie une communication SSE (Server-Sent Events).\nLes SSE se basent sur HTTP, et ont été standardisés par le W3C pour HTML5, donc fonctionnent très bien avec les navigateurs.\nLa connexion HTTP reste ouverte, et le serveur peut simplement envoyer des données dès qu’il les a, jusqu’à ce que l’un des deux demande à fermer la connexion.\nPar contre c’est unidirectionnel : c’est le serveur qui envoie les données.\n\n\nSi on veut une connexion bidirectionnelle (par exemple pour les chats), on peut recourir aux WebSockets.\nLes WebSockets ne se basent pas sur HTTP mais directement sur TCP, donc la mise en place peut être plus compliquée vis-à-vis des proxies et autres.\n\n\n\n\nParfois, pour économiser des requêtes on peut vouloir faire des PATCH, PUT, POST ou DELETE sur une liste d’éléments.\nIl suffit d’envoyer en body une liste d’objets avec la même information qu’on aurait donné pour faire l’opération sur un seul élément.\nCôté réponse, dans un cas comme ça on peut valider les éléments qui réussissent même si certains ont échoué.\nOn peut retourner une liste avec là aussi la réponse qu’on aurait renvoyé pour chaque élément seul : le statut, la ou les erreurs éventuelles ou le contenu pour chaque élément.\nLe statut de la réponse HTTP peut être alors 207 Multi-Status.\nAttention à bien vérifier que dans notre cas valider certains éléments et avoir des éléments sur d’autres ne crée pas d’incohérence.\n\n\n\n\nLe type d’API qu’on utilise doit être choisi en fonction du contexte, à la fois celui du consumer parce que l’API est pour lui, et celui du provider parce qu’il a aussi des contraintes. Il faut éviter de céder au biais de choisir l’outil ou le format qu’on connaît le mieux à chaque fois.\nActuellement il y a 3 types d’API connues : REST, GraphQL et gRPC.\nREST étant de loin le plus connu, et répondant à la plupart des besoins, il est considéré comme celui à prendre par défaut si on n’a pas de contraintes particulières. Les autres peuvent aussi être très bien en fonction du contexte.\nREST est basé sur les ressources et suit le protocole HTTP de très près. Il est le plus standardisé, et permet de profiter nativement des fonctionnalités de HTTP (content-negotiation, caching, requêtes conditionnelles etc.).\nLa gestion d’erreur est standardisée : 4XX, 5XX.\n\n\ngRPC (g pour Google, et RPC pour Remote Procedure Call) consiste à appeler des fonctions dans son code en leur donnant des paramètres. Ces fonctions sont complètement custom de la part du provider, et ne se basent pas sur un modèle comme les ressources pour REST.\nIl est en général utilisé avec des données formatées en protobuf (à la place de JSON), qui est un format binaire et ne répétant pas le nom de chaque propriété contrairement à JSON (donc peut diviser par presque 2 la taille des données transférées). Par contre c'est moins connu que JSON, et moins facile à afficher / débugger parce que binaire.\nIl est construit par dessus HTTP mais n’utilise pas la plupart de ses fonctionnalités comme REST. Par contre, il permet les communications bidirectionnelles grâce à HTTP2.\nIl a un format standard d’erreurs inspiré de REST, et pas de mécanisme standard de caching.\nIl faut l’utiliser plutôt que REST quand on a une API privée vers client privé (communication entre services d’un même backend), quand les millisecondes gagnées importent vraiment.\n\n\nGraphQL adopte le même modèle basé sur les fonctions que gRPC pour la création/modification, par contre pour le querying il a un modèle basé sur les données : c’est l’utilisateur qui spécifie non seulement les propriétés qu’il veut, mais aussi avec quel autre donnée il veut faire un lien etc.\nPour le query l’utilisateur a une grande flexibilité, mais il est aussi en capacité de demander des requêtes extrêmement complexes et coûteuses. Le rate-limiting peut ne pas suffire pour se protéger parce qu’on ne peut pas prévoir ce que le client va faire.\nIl est agnostique niveau protocole, et est utilisé la plupart du temps par dessus HTTP avec des POST sur un endpoint unique.\nIl a un format standard d’erreurs qui consiste à donner un texte d’erreur, et la ligne qui a causé l’erreur dans la requête GraphQL et la propriété problématique dans la donnée en réponse (si une réponse partielle a pu être faite).\nIl n’a pas de mécanisme standard pour le caching, et celui-ci est difficile étant donné la flexibilité des données.\nIl faut aller vers GraphQL plutôt que REST quand on est dans un contexte d’API privée et alimentant des périphériques mobiles qui ont besoin d’économiser la quantité de données échangée.\n\n\nA noter qu’on peut tout à fait avoir par ex une API BFF spécialisée qui expose du GraphQL pour les mobiles, et qui consomme une API REST ou gRPC plus générique dans notre backend.","12---documenting-an-api#12 - Documenting an API":"Créer une documentation est une manière de tester le design : si on est incapable de l’expliquer c’est qu’il y a peut être des incohérences.\nDans la documentation d’une API, il faut bien évidemment une référence des endpoints possibles. Typiquement le genre de documentation qu’on peut générer à partir d’une spécification OpenAPI.\nPour fournir plusieurs exemples dans OpenAPI :\nrequestBody:\ncontent:\n\"application/json\":\n#...\nexamples:\npremierExemple:\n# ...\ndeuxiemeExemple:\nsummary: Résumé du 2ème exemple\ndescription: Description du 2ème exemple\nvalue:\nprop1: \"blablabla\"\nprop2: 3\nprop3: \"bliblibli\"\n\nLes outils comme ReDoc sont même capables de générer des exemples de données JSON complets à partir des types et formats déclarés dans la spécification OpenAPI.\nIl faut décrire ce que fait chaque endpoint, les données qu’il prend et renvoie, mais aussi chaque cas possible de réponse de réussite ou d’erreur. Et ne pas oublier la sécurité (scopes, autorisations). Tout ça se fait dans OpenAPI.\nA propos de l’idée de générer la documentation à partir du code (avec éventuellement un format OpenAPI automatique intermédiaire) versus maintenir le fichier OpenAPI à la main :\nL’avantage c’est la synchronisation plus facile entre le code et la doc.\nLes inconvénients sont surtout le manque de flexibilité : la documentation finale contiendra moins de choses si tout vient du code. Et le risque que l’API ait plus tendance à exposer le point de vue du provider, ce qu’on souhaite éviter.\n\n\n\n\nIl faut aussi un guide utilisateur qui va expliquer l’API d’un point de vue global, les principes généraux, comment éventuellement s’inscrire pour avoir un token etc.\nPour le coup cette partie doit être écrite à la main, par exemple en markdown.\nOn y ajoute des cas d’usage détaillés, qui seront le reflet de l’API goal canvas qu’on a utilisé pour designer notre API : telle personne a besoin de telle et telle info pour pour faire telle chose + le détail des infos qu’elle donne, les API calls, les réponses qu’elle reçoit.\nOn peut aussi y ajouter les choses communes à l’API comme le type de pagination utilisée, les formats de données et plus généralement les headers HTTP supportés etc.\n\n\nIl peut être utile d’avoir une documentation spécifique pour les développeurs de l’implémentation.\nElle peut contenir des choses que les consommateurs n’ont pas besoin de savoir, et qui sont liées à la manière dont l’API s’intègre avec l’implémentation. Par exemple des infos sur d’où vient telle ou telle propriété dans le système, des liens vers des formats pour aider à implémenter etc.\n\n\nEt enfin il faut un changelog listant pour chaque version ce qui est ajouté/modifié, et surtout les breaking changes.\nOpenAPI ne permet pas de faire de changelog, mais il permet d’ajouter une propriété deprecated: true sur les paramètres, les endpoints, les propriétés.","13---growing-apis#13 - Growing APIs":"A partir du moment où plusieurs personnes travaillent sur l’API, ou qu’il y a plusieurs APIs dans l’organisation, il y a de fortes chances pour que de l’incohérence s’installe entre APIs et au sein d’une API. Il faut pour ça rédiger des API guidelines à destination des designers d’API.\nIl y a 3 parties :\nIl y a d’abord les reference guidelines qui sont le minimum : le document décrit quelles méthodes, codes de statut, headers sont utilisés couramment dans l’API, le format des ressources (la structure de leur path), la structure habituelle des données, la manière dont la pagination est gérée etc.\nOn y définit aussi les termes utilisés dans l’API (ressource, version etc.) à l’image du langage ubiquitaire du DDD.\n\n\nEnsuite il faut les use case guidelines qui sont une version plus digeste et prête à l’emploi pour étendre l’API. Par exemple comment créer un nouvel élément dans l’API, et on montre pas à pas, comme un tuto.\nOn fait bien attention à utiliser le même vocabulaire partagé que celui défini dans les reference guidelines.\n\n\nEnfin, on peut ajouter des design process guidelines où on va donner des référence vers des ressources utiles pour creuser tel ou tel aspect utile pour le design de l’API. Par exemple des liens vers des spécifications, tutoriels, livres etc.\n\n\nLe site webconcepts.info rassemble des informations fiables sur HTTP, OAuth et d’autres concepts liés avec des liens vers les spécifications, intéressant quand on cherche la bonne spécification à suivre.\nLe site apistylebook.com rassemble des API guidelines célèbres dont on peut s’inspirer.\nIl faut écrire les guidelines petit à petit de manière itérative, en n’y mettant que ce dont on est sûr à chaque fois, et en étant prêt à revoir le contenu.\nIl faut aussi que ce soit un travail collectif et permanent de tous ceux qui sont en charge du design de l’API, sinon les guidelines ne seront pas respectées.\n\n\n\n\nUne API étant potentiellement difficile à changer, et pouvant poser des problèmes importants de sécurité, il est important qu’il y ait une procédure de review sérieuse à chaque changement.\nIl faut avant tout identifier les besoins derrière un changement d’API.\nLa méthode des 5 “Pourquoi ?” (Pourquoi vouloir faire ceci ? Parce que cela. Et pourquoi cela ? etc.) permet en général de remonter au besoin racine.\nIl faut aussi bien comprendre le contexte des utilisateurs, du provider de l’API, les aspects sécurité concernés (données sensibles ?) etc.\n\n\nEnsuite il faut linter l’API :\nExaminer si le design proposé suit bien les guidelines.\nS’il est cohérent avec le reste de l’API (même si c’est pas encore écrit dans les guidelines).\nSi des erreurs de forme ne sont pas glissées dedans. Par exemple des propriétés abrégées ou manifestement pas user friendly.\nUne partie peut être faite avec des outils automatiques pour gagner du temps.\n\n\nIl faut reviewer le design du point de vue du provider.\nIl faut vérifier que l’API est extensible, sécurisée et ne devrait à priori pas poser de problèmes de performance ou d’implémentation.\n\n\nLe reviewer du point de vue du consommateur.\nSe demander si on n’expose pas la structure interne du provider au lieu de répondre au besoin de l’utilisateur. Vérifier que l’API est facile à utiliser, que le flow est suffisamment simple etc.\nObtenir du feedback des clients peut aussi avoir de la valeur pour obtenir une bonne API.\n\n\nEt enfin une fois que l’implémentation est faite (ou en cours), vérifier qu’elle correspond bien avec la spécification de l’API.\nÇa se fait avec des tests unitaires, et des tests au niveau de l’API.\nIl faut toujours réaliser les tests de sécurité (vérifier le contrôle d’accès, vérifier que les données sensibles ne fuitent pas).\nIl faut tester que l’API fonctionne comme prévu au runtime, pas seulement avec des techniques statiques sans faire tourner l’API.\nIl ne faut pas oublier de tester le caractère obligatoire des propriétés ou encore le fait qu’une valeur doive être entre un min et un max.\nIl faut tester l’API aussi dans toute la chaîne réseau pour vérifier que des proxis, pare-feux, routeurs ne nous causent pas de problèmes."}}}