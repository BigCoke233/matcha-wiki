(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[872],{9979:function(e,n,s){(window.__NEXT_P=window.__NEXT_P||[]).push(["/books/designing-cloud-data-platforms",function(){return s(1556)}])},9923:function(e,n,s){"use strict";s.d(n,{Z:function(){return c}});var i=s(5893);s(7294);let r="My detailed reading notes from computer science books",l="/reading-notes";var t=s(1163),a=s(5675),d=s.n(a);let o={logo:(0,i.jsx)(function(){return(0,i.jsx)(d(),{src:"".concat(l,"/logo.png"),alt:"Reading notes homepage",width:30,height:30})},{}),project:{link:"https://github.com/mkrtchian/reading-notes"},docsRepositoryBase:"https://github.com/mkrtchian/reading-notes/blob/main",footer:{text:"Made by Roman Mkrtchian"},head:function(){let e="Reading notes";return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)("meta",{name:"msapplication-TileColor",content:"#fff"}),(0,i.jsx)("meta",{httpEquiv:"Content-Language",content:"en"}),(0,i.jsx)("meta",{name:"description",content:r}),(0,i.jsx)("meta",{property:"og:title",content:e}),(0,i.jsx)("meta",{property:"og:description",content:r}),(0,i.jsx)("meta",{name:"apple-mobile-web-app-title",content:e}),(0,i.jsx)("link",{rel:"icon",type:"image/x-icon",href:"".concat(l,"/favicon.ico")})]})},feedback:{content:()=>(0,i.jsx)(i.Fragment,{children:"Question? Give me feedback →"}),labels:"feedback"},useNextSeoProps:function(){let{route:e}=(0,t.useRouter)(),n={description:r};return"/"!==e?n.titleTemplate="%s – Reading notes":n.titleTemplate="%s",n},i18n:[]};var c=o},1556:function(e,n,s){"use strict";s.r(n),s.d(n,{default:function(){return x}});var i=s(5893),r=s(2673),l=s(902),t=s(9923);s(9966);var a=s(1151);function d(e){let{children:n}=e;return(0,i.jsx)("em",{style:{color:"#3d85c6",fontWeight:"bold",fontStyle:"normal"},children:n})}s(5675);let o={MDXContent:function(){let e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{},{wrapper:n}=Object.assign({},(0,a.ah)(),e.components);return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)},pageOpts:{filePath:"pages/books/designing-cloud-data-platforms.mdx",route:"/books/designing-cloud-data-platforms",headings:[{depth:1,value:"Designing Cloud Data Platforms",id:"designing-cloud-data-platforms"},{depth:2,value:"1 - Introducing the data platform",id:"1---introducing-the-data-platform"},{depth:2,value:"2 - Why a data platform and not just a data warehouse",id:"2---why-a-data-platform-and-not-just-a-data-warehouse"},{depth:2,value:"3 - Getting bigger and leveraging the Big 3: Amazon, Microsoft, and Google",id:"3---getting-bigger-and-leveraging-the-big-3-amazon-microsoft-and-google"},{depth:2,value:"4 - Getting data into the platform",id:"4---getting-data-into-the-platform"}],pageMap:[{kind:"Meta",data:{index:"Introduction",books:"Reading notes"}},{kind:"Folder",name:"books",route:"/books",children:[{kind:"Meta",data:{"continuous-discovery-habits":"Continuous Discovery Habits","designing-cloud-data-platforms":"Designing Cloud Data Platforms","designing-data-intensive-applications":"Designing Data-Intensive Applications","effective-kafka":"Effective Kafka","get-your-hands-dirty-on-clean-architecture":"Get Your Hands Dirty on Clean Architecture","learning-domain-driven-design":"Learning Domain-Driven Design","learning-to-scale":"Learning to Scale","monolith-to-microservices":"Monolith to Microservices",refactoring:"Refactoring: Improving the Design of Existing Code","reinventing-organizations":"Reinventing Organizations","team-topologies":"Team Topologies","the-design-of-web-apis":"The Design of Web APIs","unit-testing":"Unit Testing: Principles, Practices, and Patterns"}},{kind:"MdxPage",name:"continuous-discovery-habits",route:"/books/continuous-discovery-habits"},{kind:"MdxPage",name:"designing-cloud-data-platforms",route:"/books/designing-cloud-data-platforms"},{kind:"MdxPage",name:"designing-data-intensive-applications",route:"/books/designing-data-intensive-applications"},{kind:"MdxPage",name:"effective-kafka",route:"/books/effective-kafka"},{kind:"MdxPage",name:"get-your-hands-dirty-on-clean-architecture",route:"/books/get-your-hands-dirty-on-clean-architecture"},{kind:"MdxPage",name:"learning-domain-driven-design",route:"/books/learning-domain-driven-design"},{kind:"MdxPage",name:"learning-to-scale",route:"/books/learning-to-scale"},{kind:"MdxPage",name:"monolith-to-microservices",route:"/books/monolith-to-microservices"},{kind:"MdxPage",name:"refactoring",route:"/books/refactoring"},{kind:"MdxPage",name:"reinventing-organizations",route:"/books/reinventing-organizations"},{kind:"MdxPage",name:"team-topologies",route:"/books/team-topologies"},{kind:"MdxPage",name:"the-design-of-web-apis",route:"/books/the-design-of-web-apis"},{kind:"MdxPage",name:"unit-testing",route:"/books/unit-testing"}]},{kind:"MdxPage",name:"index",route:"/"}],flexsearch:{codeblocks:!0},title:"Designing Cloud Data Platforms"},pageNextRoute:"/books/designing-cloud-data-platforms",nextraLayout:l.ZP,themeConfig:t.Z};function c(e){let n=Object.assign({h1:"h1",h2:"h2",ul:"ul",li:"li",strong:"strong",em:"em",pre:"pre",code:"code",span:"span"},(0,a.ah)(),e.components);return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.h1,{children:"Designing Cloud Data Platforms"}),"\n",(0,i.jsx)(n.h2,{id:"1---introducing-the-data-platform",children:"1 - Introducing the data platform"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Les ",(0,i.jsx)(n.strong,{children:"analytics"})," permettent essentiellement d’obtenir des m\xe9triques pour faire des choix business.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Avant l’av\xe8nement des ordinateurs, les entreprises utilisaient des moyens manuels, et leur intuition."}),"\n",(0,i.jsx)(n.li,{children:"Dans les ann\xe9es 80 on a vu \xe9merger le concept de data warehouse, qui est une base centralis\xe9e de donn\xe9es venant de diverses sources."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Les ",(0,i.jsx)(n.strong,{children:"data warehouses"})," posent de plus en plus de ",(0,i.jsx)(n.strong,{children:"probl\xe8mes"})," de nos jours.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Les tendances suivantes y contribuent :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Les donn\xe9es sont issues de sources de diverses nature, y compris certaines d’entre-elles non structur\xe9es, et leur volume est de plus en plus important."}),"\n",(0,i.jsx)(n.li,{children:"Le d\xe9coupage des applications en microservices fait que collecter des donn\xe9es revient forc\xe9ment \xe0 devoir agr\xe9ger de multiples sources."}),"\n",(0,i.jsx)(n.li,{children:"Les data scientists ont aussi besoin d’acc\xe9der \xe0 une version brute de la donn\xe9e, et cet usage ne peut pas passer par un data warehouse."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Elles ont du mal avec les ",(0,i.jsx)(n.strong,{children:"3V"})," (Variety, Volume, Velocity).","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Variety"})," : les data warehouses ne supportent que les ",(0,i.jsx)(n.em,{children:"structured data"})," dont le sch\xe9ma est stable, c’est-\xe0-dire en pratique qui sont issues de DB relationnelles.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Or avec l’av\xe8nement des SaaS, des r\xe9seaux sociaux, et de l’IoT, on se retrouve avec :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Des ",(0,i.jsx)(n.em,{children:"semistructured data"})," du type JSON, Avro etc, dont le sch\xe9ma varie souvent."]}),"\n",(0,i.jsxs)(n.li,{children:["Des ",(0,i.jsx)(n.em,{children:"unstructured data"})," comme le binaire, le son, la vid\xe9o."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Volume"})," : le fait que dans un data warehouse, la puissance de calcul et le stockage doivent se trouver sur ",(0,i.jsx)(n.strong,{children:"la m\xeame machine physique"}),", implique qu’on ne peut pas scaler les deux s\xe9par\xe9ment, et donc les co\xfbts explosent.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"M\xeame les petites organisations peuvent \xeatre amen\xe9es \xe0 traiter plusieurs TB de donn\xe9es."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Velocity"})," : les data warehouses ne sont pas adapt\xe9es aux analytics en mode real time, elles sont plus orient\xe9es batch processing."]}),"\n",(0,i.jsx)(n.li,{children:"Le machine learning en particulier pose tous les probl\xe8mes en m\xeame temps : il n\xe9cessite une grande quantit\xe9 de donn\xe9es vari\xe9es, et accapare la puissance de calcul du data warehouse."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Les ",(0,i.jsx)(n.strong,{children:"data lakes"})," r\xe9pondent en partie \xe0 ces probl\xe8mes.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["L’id\xe9e principale des data lakes c’est qu’on ",(0,i.jsx)(n.strong,{children:"stocke de la donn\xe9e telle quelle"})," (ou quasi), et qu’on essayera de la traiter et de lui coller un sch\xe9ma d\xe8s qu’on en aura besoin."]}),"\n",(0,i.jsxs)(n.li,{children:["Les data lakes se sont g\xe9n\xe9ralis\xe9s \xe0 partir de 2006 avec l’arriv\xe9e de ",(0,i.jsx)(d,{children:"Hadoop"}),", qui est un ",(0,i.jsx)(n.strong,{children:"filesystem distribu\xe9 sur plusieurs machines"})," pas ch\xe8res.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Hadoop r\xe9pond en partie aux 3V :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["A la ",(0,i.jsx)(n.em,{children:"Variety"})," par l’\xe9criture schema-less."]}),"\n",(0,i.jsxs)(n.li,{children:["Au ",(0,i.jsx)(n.em,{children:"Volume"})," par le fait que ce soit distribu\xe9 sur des machines pas ch\xe8res."]}),"\n",(0,i.jsxs)(n.li,{children:["A la ",(0,i.jsx)(n.em,{children:"Velocity"})," par la facilit\xe9 de streaming \xe0 partir du filesystem distribu\xe9."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Mais il a aussi des probl\xe8mes :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"C’est un syst\xe8me complexe qu’il faut installer sur un datacenter et g\xe9rer par des Ops exp\xe9riment\xe9s."}),"\n",(0,i.jsx)(n.li,{children:"D’un point de vue business, c’est plus difficile de travailler avec les outils qui traitent les donn\xe9es non structur\xe9es qu’avec du SQL comme dans un data warehouse."}),"\n",(0,i.jsx)(n.li,{children:"Bien qu’il soit distribu\xe9 sur de petites machines pas ch\xe8res, le computing et le stockage ne sont pas s\xe9par\xe9s, ce qui limite quand m\xeame la r\xe9duction de co\xfbt quand on a besoin de beaucoup de l’un sans l’autre."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Le ",(0,i.jsx)(n.strong,{children:"cloud public"})," vient r\xe9pondre aux probl\xe8mes de Hadoop.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Les data warehouses et les data lakes ont \xe9t\xe9 propos\xe9s par les cloud providers, avec de nombreux avantages :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"La possibilit\xe9 de scaler la puissance de calcul et le stockage s\xe9par\xe9ment."}),"\n",(0,i.jsx)(n.li,{children:"Payer uniquement \xe0 l’usage des machines qu’on emprunte."}),"\n",(0,i.jsx)(n.li,{children:"Ne plus avoir \xe0 g\xe9rer la complexit\xe9 de l’infrastructure."}),"\n",(0,i.jsx)(n.li,{children:"Des outils et frameworks avanc\xe9s d\xe9velopp\xe9s par les cloud providers autour de leurs produits."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Exemple : ",(0,i.jsx)(n.strong,{children:"AWS EMR"})," permet de lancer un cluster sur lequel on va pouvoir ex\xe9cuter des jobs ",(0,i.jsx)(n.strong,{children:"Hadoop"})," et ",(0,i.jsx)(n.strong,{children:"Spark"}),",","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"On a juste \xe0 indiquer le nombre de nœuds qu’on veut, et les packages qu’on veut installer dessus."}),"\n",(0,i.jsxs)(n.li,{children:["Et on a la possibilit\xe9 de faire des allers-retours vers ",(0,i.jsx)(n.strong,{children:"S3"})," pour scaler diff\xe9remment le calcul et le stockage."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["La ",(0,i.jsx)(n.strong,{children:"cloud data platform"})," moderne utilise \xe0 la fois le data warehouse et le data lake, h\xe9berg\xe9s dans un cloud public, chacun d’entre eux remplissant un usage particulier.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Pour \xeatre polyvalente et pas ch\xe8re, la data platform doit avoir des ",(0,i.jsx)(n.strong,{children:"4 composants principaux faiblement coupl\xe9s"}),", interagissant entre-eux avec une API bien d\xe9finie.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Ingestion layer"})," : on va chercher les donn\xe9es chez les diff\xe9rents types de sources (DB relationnelle, DB NoSQL, API externes etc.).","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"On va en g\xe9n\xe9ral utiliser un ensemble d’outils open source ou commerciaux pour chaque type de donn\xe9es \xe0 aller chercher."}),"\n",(0,i.jsxs)(n.li,{children:["Il ne faut ",(0,i.jsx)(n.strong,{children:"surtout pas alt\xe9rer la donn\xe9e \xe0 cette \xe9tape"}),", pour que la donn\xe9e brute soit disponible pour les data scientists qui en auraient l’usage."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Storage layer"})," : on utilise le stockage cloud comme stockage de notre ",(0,i.jsx)(n.em,{children:"data lake"}),", dans lequel on met ce qu’on a ing\xe9r\xe9.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Le stockage cloud a l’avantage de ne pas avoir besoin de planifier la capacit\xe9 de stockage : il grossit automatiquement au besoin."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Processing layer"})," : on transforme la donn\xe9e pour la rendre utilisable par la plupart des clients de la plateforme.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["C’est la partie calcul de notre ",(0,i.jsx)(n.em,{children:"data lake"}),", il va lire depuis le cloud storage puis \xe9crire \xe0 nouveau dedans."]}),"\n",(0,i.jsxs)(n.li,{children:["Dans le cas du ",(0,i.jsx)(n.strong,{children:"streaming"}),", on ne passe pas par le storage layer qui prend trop de temps, mais on envoie la donn\xe9e ",(0,i.jsx)(n.strong,{children:"directement au processing layer"}),", qui va ensuite la rendre disponible au layer d’apr\xe8s."]}),"\n",(0,i.jsxs)(n.li,{children:["Le processing est g\xe9n\xe9ralement fait avec des outils open source, les plus connus \xe9tant ",(0,i.jsx)(n.strong,{children:"Spark"}),", ",(0,i.jsx)(n.strong,{children:"Beam"})," et ",(0,i.jsx)(n.strong,{children:"Flink"}),"."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Serving layer"})," : on rend la donn\xe9e disponible sous divers formats, selon les besoins des clients de la plateforme.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Les usages peuvent \xeatre :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Des analystes qui ont besoin d’ex\xe9cuter des requ\xeates SQL sur la donn\xe9e.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["On peut charger la donn\xe9e dans un ",(0,i.jsx)(n.em,{children:"data warehouse"})," chez le cloud provider."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Des applications qui ont besoin d’un acc\xe8s rapide \xe0 la donn\xe9e.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"On peut la charger dans une key / value DB, ou une document DB."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Des \xe9quipes de data scientists / engineers ont besoin de transformer la donn\xe9e eux-m\xeames.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["On peut leur donner acc\xe8s au storage du ",(0,i.jsx)(n.em,{children:"data lake"}),", et les laisser utiliser ",(0,i.jsx)(n.strong,{children:"Spark"}),", ",(0,i.jsx)(n.strong,{children:"Beam"})," ou ",(0,i.jsx)(n.strong,{children:"Flink"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["La cloud data platform r\xe9pond aux 3V :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["L’ingestion layer coupl\xe9 au stockage sans sch\xe9ma permet une grande ",(0,i.jsx)(n.em,{children:"Variety"})," des donn\xe9es."]}),"\n",(0,i.jsxs)(n.li,{children:["La s\xe9paration calcul / stockage et le fait de ne payer que ce qu’on utilise permet d’optimiser les co\xfbts, et d’avoir un gros ",(0,i.jsx)(n.em,{children:"Volume"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["La possibilit\xe9 d’envoyer directement au ",(0,i.jsx)(n.em,{children:"processing layer"})," permet de la ",(0,i.jsx)(n.em,{children:"Velocity"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["On peut aussi prendre en compte deux autres V :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["La ",(0,i.jsx)(n.em,{children:"Veracity"})," qui indique le niveau de ",(0,i.jsx)(n.em,{children:"data governance"}),", c’est-\xe0-dire la qualit\xe9 de la donn\xe9e. On l’obtient it\xe9rativement, au cours d’\xe9tapes au sein du data lake."]}),"\n",(0,i.jsxs)(n.li,{children:["Et la ",(0,i.jsx)(n.em,{children:"Value"})," qu’on peut tirer de la donn\xe9e, qui peut \xeatre plus \xe9lev\xe9e si on prend plus de donn\xe9es en amont de notre processus de nettoyage."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Il faut comprendre les ",(0,i.jsx)(n.strong,{children:"cas d’usages principaux"})," d’un ",(0,i.jsx)(n.em,{children:"data lake"}),", pour \xe9viter de le transformer en ",(0,i.jsx)(n.em,{children:"data swamp"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Parmi les plus courants il y a la ",(0,i.jsx)(n.strong,{children:"vue 360\xb0 des clients"}),", o\xf9 il s’agit de r\xe9cup\xe9rer toutes les donn\xe9es d’interaction avec eux, pour proposer ensuite des services plus personnalis\xe9s, vendre plus etc."]}),"\n",(0,i.jsxs)(n.li,{children:["Il y a aussi les ",(0,i.jsx)(n.strong,{children:"donn\xe9es venant d’IoT"}),", qui ont la particularit\xe9 d’\xeatre incertaines et d’avoir un gros volume, ce qui rend l’utilisation du ",(0,i.jsx)(n.em,{children:"data warehouse"})," peu int\xe9ressante."]}),"\n",(0,i.jsxs)(n.li,{children:["Et enfin il y a le ",(0,i.jsx)(n.strong,{children:"machine learning"})," qui a besoin d’une tr\xe8s grande quantit\xe9 de donn\xe9es, et qui tire avantage de puissance de calcul s\xe9par\xe9e des autres use-cases gr\xe2ce au ",(0,i.jsx)(n.em,{children:"data lake"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"2---why-a-data-platform-and-not-just-a-data-warehouse",children:"2 - Why a data platform and not just a data warehouse"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Ce chapitre donne des ",(0,i.jsxs)(n.strong,{children:["arguments pour le choix d’une ",(0,i.jsx)(n.em,{children:"cloud data platform"}),", plut\xf4t qu’une simple ",(0,i.jsx)(n.em,{children:"data warehouse"})]}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["On impl\xe9mente les deux solutions pour une situation d’",(0,i.jsx)(n.strong,{children:"exemple"})," qu’on va utiliser dans ce chapitre :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Nous sommes l’\xe9quipe data, et le d\xe9partement marketing a besoin que nous r\xe9cup\xe9rions deux sources de donn\xe9es et qu’on les corr\xe8le r\xe9guli\xe8rement.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"L’une des sources est une table de campagnes de marketing, issue d’une DB MySQL interne."}),"\n",(0,i.jsxs)(n.li,{children:["Et l’autre est constitu\xe9e de fichiers CSV de clics utilisateurs, issus de logs applicatifs (et donc ",(0,i.jsx)(n.em,{children:"semistructured"}),")."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.li,{children:"On part sur Microsoft Azure pour les deux solutions."}),"\n",(0,i.jsxs)(n.li,{children:["Concernant l’impl\xe9mentation ",(0,i.jsx)(n.em,{children:"data warehouse only"})," :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["1 - On va utiliser deux ",(0,i.jsx)(n.strong,{children:"Azure Data Factory"})," pour r\xe9cup\xe9rer la donn\xe9e dans le serveur de DB et dans les fichiers CSV dans le serveur SFTP. C’est notre ",(0,i.jsx)(n.em,{children:"ingest layer"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["2 - Ensuite on redirige \xe7a vers l’",(0,i.jsx)(n.strong,{children:"Azure Synapse"}),", qui est la data warehouse de chez Azure. Elle va faire office de ",(0,i.jsx)(n.em,{children:"store layer"}),", ",(0,i.jsx)(n.em,{children:"process layer"})," et ",(0,i.jsx)(n.em,{children:"serve layer"}),"."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Concernant l’impl\xe9mentation ",(0,i.jsx)(n.em,{children:"cloud data platform"})," :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["1 - On a notre ",(0,i.jsx)(n.em,{children:"ingest layer"})," avec ",(0,i.jsx)(n.strong,{children:"Azure Data Factory"}),", qui redirige les donn\xe9es vers le ",(0,i.jsx)(n.em,{children:"store layer"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["2 - Le ",(0,i.jsx)(n.em,{children:"store layer"})," est impl\xe9ment\xe9 avec ",(0,i.jsx)(n.strong,{children:"Azure Blob Storage"}),". Il s’agit d’un stockage de type ",(0,i.jsx)(n.em,{children:"data lake"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["3 - On a un ",(0,i.jsx)(n.em,{children:"process layer"})," qui utilise ",(0,i.jsx)(n.strong,{children:"Azure Databricks"}),", et qui fait tourner ",(0,i.jsx)(n.strong,{children:"Spark"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["4 - Le ",(0,i.jsx)(n.em,{children:"serve layer"})," utilise enfin ",(0,i.jsx)(n.strong,{children:"Azure Synapse"})," qui est le data warehouse."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Concernant l’",(0,i.jsx)(n.strong,{children:"ingestion"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Pour la version ",(0,i.jsx)(n.em,{children:"data warehouse only"})," :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["La pipeline contient :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Des ",(0,i.jsx)(n.em,{children:"linked services"})," : ici la ",(0,i.jsx)(n.em,{children:"data source"})," MySQL en entr\xe9e, et la ",(0,i.jsx)(n.em,{children:"data sink"})," ",(0,i.jsx)(n.strong,{children:"Azure Synapse"})," en sortie."]}),"\n",(0,i.jsxs)(n.li,{children:["Des ",(0,i.jsx)(n.em,{children:"data sets"})," : il s’agit de la description du sch\xe9ma de donn\xe9es d’entr\xe9e et de sortie, et leur mapping."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Si le sch\xe9ma de la DB source change, il faudra mettre \xe0 jour le sch\xe9ma d\xe9fini dans la pipeline et le mapping.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Mais surtout il faudra ",(0,i.jsx)(n.strong,{children:"g\xe9rer soi-m\xeame la migration"})," du ",(0,i.jsx)(n.em,{children:"data sink"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Pour la version ",(0,i.jsx)(n.em,{children:"cloud data platform"})," :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Cette fois le ",(0,i.jsx)(n.em,{children:"data sink"})," est un ",(0,i.jsx)(n.strong,{children:"Azure Blob Storage"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Il n’y a plus besoin de sp\xe9cifier les sch\xe9mas et le mapping entre input et output puisque l’output accueille la donn\xe9e telle quelle."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Si le sch\xe9ma de la DB source change, il n’y a ",(0,i.jsx)(n.strong,{children:"rien \xe0 faire c\xf4t\xe9 ingestion"})," : on \xe9crira de toute fa\xe7on la donn\xe9e dans un nouveau fichier.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"On d\xe9place le probl\xe8me de mapping plus loin."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Concernant le ",(0,i.jsx)(n.strong,{children:"processing"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Dans la version ",(0,i.jsx)(n.em,{children:"data warehouse only"})," :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["On va charger les deux donn\xe9es :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"La DB MySQL sans charger sa structure parce qu’elle est d\xe9j\xe0 relationnelle."}),"\n",(0,i.jsx)(n.li,{children:"La donn\xe9e CSV semistructur\xe9e dans des rows de type texte qu’on parsera en JSON avec une fonction SQL built-in."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["La ",(0,i.jsx)(n.strong,{children:"requ\xeate SQL"})," qu’on va \xe9crire aura les d\xe9savantages suivants :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Elle sera ",(0,i.jsx)(n.strong,{children:"peu lisible"}),", \xe0 cause du code de parsing n\xe9cessaire.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"On pourrait la rendre plus lisible en pr\xe9-parsant la donn\xe9e, mais \xe7a veut dire plus de temps et des co\xfbts plus \xe9lev\xe9s."}),"\n",(0,i.jsxs)(n.li,{children:["Une autre solution de lisibilit\xe9 pourrait \xeatre d’ajouter des UDF (User Defined Functions), qu’il faudrait maintenir et d\xe9ployer sur chaque instance d’",(0,i.jsx)(n.strong,{children:"Azure Synapse"}),"."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Elle sera ",(0,i.jsx)(n.strong,{children:"difficile \xe0 tester"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["Elle risque de ne pas profiter de la ",(0,i.jsx)(n.strong,{children:"performance"})," offerte par la structure en colonne du data warehouse, parce que les donn\xe9es texte qu’on parse en JSON ne sont pas organisables physiquement en colonnes."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Dans la version ",(0,i.jsx)(n.em,{children:"cloud data platform"})," :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["On a la possibilit\xe9 d’utiliser un ",(0,i.jsx)(n.em,{children:"distributed data processing engine"})," comme ",(0,i.jsx)(n.strong,{children:"Apache Spark"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"On pourra \xe9crire des requ\xeates SQL pour des exp\xe9rimentations rapides."}),"\n",(0,i.jsxs)(n.li,{children:["Et on pourra aussi \xe9crire du code ",(0,i.jsx)(n.strong,{children:"lisible, maintenable et testable"})," dans un langage comme Python ou Scala, quand il s’agit de projet de plus long terme."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Concernant l’",(0,i.jsx)(n.strong,{children:"acc\xe8s \xe0 la donn\xe9e"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Il peut y avoir plusieurs types de consommateurs :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Des utilisateurs plut\xf4t ",(0,i.jsx)(n.strong,{children:"orient\xe9s business"})," comme des \xe9quipes marketing.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Ils vont pr\xe9f\xe9rer utiliser des outils de reporting type ",(0,i.jsx)(n.strong,{children:"Power BI"}),", et donc auront besoin de la donn\xe9e sous forme relationnelle, par exemple dans ",(0,i.jsx)(n.strong,{children:"Azure Synapse"}),"."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Des utilisateurs orient\xe9s ",(0,i.jsx)(n.strong,{children:"data analyse / data science"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Ils pourront b\xe9n\xe9ficier de SQL qu’ils utilisent souvent directement, au travers de ",(0,i.jsx)(n.strong,{children:"Spark SQL"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["Ils pourront avoir acc\xe8s \xe0 des donn\xe9es non filtr\xe9es pour leur projets data science, gr\xe2ce ",(0,i.jsx)(n.strong,{children:"Spark"})," directement."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Au final la ",(0,i.jsx)(n.em,{children:"cloud data platform"}),", qui contient \xe0 la fois la donn\xe9e sous forme brute dans le ",(0,i.jsx)(n.em,{children:"data lake"}),", et la donn\xe9e dans le ",(0,i.jsx)(n.em,{children:"data warehouse"}),", est ",(0,i.jsx)(n.strong,{children:"adapt\xe9e \xe0 chaque usage"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["A propos des ",(0,i.jsx)(n.strong,{children:"co\xfbts financiers"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Il est difficile de comparer les co\xfbts des services cloud.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"En g\xe9n\xe9ral on constate que le stockage est plut\xf4t pas cher, et que l’essentiel des co\xfbts se trouve dans les calculs."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["L’",(0,i.jsx)(n.strong,{children:"elastic scaling"})," consiste \xe0 pouvoir calibrer le service pour l’usage exact qu’on en a, et de ne pas avoir \xe0 payer plus.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"C’est un des \xe9l\xe9ments qui permet de vraiment optimiser les co\xfbts."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Pour la version ",(0,i.jsx)(n.em,{children:"data warehouse only"}),", l’essentiel des co\xfbts va aller dans ",(0,i.jsx)(n.strong,{children:"Azure Synapse"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Le scaling de ce service peut prendre des dizaines de minutes, donc c’est quelque chose qu’on ne peut faire que de temps en temps."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Pour la version ",(0,i.jsx)(n.em,{children:"cloud data platform"}),", l’essentiel des co\xfbts est port\xe9 par le ",(0,i.jsx)(n.em,{children:"processing layer"}),", par exemple ",(0,i.jsx)(n.strong,{children:"Spark"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Spark"})," est particuli\xe8rement \xe9lastique, au point o\xf9 il est commun de d\xe9marrer une instance juste le temps d’une requ\xeate."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"3---getting-bigger-and-leveraging-the-big-3-amazon-microsoft-and-google",children:"3 - Getting bigger and leveraging the Big 3: Amazon, Microsoft, and Google"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Il existe un trade off entre choisir des ",(0,i.jsx)(n.strong,{children:"services vendor-specific"})," de type PaaS, et choisir des ",(0,i.jsx)(n.strong,{children:"services open source"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"D’un c\xf4t\xe9 on se couple au vendor mais on minimise les co\xfbts d’Ops, et de l’autre on permet une meilleure portabilit\xe9 mais on augmente les co\xfbts d’Ops."}),"\n",(0,i.jsx)(n.li,{children:"Les auteurs trouvent que la solution vendor-specific est celle qui a en g\xe9n\xe9ral le moins de d\xe9savantages."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Pour r\xe9pondre aux probl\xe9matiques de la data moderne, il faut une ",(0,i.jsx)(n.strong,{children:"architecture en 6 couches"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"1 - Data ingestion layer"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Son but est de :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Se connecter aux sources et r\xe9cup\xe9rer la donn\xe9e dans le data lake sans trop la modifier."}),"\n",(0,i.jsxs)(n.li,{children:["Enregistrer des statistiques et un statut dans le ",(0,i.jsx)(n.em,{children:"metadata repository"}),"."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Selon les auteurs, il vaut mieux mettre en place ",(0,i.jsx)(n.strong,{children:"\xe0 la fois un m\xe9canisme de type batch et un m\xe9canisme de type streaming"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"L'industrie est en train de se diriger vers le streaming, mais de nombreuses sources externes fournissent la donn\xe9e sous un format de type batch avec des \xe9l\xe9ments group\xe9s, par exemple CSV, JSON, XML."}),"\n",(0,i.jsxs)(n.li,{children:["On pourrait utiliser la partie batch pour ing\xe9rer des donn\xe9es par petits batchs, et \xe9viter de faire la version streaming. Mais \xe7a cr\xe9erait de la ",(0,i.jsx)(n.strong,{children:"dette technique"})," parce qu’on finira par avoir besoin du streaming \xe0 un moment ou un autre."]}),"\n",(0,i.jsxs)(n.li,{children:["La ",(0,i.jsx)(n.em,{children:"lambda architecture"})," consiste \xe0 avoir la donn\xe9e qui passe \xe0 la fois par le m\xe9canisme de batch et par le m\xe9canisme de streaming.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Cette duplication \xe9tait n\xe9cessaire parce que le streaming n’\xe9tait pas fiable dans les d\xe9buts de Hadoop, mais ce n’est plus le cas."}),"\n",(0,i.jsxs)(n.li,{children:["Le ",(0,i.jsx)(n.em,{children:"cloud data platform"})," ne consiste pas \xe0 faire une telle duplication : selon la source, la donn\xe9e va passer par le m\xe9canisme de streaming ou de batch."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["On entend parfois plusieurs choses diff\xe9rentes quand on parle de ",(0,i.jsx)(n.em,{children:"real time"})," pour des analytics :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["1 - La ",(0,i.jsx)(n.em,{children:"real time ingestion"})," consiste \xe0 avoir la donn\xe9e disponible pour de l’analyse d\xe8s qu’elle arrive."]}),"\n",(0,i.jsxs)(n.li,{children:["2 - Le ",(0,i.jsx)(n.em,{children:"real time analytics"})," consiste \xe0 avoir des fonctionnalit\xe9s d’analytics qui se mettent \xe0 jour \xe0 chaque arriv\xe9e de donn\xe9e.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Cette derni\xe8re est plus difficile \xe0 faire, donc il vaut mieux bien clarifier les besoins."}),"\n",(0,i.jsx)(n.li,{children:"Exemple : d\xe9tection de fraude en temps r\xe9el."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"2 - Storage layer"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Son but est de :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Stocker la donn\xe9e pour du court terme et du long terme."}),"\n",(0,i.jsx)(n.li,{children:"La rendre disponible pour la consommation streaming et la consommation batch."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Le ",(0,i.jsx)(n.strong,{children:"slow storage"})," est l\xe0 pour le mode batch.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"La donn\xe9e y est persist\xe9e pour pas cher, gr\xe2ce \xe0 la possibilit\xe9 de scaler le stockage sans ajouter de capacit\xe9 de calcul."}),"\n",(0,i.jsx)(n.li,{children:"Par contre les temps d’acc\xe8s sont grands."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Le ",(0,i.jsx)(n.strong,{children:"fast storage"})," est l\xe0 pour le mode streaming.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Il s’agit d’utiliser un outil qui est fait pour l’acc\xe8s rapide, comme ",(0,i.jsx)(n.strong,{children:"Apache Kafka"}),"."]}),"\n",(0,i.jsx)(n.li,{children:"Par contre, on n’a en g\xe9n\xe9ral pas la possibilit\xe9 de scaler le stockage sans ajouter de puissance de calcul, et donc les co\xfbts sont plus grands."}),"\n",(0,i.jsx)(n.li,{children:"On va donc purger r\xe9guli\xe8rement la donn\xe9e du fast storage, et de la transf\xe9rer dans le slow storage."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"3 - Processing layer"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Son but est de :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Lire la donn\xe9e depuis le stockage et y appliquer de la business logic."}),"\n",(0,i.jsx)(n.li,{children:"Persister la donn\xe9e modifi\xe9e \xe0 nouveau dans le stockage pour un usage par les data scientists."}),"\n",(0,i.jsx)(n.li,{children:"D\xe9livrer la donn\xe9e aux autres consumers."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Il faut un ou plusieurs outils qui permettent de r\xe9aliser des transformations de donn\xe9es, y compris avec du calcul distribu\xe9.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Un exemple peut \xeatre ",(0,i.jsx)(n.strong,{children:"Google Dataflow"}),", qui est une version PaaS d’",(0,i.jsx)(n.strong,{children:"Apache Beam"}),", qui supporte \xe0 la fois le mode streaming et le mode batch."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"4 - Technical metadata layer"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Son but est de :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Stocker des informations techniques sur chaque layer.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"\xc7a peut \xeatre les sch\xe9mas d’ingestion, le statut de telle ou telle \xe9tape, des statistiques sur les donn\xe9es ou les erreurs, etc."}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.li,{children:"Permettre \xe0 chaque d’ajouter/modifier ou consulter des informations."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Par exemple, le processing layer peut v\xe9rifier dans la ",(0,i.jsx)(n.em,{children:"technical metadata layer"})," qu’une certaine donn\xe9e est disponible pour aller la chercher, plut\xf4t que de demander \xe0 l’ingestion layer.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Ce qui permet un certain ",(0,i.jsx)(n.strong,{children:"d\xe9couplage"}),"."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["D’autres exemples peuvent impliquer des usages de ",(0,i.jsx)(n.strong,{children:"monitoring"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["La ",(0,i.jsx)(n.em,{children:"business metadata"})," est une autre notion qui peut avoir son layer, mais qui n’est pas explor\xe9e dans ce livre.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Il s’agit d’identifier l’usage business qui est fait de chaque donn\xe9e qu’on r\xe9cup\xe8re des sources, et d’en faire un catalogue."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Il n’y a pas vraiment d’outil unique qui permette de remplir ce r\xf4le pour le moment, donc on devra sans doute en utiliser plusieurs.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Par exemple ",(0,i.jsx)(n.strong,{children:"Confluent Schema Registry"})," et ",(0,i.jsx)(n.strong,{children:"Amazon Glue"})," peuvent supporter certains des cas d’usages."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"5 - Serving layer"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Son but est de :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Servir les consumers qui ont besoin de donn\xe9es relationnelles via une ",(0,i.jsx)(n.em,{children:"data warehouse"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["Servir les consumers qui ont besoin de la donn\xe9e brute, en acc\xe9dant directement au ",(0,i.jsx)(n.em,{children:"data lake"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Les data scientistes vont en g\xe9n\xe9ral vouloir y acc\xe9der via le slow storage."}),"\n",(0,i.jsxs)(n.li,{children:["Et l’acc\xe8s via le fast storage va plut\xf4t int\xe9resser les applications qui s’abonnent en mode streaming.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Par exemple un syst\xe8me de recommandation ecommerce en temps r\xe9el."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"6.1 - Orchestration overlay layer"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Son but est de :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Coordonner l’ex\xe9cution de jobs, sous la forme d’un graphe de d\xe9pendance."}),"\n",(0,i.jsx)(n.li,{children:"G\xe9rer les \xe9checs et les retries."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["C’est un peu le compl\xe9ment du ",(0,i.jsx)(n.em,{children:"technical metadata layer"})," pour permettre le faible couplage entre les layers."]}),"\n",(0,i.jsxs)(n.li,{children:["L’outil le plus connu d’orchestration est ",(0,i.jsx)(n.strong,{children:"Apache Airflow"}),", adopt\xe9 par Google Cloud Platform sous le nom de ",(0,i.jsx)(n.strong,{children:"Google Composer"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"AWS et Azure ont quant \xe0 eux choisi d’inclure des fonctionnalit\xe9s d’orchestration dans leur outil d’ETL."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"6.2 - ETL overlay layer"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Son but est de :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Prendre en charge les fonctionnalit\xe9s de certains layers (ingestion, processing, metadata, orchestration) ",(0,i.jsx)(n.strong,{children:"avec peu ou pas de code"}),"."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["On pourrait faire l’ensemble de notre pipeline avec cet outil ETL, la question \xe0 se poser c’est : ",(0,i.jsx)(n.strong,{children:"\xe0 quel point il est ouvert \xe0 l’extension ?"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"On peut vouloir \xe0 l’avenir par exemple utiliser un autre outil de processing, ou s’interfacer avec un outil open source."}),"\n",(0,i.jsxs)(n.li,{children:["Dans le cas o\xf9 il y a une incompatibilit\xe9 avec un usage qu’on a, on peut toujours l’impl\xe9menter \xe0 part de l’outil ETL.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Le probl\xe8me c’est qu’au bout d’un moment, les usages \xe0 c\xf4t\xe9 deviennent aussi complexes que la solution enti\xe8re sans l’outil ETL, mais avec une ",(0,i.jsx)(n.strong,{children:"architecture spaghetti"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Parmi les outils ETL il y a ",(0,i.jsx)(n.strong,{children:"AWS Glue"}),", ",(0,i.jsx)(n.strong,{children:"Azure Data Factory"})," et ",(0,i.jsx)(n.strong,{children:"Google Cloud Data Fusion"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Il existe des solutions commerciales non cloud-natives comme ",(0,i.jsx)(n.strong,{children:"Talend"})," et ",(0,i.jsx)(n.strong,{children:"Informatica"}),", mais ce livre se limite au cloud-native et aux outils open source."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Les ",(0,i.jsx)(n.strong,{children:"couches"})," doivent \xeatre bien ",(0,i.jsx)(n.strong,{children:"s\xe9par\xe9es et d\xe9coupl\xe9es"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Une premi\xe8re raison est de pouvoir utiliser les outils les plus adapt\xe9s aux besoins de chaque couche.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Le cloud bougeant tr\xe8s vite, on voudra sans doute pouvoir changer seulement l’un d’entre eux quand on a une meilleure alternative pour une couche en particulier."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Une autre raison est qu’on peut avoir plusieurs \xe9quipes en charge de la data platform, et il vaut mieux qu’elles ne se g\xeanent pas.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Par exemple, on voudra souvent avoir l’ingestion plut\xf4t centralis\xe9e, et le processing plut\xf4t en mode libre service pour chaque \xe9quipe qui en a besoin."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Les ",(0,i.jsx)(n.strong,{children:"outils"})," pouvant servir dans une des couches de notre plateforme sont class\xe9s en 4 cat\xe9gories (les auteurs les priorisent dans cet ordre) :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["1 - Solutions ",(0,i.jsx)(n.strong,{children:"cloud-native PaaS"})," d’AWS, GCP ou Azure.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Leur avantage principal c’est le gain de temps : on n’a pas \xe0 se pr\xe9occuper de la compatibilit\xe9. On configure tr\xe8s facilement et c’est en prod."}),"\n",(0,i.jsx)(n.li,{children:"Par contre, c’est la solution qui va \xeatre la moins extensible : si par exemple un connecteur n’est pas support\xe9, on aura du mal \xe0 l’ajouter."}),"\n",(0,i.jsx)(n.li,{children:"Elle est aussi peu portable, vu qu’on n’a pas les m\xeames services d’un cloud provider \xe0 un autre."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["2 - Solutions ",(0,i.jsx)(n.strong,{children:"serverless"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Il s’agit de pouvoir d\xe9ployer son code custom, mais sans avoir \xe0 se pr\xe9occuper des serveurs, de leur configuration, du scaling etc."}),"\n",(0,i.jsx)(n.li,{children:"C’est une solution interm\xe9diaire d’un point de vue trade-offs sur la flexibilit\xe9, la portabilit\xe9 et le gain de temps."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["3 - Solutions ",(0,i.jsx)(n.strong,{children:"open-source"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Leur avantage c’est c’est la flexibilit\xe9 et la portabilit\xe9 maximales, mais de l’autre c\xf4t\xe9 on a \xe0 g\xe9rer soi-m\xeame des VMs dans le cloud donc plus de travail d’Ops."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["4 - Solutions ",(0,i.jsx)(n.strong,{children:"SaaS commerciales"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Elles peuvent avoir un int\xe9r\xeat si elles ont une fonctionnalit\xe9 non disponible sous forme PaaS ou open source."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Dans les faits, on va utiliser un ",(0,i.jsx)(n.strong,{children:"mix de solutions des 4 cat\xe9gories"})," en fonction des layers et des besoins qu’on a.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["On a de plus en plus d’entreprises qui utilisent des solutions de ",(0,i.jsx)(n.strong,{children:"plusieurs cloud providers"}),". Par exemple le gros des services sur AWS, et le use-case machine learning sur GCP."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Outils sur ",(0,i.jsx)(n.strong,{children:"AWS"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Batch ingestion.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"AWS Glue"})," supporte l’ingestion \xe0 partir de ",(0,i.jsx)(n.strong,{children:"AWS S3"}),", ou \xe0 partir d’une connexion JDBC."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"AWS Database Migration Service"})," sert \xe0 la base \xe0 transf\xe9rer ses DBs vers AWS, mais on peut l’utiliser comme ingestion layer."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"AWS DMS"})," permet d’impl\xe9menter un m\xe9canisme de change data capture \xe0 partir d’une DB."]}),"\n",(0,i.jsxs)(n.li,{children:["Si aucune des solutions PaaS ne supporte notre data source, on peut utiliser la solution serverless ",(0,i.jsx)(n.strong,{children:"AWS Lambda"})," o\xf9 il faudra \xe9crire et maintenir du code."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Streaming ingestion.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"AWS Kinesis"})," est un message broker pour lequel il faudra \xe9crire du code pour publier dedans. Il a malheureusement tr\xe8s peu de connecteurs entrants.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["En revanche il a des connecteurs sortants appel\xe9s ",(0,i.jsx)(n.strong,{children:"Kinesis Firehose"}),", qui permettent par exemple d’envoyer la donn\xe9e de Kinesis dans un ",(0,i.jsx)(n.strong,{children:"S3"})," sous format Parquet."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"AWS Managed Streaming for Apache Kafka (MSK)"})," est une version de ",(0,i.jsx)(n.strong,{children:"Kafka"})," enti\xe8rement manag\xe9e.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["On peut l’utiliser \xe0 la place de ",(0,i.jsx)(n.strong,{children:"Kinesis"}),", par exemple si on migre une application avec ",(0,i.jsx)(n.strong,{children:"Kafka"})," vers AWS."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Storage.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"AWS S3"})," permet de stocker de la donn\xe9e de mani\xe8re scalable, avec la possibilit\xe9 de choisir entre plusieurs formules avec des latences plus ou moins grandes."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Batch processing.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"AWS Elastic MapReduce (EMR)"})," est une version manag\xe9e de ",(0,i.jsx)(n.strong,{children:"Spark"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["On va en g\xe9n\xe9ral lire la donn\xe9e depuis ",(0,i.jsx)(n.strong,{children:"S3"}),", faire le calcul, puis d\xe9truire le cluster ",(0,i.jsx)(n.strong,{children:"EMR"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Streaming processing.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"AWS Kinesis Data Analytics"})," permet de se brancher sur ",(0,i.jsx)(n.strong,{children:"Kinesis"}),", et de faire du processing en streaming."]}),"\n",(0,i.jsxs)(n.li,{children:["Si on utilise ",(0,i.jsx)(n.strong,{children:"AWS MSK"}),", on peut brancher dessus ",(0,i.jsx)(n.strong,{children:"Kafka Streams"})," pour le processing en streaming."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Data warehouse.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"AWS Redshift"})," est un data warehouse distribut\xe9 sur plusieurs noeuds.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Redshift Spectrum"})," permet de faire des requ\xeates depuis ",(0,i.jsx)(n.strong,{children:"Redshift"})," pour obtenir des donn\xe9es qui sont en fait sur ",(0,i.jsx)(n.strong,{children:"S3"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Il faudra d\xe9finir des “tables externes”, et la performance de la query sera moins bonne, mais \xe7a permet d’\xe9conomiser de la place dans le data warehouse."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Direct access.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"AWS Athena"})," permet de faire une requ\xeate SQL distribu\xe9e en utilisant directement la donn\xe9e sur ",(0,i.jsx)(n.strong,{children:"S3"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"On lance l’instance le temps de la requ\xeate, puis on d\xe9truit l’instance."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["ETL overlay et metadata repository.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"AWS Glue"})," est un outil d’ETL complet.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Il est construit autour de ",(0,i.jsx)(n.strong,{children:"Spark"}),", et poss\xe8de des templates pour faciliter de nombreuses transformations.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Il a aussi des add-ons ",(0,i.jsx)(n.strong,{children:"Spark"})," non-standards, ce qui nuit \xe0 la portabilit\xe9 par rapport \xe0 un simple ",(0,i.jsx)(n.strong,{children:"Spark"})," manag\xe9."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Il maintient un ",(0,i.jsx)(n.em,{children:"Data Catalog"})," \xe0 partir des donn\xe9es disponibles sur ",(0,i.jsx)(n.strong,{children:"S3"}),"."]}),"\n",(0,i.jsx)(n.li,{children:"Il maintient un ensemble de statistiques sur l’ex\xe9cution des jobs."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Orchestration.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"AWS Step Functions"})," permet de cr\xe9er des workflows qui mettent en jeu diff\xe9rents services, y compris ceux qui ne seraient pas g\xe9r\xe9s par ",(0,i.jsx)(n.strong,{children:"Glue"})," comme ",(0,i.jsx)(n.strong,{children:"AWS Lambda"})," avec du code custom."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Consumers.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Pour les outils comme ",(0,i.jsx)(n.strong,{children:"Tableau"})," qui ont besoin d’une connexion JDBC/ODBC qui supporte SQL, elles peuvent se connecter \xe0 ",(0,i.jsx)(n.strong,{children:"Redshift"})," ou ",(0,i.jsx)(n.strong,{children:"Athena"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["Pour du streaming avec faible latence, on peut envoyer la donn\xe9e dans un key/value store comme ",(0,i.jsx)(n.strong,{children:"DynamoDB"}),", ou dans une DB comme ",(0,i.jsx)(n.strong,{children:"AWS RDS"})," ou ",(0,i.jsx)(n.strong,{children:"AWS Aurora"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Outils sur ",(0,i.jsx)(n.strong,{children:"GCP"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Batch ingestion.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Cloud Data Fusion"})," est un ETL overlay qui permet d’ing\xe9rer des donn\xe9es depuis une DB relationnelle avec JDBC, des fichiers depuis ",(0,i.jsx)(n.strong,{children:"Google Cloud Storage"}),", et m\xeame depuis un FTP ou depuis ",(0,i.jsx)(n.strong,{children:"AWS S3"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Il est bas\xe9 sur un projet open source, et donc supporte des connecteurs custom."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"BigQuery Data Transfer Service"})," permet d’ing\xe9rer de la donn\xe9e depuis les services SaaS de Google, et depuis des centaines d’autres services SaaS connus gr\xe2ce \xe0 un partenariat avec Fivetran.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Par contre, la donn\xe9e va directement dans le data warehouse, ce qui ne permet pas vraiment l’architecture modulaire qu’on vise."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Cloud Functions"})," repr\xe9sente l’\xe9quivalent d’",(0,i.jsx)(n.strong,{children:"AWS Lambda"}),", avec le d\xe9savantage d’avoir une limite de temps d’ex\xe9cution des fonctions serverless."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Stream ingrestion.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Cloud Pub/Sub"})," est un broker \xe9quivalent \xe0 ",(0,i.jsx)(n.strong,{children:"AWS Kinesis"}),"."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Storage.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Google Cloud Storage"})," est un \xe9quivalent \xe0 ",(0,i.jsx)(n.strong,{children:"AWS S3"}),"."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Batch processing.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Dataproc"})," est un ",(0,i.jsx)(n.strong,{children:"Spark"})," manag\xe9 \xe9quivalent \xe0 ",(0,i.jsx)(n.strong,{children:"AWS EMR"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Cloud Dataflow"})," est un ",(0,i.jsx)(n.strong,{children:"Apache Beam"})," manag\xe9.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Beam"})," a l’avantage d’offrir une m\xeame API pour le batch processing et le streaming processing, l\xe0 o\xf9 ",(0,i.jsx)(n.strong,{children:"Spark"})," ne supporte que le batch mais est une techno plus mature."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Streaming processing.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Cloud Dataflow"})," repr\xe9sente la mani\xe8re clou-native de faire du streaming sur GCP."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Dataproc"})," avec ",(0,i.jsx)(n.strong,{children:"Spark Streaming"})," peut repr\xe9senter une alternative, mais il s’agit en fait de micro-batch et non pas de traiter les messages un par un.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Les auteurs conseillent ",(0,i.jsx)(n.strong,{children:"Beam"}),", sauf si on a d\xe9j\xe0 investi en temps ou connaissances sur ",(0,i.jsx)(n.strong,{children:"Spark"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Data warehouse.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"BigQuery"})," est un \xe9quivalent \xe0 ",(0,i.jsx)(n.strong,{children:"AWS RedShift"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Il a l’avantage de scaler le nombre de nœuds tout seul."}),"\n",(0,i.jsx)(n.li,{children:"Par contre il a un mod\xe8le de facturation bas\xe9 sur la donn\xe9e lue par chaque requ\xeate, ce qui peut rendre les co\xfbts difficiles \xe0 pr\xe9dire."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Direct access.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["GCP ne propose pas de services pour acc\xe9der au ",(0,i.jsx)(n.em,{children:"data lake"})," directement avec du SQL.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["On peut \xe9ventuellement cr\xe9er des tables vers de la donn\xe9e externe (donc dans le ",(0,i.jsx)(n.em,{children:"data lake"}),") \xe0 partir de ",(0,i.jsx)(n.strong,{children:"BigQuery"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["On peut aussi utiliser ",(0,i.jsx)(n.strong,{children:"Spark SQL"})," pour identifier et lire de la donn\xe9e sur le ",(0,i.jsx)(n.em,{children:"data lake"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["ETL overlay et metadata repository.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Cloud Data Fusion"})," est un ETL overlay \xe9quivalent \xe0 ",(0,i.jsx)(n.strong,{children:"AWS Glue"}),". Il fournit une UI qui permet de configurer la pipeline.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Il met \xe0 disposition un moyen d’analyser quelle partie de la pipeline peut affecter telle ou telle donn\xe9e."}),"\n",(0,i.jsx)(n.li,{children:"Il met aussi \xe0 disposition des statistiques sur l’ex\xe9cution des jobs."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Orchestration.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Cloud Composer"})," permet de cr\xe9er des flows d’orchestration entre jobs.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Il est bas\xe9 sur ",(0,i.jsx)(n.strong,{children:"Apache Airflow"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Consumers.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"BigQuery"})," n’a pas de connexion JDBC/ODBC pour y connecter un outil BI par exemple.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Il a une API REST, et il est directement compatible avec certains outils BI."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Si on veut consommer la donn\xe9e avec une faible latence, on peut la mettre dans le key/value store ",(0,i.jsx)(n.strong,{children:"Cloud Bigtable"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Outils sur ",(0,i.jsx)(n.strong,{children:"Azure"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Batch ingestion.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Azure Data Factory"})," est un ETL overlay permettant de faire de l’ingestion depuis diverses sources (DB, SaaS externes, ",(0,i.jsx)(n.strong,{children:"S3"}),", ",(0,i.jsx)(n.strong,{children:"GCS"})," etc.).","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Il est celui qui a le plus de connecteurs compar\xe9 \xe0 ",(0,i.jsx)(n.strong,{children:"AWS Glue"})," et ",(0,i.jsx)(n.strong,{children:"Cloud Data Fusion"}),"."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Azure Functions"})," est l’\xe9quivalent d’",(0,i.jsx)(n.strong,{children:"AWS Lambda"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Il ne supporte que Java et Python."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Streaming ingestion.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Azure Event Hubs"})," est \xe9quivalent \xe0 ",(0,i.jsx)(n.strong,{children:"AWS Kinesis"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Il a la particularit\xe9 d’\xeatre compatible avec ",(0,i.jsx)(n.strong,{children:"Apache Kafka"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Storage.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Azure Blob Storage"})," est \xe9quivalent \xe0 ",(0,i.jsx)(n.strong,{children:"AWS S3"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Azure Data Lake Storage"})," est une version am\xe9lior\xe9e qui supporte mieux le calcul distribu\xe9 avec de grandes quantit\xe9s de donn\xe9es."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Batch processing.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Pour le batch processing, Azure a choisi de miser sur un partenariat avec ",(0,i.jsx)(n.strong,{children:"Databricks"}),", qui est un service cr\xe9\xe9 par les cr\xe9ateurs de ",(0,i.jsx)(n.strong,{children:"Spark"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["La version manag\xe9e de ",(0,i.jsx)(n.strong,{children:"Databricks"})," est disponible sur AWS et Azure, mais elle est celle par d\xe9faut sur Azure, donc mieux support\xe9e par son \xe9cosyst\xe8me."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Streaming processing.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Azure Stream Analytics se branche sur Event Hubs et permet de faire du streaming processing."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Data warehouse.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Azure Synapse"})," est le ",(0,i.jsx)(n.em,{children:"data warehouse"})," d’Azure.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Il est entre ",(0,i.jsx)(n.strong,{children:"AWS Redshift"})," et ",(0,i.jsx)(n.strong,{children:"Google BigQuery"})," dans la mesure o\xf9 il n\xe9cessite de choisir la capacit\xe9 de calcul, mais il scale l’espace disque tout seul."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Direct access.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Azure Databricks"})," est la mani\xe8re privil\xe9gi\xe9e d’acc\xe9der \xe0 la donn\xe9e sur le ",(0,i.jsx)(n.em,{children:"data lake"}),", soit par l’API native de ",(0,i.jsx)(n.strong,{children:"Spark"}),", soit en SQL avec ",(0,i.jsx)(n.strong,{children:"Spark SQL"}),"."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["ETL overlay et metadata repository.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Azure Data Factory"})," est \xe9quivalent \xe0 ",(0,i.jsx)(n.strong,{children:"AWS Glue"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Il s’int\xe8gre parfaitement avec ",(0,i.jsx)(n.strong,{children:"Databricks"})," pour les transformations complexes."]}),"\n",(0,i.jsx)(n.li,{children:"Il fournit des m\xe9triques sur la data pipeline."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Orchestration.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"La partie orchestration des jobs est prise en charge par Azure Data Factory."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Consumers.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Azure Synapse"})," fournit une connexion JDBC/ODBC pour connecter les outils de BI.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Azure Databricks"})," fournit la m\xeame chose, mais il faut un cluster Spark toujours allum\xe9, ce qui peut co\xfbter cher."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Cosmos DB"})," est une DB orient\xe9e document o\xf9 on peut stocker les r\xe9sultats de processings pour un acc\xe8s faible latence."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Alternatives ",(0,i.jsx)(n.strong,{children:"commerciales"})," ou ",(0,i.jsx)(n.strong,{children:"open source"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Certains logiciels open source sont trop difficiles \xe0 mettre en place, par exemple un ",(0,i.jsx)(n.em,{children:"data warehouse"})," distribu\xe9 comme ",(0,i.jsx)(n.strong,{children:"Apache Druid"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["Batch ingestion.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Il existe pas mal d’outils open source et commerciaux qui permettent d’ing\xe9rer des donn\xe9es, leur valeur ajout\xe9e \xe9tant en g\xe9n\xe9ral le grand nombre de sources support\xe9es."}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Apachi NiFi"})," est une solution open source qui supporte de nombreuses sources, et permet d’en ajouter soi-m\xeame en Java."]}),"\n",(0,i.jsxs)(n.li,{children:["Il existe de nombreux outils SaaS commerciaux qui g\xe8rent l’ingestion.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Ces outils vont souvent envoyer la donn\xe9e directement dans un data warehouse."}),"\n",(0,i.jsx)(n.li,{children:"Il faut bien r\xe9fl\xe9chir \xe0 la probl\xe9matique de la s\xe9curit\xe9."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Streaming ingestion.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Apache Kafka"})," est le principal outil utilis\xe9 en dehors d’une solution manag\xe9e de streaming.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Il a l’avantage de pouvoir se connecter \xe0 de nombreuses sources avec ",(0,i.jsx)(n.strong,{children:"Kafka Connect"}),", et il a un moyen d’impl\xe9menter des applications de streaming avec ",(0,i.jsx)(n.strong,{children:"Kafka Streams"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["Les raisons de choisir ",(0,i.jsx)(n.strong,{children:"Kafka"})," plut\xf4t qu’une solution cloud-native peuvent \xeatre l’investissement qu’on a d\xe9j\xe0 dans ",(0,i.jsx)(n.strong,{children:"Kafka"})," (par exemple connaissances), ou le besoin de performance n\xe9cessitant le fine-tuning du serveur ",(0,i.jsx)(n.strong,{children:"Kafka"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Orchestration.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Apache Airflow"})," est le principal outil utilis\xe9 en dehors d’une solution manag\xe9e d’orchestration.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"La raison de l’utiliser en mode non manag\xe9 peut \xeatre de profiter de sa flexibilit\xe9, avec ses fichiers en Python."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"4---getting-data-into-the-platform",children:"4 - Getting data into the platform"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Le layer d’ingestion peut avoir besoin d’ing\xe9rer diff\xe9rents types de donn\xe9es :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"1 - Les bases de donn\xe9es relationnelles"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Leurs donn\xe9es sont organis\xe9es en colonnes et typ\xe9es, mais chaque vendor a des types \xe0 lui.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Il y a donc un ",(0,i.jsx)(n.strong,{children:"mapping"})," \xe0 faire entre le type de chaque colonne et notre mod\xe8le."]}),"\n",(0,i.jsx)(n.li,{children:"Ce mapping va changer r\xe9guli\xe8rement en fonction des \xe9volutions fonctionnelles des applications qui utilisent cette DB."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Vu que la donn\xe9e est normalis\xe9e, elle se trouve dans des centaines de tables qu’on peut joindre au moment des queries.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Il faudra donc ",(0,i.jsx)(n.strong,{children:"automatiser le mapping"})," pour \xe9viter de le faire \xe0 la main."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["La donn\xe9e change r\xe9guli\xe8rement dans la DB, pour refl\xe9ter l’\xe9tat de l’application, elle est ",(0,i.jsx)(n.strong,{children:"volatile"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Il faudra donc aller chercher r\xe9guli\xe8rement les derniers changements."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"2 - Les fichiers."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Les fichiers sont structur\xe9s selon divers types de format texte ou binaire (CSV, JSON XML, Avro, Protobuf etc.) qui ne contiennent pas d’information de type.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Il faut donc pouvoir ",(0,i.jsx)(n.strong,{children:"supporter le parsing de tous ces formats"}),"."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Les fichiers ne garantissent aucun sch\xe9ma, et on voit beaucoup plus souvent des changements dans celui-ci que pour les DB relationnelles.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Il faut donc g\xe9rer les ",(0,i.jsx)(n.strong,{children:"changements de sch\xe9ma fr\xe9quents"}),"."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Les fichiers repr\xe9sentent en g\xe9n\xe9ral de la ",(0,i.jsx)(n.strong,{children:"donn\xe9e fig\xe9e"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["La nouvelle donn\xe9e est \xe9crite dans un autre fichier, donc on se retrouve \xe0 devoir ing\xe9rer ",(0,i.jsx)(n.strong,{children:"de nombreux fichiers"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"3 - La donn\xe9e SaaS via API."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Les donn\xe9es SaaS sont en g\xe9n\xe9ral disponibles via une API REST, qui renvoie du JSON."}),"\n",(0,i.jsxs)(n.li,{children:["Chaque provider a sa propre API, et son propre format. Il faudra donc ",(0,i.jsx)(n.strong,{children:"impl\xe9menter la partie ingestion pour chacun des providers"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Il faudra faire la validation du sch\xe9ma \xe0 chaque fois."}),"\n",(0,i.jsx)(n.li,{children:"Il faudra la tenir \xe0 jour en fonction des changements d’API."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"4 - Les streams"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Les m\xeames donn\xe9es peuvent arriver plusieurs fois, donc il faut que notre pipeline puisse ",(0,i.jsx)(n.strong,{children:"g\xe9rer les duplicatas"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["Les events des streams sont immutables, et peuvent \xeatre corrig\xe9s en ajoutant un autre message modifi\xe9 au stream.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Donc il faut que notre pipeline ",(0,i.jsx)(n.strong,{children:"g\xe8re la r\xe9conciliation entre plusieurs versions d’un m\xeame message"}),"."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Les donn\xe9es de streaming ont en g\xe9n\xe9ral un ",(0,i.jsx)(n.strong,{children:"grand volume"}),", donc il faut une infrastructure qui le supporte."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Concernant le cas des ",(0,i.jsx)(n.strong,{children:"bases de donn\xe9es relationnelles"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Il y a deux moyens d’ing\xe9rer de la donn\xe9e depuis une DB relationnelle :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"1 - L’utilisation de requ\xeates SQL"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Il s’agit d’avoir un composant qui va :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["1 - Ex\xe9cuter la requ\xeate vers la DB concern\xe9e.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Ca peut \xeatre un simple :","\n",(0,i.jsx)(n.pre,{"data-language":"sql","data-theme":"default",children:(0,i.jsx)(n.code,{"data-language":"sql","data-theme":"default",children:(0,i.jsxs)(n.span,{className:"line",children:[(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"SELECT"}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" "}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"*"}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" "}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"FROM"}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" "}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"table"})]})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.li,{children:"2 - R\xe9cup\xe9rer la donn\xe9e sous un format qu’il comprend."}),"\n",(0,i.jsxs)(n.li,{children:["3 - Mapper la donn\xe9e dans le bon format pour la stocker sur le ",(0,i.jsx)(n.em,{children:"storage layer"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["Il y a donc ",(0,i.jsx)(n.strong,{children:"2 mappings"})," qui se produisent pendant l’op\xe9ration."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Alors que la donn\xe9e op\xe9rationnelle s’int\xe9resse \xe0 l’\xe9tat actuel (“Quels sont les articles dans le panier ?”), ",(0,i.jsx)(n.strong,{children:"la donn\xe9e analytique s’int\xe9resse \xe0 l’\xe9volution de l’\xe9tat dans le temps"})," (“Quels articles ont \xe9t\xe9 ajout\xe9s ou enlev\xe9s et dans quel ordre ?”).","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Il faut donc un moyen pour capturer l’\xe9volution de la donn\xe9e dans le temps."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Une 1\xe8re solution pour garder l’\xe9volution dans le temps est de faire une ",(0,i.jsx)(n.strong,{children:"full table ingestion"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["On va r\xe9cup\xe9rer l’ensemble des donn\xe9es d’une table \xe0 intervals r\xe9guliers, sauver ces snapshots dans le ",(0,i.jsx)(n.em,{children:"data lake"}),", et les charger dans le ",(0,i.jsx)(n.em,{children:"data warehouse"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["Pour en tirer quelque chose, il faut** superposer les rows des snapshots** dans la m\xeame table du ",(0,i.jsx)(n.em,{children:"data warehouse"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Pour diff\xe9rencier les rows de chaque snapshot, on peut ajouter une colonne ",(0,i.jsx)(n.code,{children:"INGEST_DATE"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["On peut directement utiliser du SQL pour obtenir les donn\xe9es qu’on veut, mais pour certains usages on aura besoin de faire une transformation dans le ",(0,i.jsx)(n.em,{children:"processing layer"}),"."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Parmi les donn\xe9es d\xe9riv\xe9es qu’on voudra cr\xe9er, il peut y avoir :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Cr\xe9er une ",(0,i.jsx)(n.em,{children:"view"})," qui ne montre que les rows du dernier snapshot."]}),"\n",(0,i.jsxs)(n.li,{children:["De la donn\xe9e qui ",(0,i.jsx)(n.strong,{children:"identifie les suppressions"}),", en identifiant les rows qui existaient dans un snapshot et n’existaient plus dans le suivant."]}),"\n",(0,i.jsx)(n.li,{children:"Une version “compact\xe9e”, qui \xe9limine les rows qui n’ont pas chang\xe9 par rapport au snapshot pr\xe9c\xe9dent."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Le probl\xe8me de la full table ingestion, c’est la charge sur la machine de DB, et l’",(0,i.jsx)(n.strong,{children:"\xe9norme quantit\xe9 de donn\xe9es"})," qu’on finit par avoir."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Une autre solution peut \xeatre l’",(0,i.jsx)(n.strong,{children:"incremental table ingestion"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Il s’agit toujours de r\xe9cup\xe9rer des snapshots \xe0 intervalles r\xe9guliers, mais ",(0,i.jsx)(n.strong,{children:"seulement de la donn\xe9e qui a chang\xe9"})," depuis le pr\xe9c\xe9dent snapshot."]}),"\n",(0,i.jsxs)(n.li,{children:["Pour savoir quelle donn\xe9e a chang\xe9 :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["La table d’origine doit avoir un champ ",(0,i.jsx)(n.code,{children:"LAST_MODIFIED"}),", mis \xe0 jour automatiquement par la DB."]}),"\n",(0,i.jsxs)(n.li,{children:["En retenant le ",(0,i.jsx)(n.code,{children:"MAX(LAST_MODIFIED)"})," du dernier run d’ingestion (qu’on appelle le ",(0,i.jsx)(n.em,{children:"highest watermark"}),"), on peut construire une query qui r\xe9cup\xe8re uniquement les nouvelles donn\xe9es :","\n",(0,i.jsx)(n.pre,{"data-language":"sql","data-theme":"default",children:(0,i.jsx)(n.code,{"data-language":"sql","data-theme":"default",children:(0,i.jsxs)(n.span,{className:"line",children:[(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"SELECT"}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" "}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"*"}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" "}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"FROM"}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" subscriptions "}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"WHERE"}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" LAST_MODIFIED "}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:">"}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" "}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-string-expression)"},children:'"2019-05-01 17:01:00"'})]})})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["On pourra mettre le ",(0,i.jsx)(n.em,{children:"highest watermark"})," dans le ",(0,i.jsx)(n.em,{children:"technical metadata layer"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"AWS Glue"})," g\xe8re nativement le stockage de ce genre de donn\xe9es, mais on peut le mettre dans une DB manag\xe9e comme ",(0,i.jsx)(n.strong,{children:"Google Cloud SQL"})," ou ",(0,i.jsx)(n.strong,{children:"Azure SQL Database"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Cette ",(0,i.jsx)(n.em,{children:"incremental table ingestion"})," permet d’ing\xe9rer moins de donn\xe9es dupliqu\xe9es, mais elle a encore des inconv\xe9nients :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Il faut faire du processing pour faire appara\xeetre les donn\xe9es supprim\xe9es, en comparant les snapshots entre eux."}),"\n",(0,i.jsxs)(n.li,{children:["Les donn\xe9es qui sont ins\xe9r\xe9es puis supprim\xe9es entre deux snapshots ne seront pas captur\xe9es par ce m\xe9canisme, donc ",(0,i.jsx)(n.strong,{children:"on perd des informations"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"2 - Le Change Data Capture (CDC)"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Le CDC permet de r\xe9cup\xe9rer ",(0,i.jsx)(n.strong,{children:"l’ensemble des op\xe9rations"})," qui ont lieu sur la table, ",(0,i.jsx)(n.strong,{children:"sans aucun doublon"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["Il s’agit de lire le log de changements cr\xe9\xe9 par la DB, \xe0 l’aide d’une application qui sait le faire.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["L’application peut \xeatre fournie par la DB, ou une application cloud-native comme ",(0,i.jsx)(n.strong,{children:"AWS Database Migration Service"}),", ou une application open source comme ",(0,i.jsx)(n.strong,{children:"Debezium"}),"."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Etant donn\xe9 que les DBs ne gardent pas longtemps leur log de changements, ",(0,i.jsx)(n.strong,{children:"le CDC n\xe9cessite une infrastructure de type streaming"})," pour \xeatre r\xe9cup\xe9r\xe9."]}),"\n",(0,i.jsxs)(n.li,{children:["Le format des messages r\xe9cup\xe9r\xe9s depuis le log de changements contient la valeur du row avant, sa valeur apr\xe8s l’op\xe9ration, le type d’op\xe9ration, et des metadata.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["On va vouloir mettre dans le ",(0,i.jsx)(n.em,{children:"data warehouse"})," uniquement la valeur apr\xe8s l’op\xe9ration et le type d’op\xe9ration."]}),"\n",(0,i.jsxs)(n.li,{children:["La table dans le ",(0,i.jsx)(n.em,{children:"data warehouse"})," ressemble du coup au cas de l’",(0,i.jsx)(n.em,{children:"incremental table ingestion"})," : on a une entr\xe9e par changement."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Le CDC sur une DB ",(0,i.jsx)(n.strong,{children:"Oracle"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Oracle fournit ",(0,i.jsx)(n.strong,{children:"Oracle GoldenGate"}),", une application qui permet de lire son log de changement et de le transf\xe9rer vers diverses plateformes.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Il faut acheter la licence pour pouvoir l’utiliser."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["On peut mettre en place ",(0,i.jsx)(n.strong,{children:"Debezium"})," qui est open source, mais il faudra qu’il puisse se connecter \xe0 ",(0,i.jsx)(n.strong,{children:"Oracle XStream API"}),", qui lui-m\xeame n\xe9cessite quand m\xeame une licence ",(0,i.jsx)(n.strong,{children:"GoldenGate"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["Oracle fournit un outil d’analyse qui s’appelle ",(0,i.jsx)(n.strong,{children:"LogMiner"}),", qui est consid\xe9r\xe9 comme pas 100% fiable.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Certains outils comme ",(0,i.jsx)(n.strong,{children:"AWS Database Migration Service"})," l’utilisent malgr\xe9 tout."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Une alternative moins ch\xe8re \xe0 ",(0,i.jsx)(n.strong,{children:"GoldenGate"})," peut \xeatre ",(0,i.jsx)(n.strong,{children:"SharePlex"}),", un produit fait par Quest."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Le CDC sur une DB ",(0,i.jsx)(n.strong,{children:"MySQL"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"MySQL"})," \xe9crit les changements dans un log servant principalement \xe0 la r\xe9plication, pour ajouter des DBs followers."]}),"\n",(0,i.jsxs)(n.li,{children:["Vu que c’est une DB open source, il existe de nombreux outils pour servir d’application CDC \xe0 partir de ce log, par exemple : ",(0,i.jsx)(n.strong,{children:"Debezium"})," et ",(0,i.jsx)(n.strong,{children:"Apache NiFi"}),"."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Le CDC sur une DB ",(0,i.jsx)(n.strong,{children:"MS SQL Server"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"MS SQL Server"})," fournit la possibilit\xe9 de rediriger le log de changements d’une table vers une table cr\xe9\xe9e sp\xe9cialement pour \xe7a.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"On peut donc facilement impl\xe9menter un outil CDC qui n’a qu’\xe0 utiliser SQL pour lire cette nouvelle table r\xe9guli\xe8rement."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Parmi les outils qui supportent le CDC sur ",(0,i.jsx)(n.strong,{children:"MS SQL Server"}),", il y a par exemple: ",(0,i.jsx)(n.strong,{children:"Debezium"}),", ",(0,i.jsx)(n.strong,{children:"Apache NiFi"})," et ",(0,i.jsx)(n.strong,{children:"AWS Database Migration Service"}),"."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Le CDC sur une DB ",(0,i.jsx)(n.strong,{children:"PostgreSQL"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"PostgreSQL"})," supporte le fait de fournir son log de changements sous un format Protobuf ou JSON, ce qui facilite le travail des applications CDC."]}),"\n",(0,i.jsxs)(n.li,{children:["Il existe de nombreux outils qui savent lire ces donn\xe9es, par exemple : ",(0,i.jsx)(n.strong,{children:"Debezium"})," et ",(0,i.jsx)(n.strong,{children:"AWS Database Migration Service"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Concernant le ",(0,i.jsx)(n.strong,{children:"mapping des donn\xe9es"})," depuis la DB vers le ",(0,i.jsx)(n.em,{children:"data warehouse"}),", il va falloir faire une ",(0,i.jsx)(n.strong,{children:"analyse pour v\xe9rifier la compatibilit\xe9"})," :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["1 - On pr\xe9pare une liste des types de donn\xe9es support\xe9es par la DB dont on veut capturer les donn\xe9es.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Il vaut mieux prendre l’ensemble des types, en pr\xe9vision d’ajout de colonnes avec des types qui n’\xe9taient pas utilis\xe9s jusque l\xe0 par l’application."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["2 - On pr\xe9pare une liste des types support\xe9s par le ",(0,i.jsx)(n.em,{children:"data warehouse"})," de destination, et on identifie les diff\xe9rences avec la pr\xe9c\xe9dente."]}),"\n",(0,i.jsxs)(n.li,{children:["3 - On identifie les types qui ne correspondent pas exactement, mais permetteront une conversion sans perte d’information.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Par exemple un type ",(0,i.jsx)(n.code,{children:"SMALLINT"})," sur ",(0,i.jsx)(n.strong,{children:"MySQL"})," comme source, et le seul entier disponible sur ",(0,i.jsx)(n.strong,{children:"Google BigQuery"})," qui est l’\xe9quivalent d’un ",(0,i.jsx)(n.code,{children:"BIGINT"}),"."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["4 - On identifie les types qui n’ont pas de correspondance satisfaisante, et pourraient mener \xe0 une perte d’information.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"On essaye de voir si on ne peut pas trouver un workaround, par exemple transformer des donn\xe9es g\xe9ospatiales en string, puis utiliser du processing pour les parser au moment de la lecture."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["5 - Si on est devant une impasse, on essaye de voir s’il n’y a pas un outil de ",(0,i.jsx)(n.em,{children:"data warehouse"})," plus adapt\xe9."]}),"\n",(0,i.jsxs)(n.li,{children:["6 - Dans le cas o\xf9 notre application d’ingestion n’est pas faite \xe0 la main, on v\xe9rifie les types qu’elle supporte, et leur compatibilit\xe9 avec la source et la destination.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Les auteurs conseillent de faire plusieurs PoC, et disent de ne pas faire confiance aux documentations de ces outils."}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.li,{children:"7 - Si on \xe9crit l’application d’ingestion \xe0 la main, il faut v\xe9rifier les types support\xe9s par le driver qui nous permet d’acc\xe9der \xe0 la DB. Par exemple le driver JDBC."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Les DBs ",(0,i.jsx)(n.strong,{children:"NoSQL"})," sont \xe0 traiter diff\xe9remment des DBs relationnelles.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Parmi les solutions courantes :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"On peut utiliser un outil SaaS commercial qui supporte notre DB NoSQL : dans ce cas rien de plus \xe0 faire."}),"\n",(0,i.jsx)(n.li,{children:"Impl\xe9menter l’application d’ingestion \xe0 la main, en utilisant l’API de notre DB NoSQL directement pour acc\xe9der aux donn\xe9es."}),"\n",(0,i.jsxs)(n.li,{children:["Utiliser une application CDC si c’est disponible.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Par exemple ",(0,i.jsx)(n.strong,{children:"Debezium"})," supporte MongoDB."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["On peut utiliser l’outil d’export de donn\xe9es de notre DB NoSQL, et le faire tourner r\xe9guli\xe8rement pour avoir un snapshot des donn\xe9es.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"MongoDB"})," permet d’obtenir les donn\xe9es sous un format CSV ou JSON, et l’outil permet d’ajouter des requ\xeates, donc on peut avoir une colonne qui a la date de la derni\xe8re modification, et faire un ",(0,i.jsx)(n.em,{children:"incremental table ingestion"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Cassandra"})," permet d’obtenir les donn\xe9es sous un format CSV, mais uniquement en mode ",(0,i.jsx)(n.em,{children:"full table ingestion"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Concernant les ",(0,i.jsx)(n.strong,{children:"metadata li\xe9es \xe0 l’ingestion"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Il faut sauvegarder un certain nombre de statistiques pour pouvoir ensuite faire des v\xe9rifications sur la ",(0,i.jsx)(n.strong,{children:"qualit\xe9 des donn\xe9es"})," ing\xe9r\xe9es, et du ",(0,i.jsx)(n.strong,{children:"monitoring"})," de l’ingestion.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["On va mettre tout \xe7a dans notre ",(0,i.jsx)(n.em,{children:"technical metadata layer"}),"."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Parmi les ",(0,i.jsx)(n.strong,{children:"statistiques"})," qu’on veut :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Le nom et l’adresse IP du serveur de DB."}),"\n",(0,i.jsx)(n.li,{children:"Le nom de la base de donn\xe9es ou du sch\xe9ma."}),"\n",(0,i.jsx)(n.li,{children:"Le nom de la table."}),"\n",(0,i.jsx)(n.li,{children:"Le type de DB dans le cas o\xf9 on en g\xe8re plusieurs."}),"\n",(0,i.jsxs)(n.li,{children:["Pour de l’ingestion en batch, le nombre de rows ing\xe9r\xe9es.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"On pourra \xe0 partir de \xe7a v\xe9rifier que l’ensemble des donn\xe9es sont arriv\xe9es \xe0 destination."}),"\n",(0,i.jsx)(n.li,{children:"On peut monitorer ce chiffre pour \xeatre alert\xe9 dans le cas d’une variation anormale."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["La dur\xe9e de chaque job d’ingestion, de m\xeame que le d\xe9but et la fin de l’ingestion.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"C’est un moyen de monitorer la sant\xe9 de la pipeline."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Pour de l’ingestion en streaming, on prend les statistiques par ",(0,i.jsx)(n.strong,{children:"fen\xeatre temporelle"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Par exemple ins\xe9rer un row toutes les 5 mn dans notre DB de technical metadata. Plus on a besoin de r\xe9agir vite, et plus on va choisir une fen\xeatre petite."}),"\n",(0,i.jsx)(n.li,{children:"On peut aussi ajouter le nombre d’inserts, updates, deletes etc. pour chaque fen\xeatre."}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.li,{children:"Les changements dans le sch\xe9ma de la DB source, ce qui nous permettra d’\xeatre alert\xe9 et d’adapter la pipeline."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]})]})}var x=(0,r.j)(o)}},function(e){e.O(0,[105,774,888,179],function(){return e(e.s=9979)}),_N_E=e.O()}]);